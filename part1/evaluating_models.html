<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Evaluating Machine Learning Models – Machine Learning for Mechanical Engineering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notebooks/supervised_linear_models.html" rel="next">
<link href="../part1/part1.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-d58359eb3ecc31d3bec1ed479c2cbb3a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part1/part1.html">Foundational Skills</a></li><li class="breadcrumb-item"><a href="../part1/evaluating_models.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Evaluating Machine Learning Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning for Mechanical Engineering</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part1/part1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foundational Skills</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part1/evaluating_models.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Evaluating Machine Learning Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/supervised_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Linear Regression and Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part1/taking_derivatives.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Taking Derivatives</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../problems/problems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Problems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../problems/ps1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Problem Set 1</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../notebooks/notebooks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">In-Class Notebooks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/california_housing_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Housing Price Data Visualization In-Class Exercise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/supervised_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Linear Regression and Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/pytorch_autograd_verlet_integrator.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Demonstration of AD on Verlet Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/helpful_tooling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Helpful Tooling for Working with and Debugging Machine Learning Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/course_progression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Course Lecture Progression</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-diagonstic-plots" id="toc-sec-diagonstic-plots" class="nav-link active" data-scroll-target="#sec-diagonstic-plots"><span class="header-section-number">2.12</span> Useful Plots and Diagnostics for ML Models</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part1/part1.html">Foundational Skills</a></li><li class="breadcrumb-item"><a href="../part1/evaluating_models.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Evaluating Machine Learning Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-evaluating-models" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Evaluating Machine Learning Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Before we address specific types or classes of models, it is useful to discuss how we <em>evaluate</em> models. Specifically, this will include:</p>
<ol type="1">
<li>Defining what we view as a measure of success for the model – for example, many of the <a href="https://scikit-learn.org/stable/modules/model_evaluation.html">model metrics mentioned in the SKLearn metrics documentation</a></li>
<li>Finding a way to get an accurate Out-of-Sample Estimate of that metric – this is where Cross Validation plays a central role, and is more difficult or complex than one might first initially expect.</li>
</ol>
<p>Both of these topics you would have covered extensively in the Stochastics and Machine Learning course (including, e.g., the Bias-Variance tradeoff), but we will briefly review some of the core concepts below while</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/ML4ME_Textbook/ML4ME_Textbook/notebooks/cross_validation_linear_regression.ipynb" data-notebook-title="Introduction to Linear Models, Regularization, and Cross Validation" data-notebook-cellid="cell-0">
<section id="introduction-to-linear-models-regularization-and-cross-validation" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction-to-linear-models-regularization-and-cross-validation"><span class="header-section-number">2.1</span> Introduction to Linear Models, Regularization, and Cross Validation</h2>
<p>This notebook introduces will briefly introduce Linear Regression (which we will study in more detail in the conext of gradient descent), how to extend linear models to be (apparently) non-linear by adding features, and the concept of <em>Regularization</em> which will help us control the complexity of the linear regression model. Ultimately, the purpose of briefly introducing linear regression now is because it provides us a nice test-case where we have multiple model hyper-parameters that we will need to <em>tune</em> (or find the best settings of), and thus will motivate some of our interest in optimization which we will build upon in the future.</p>
<div id="cell-1" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>sns.set_context(<span class="st">'poster'</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of data points</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># True Function we want to estimate</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>true_fun <span class="op">=</span> <span class="kw">lambda</span> X: np.cos(<span class="fl">1.5</span> <span class="op">*</span> np.pi <span class="op">*</span> X)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Noisy Samples from the true function</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.sort(np.random.rand(n_samples))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> true_fun(X) <span class="op">+</span> np.random.randn(n_samples) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">10</span>))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the true function:</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>X_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.plot(X_plot, true_fun(X_plot), <span class="st">'--'</span>,label<span class="op">=</span><span class="st">"True function"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data samples</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.scatter(X,y, label<span class="op">=</span><span class="st">"Samples"</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"best"</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-2-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-2" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Just for the purposes of display, set the print precision to 2 decimals</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>precision <span class="dv">2</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>X[:, np.newaxis]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>array([[1.14e-04],
       [2.74e-02],
       [3.91e-02],
       [8.50e-02],
       [9.23e-02],
       [1.40e-01],
       [1.47e-01],
       [1.70e-01],
       [1.86e-01],
       [1.98e-01],
       [2.04e-01],
       [3.02e-01],
       [3.13e-01],
       [3.46e-01],
       [3.97e-01],
       [4.17e-01],
       [4.17e-01],
       [4.19e-01],
       [5.39e-01],
       [5.59e-01],
       [6.70e-01],
       [6.85e-01],
       [6.92e-01],
       [7.20e-01],
       [8.01e-01],
       [8.76e-01],
       [8.78e-01],
       [8.78e-01],
       [8.95e-01],
       [9.68e-01]])</code></pre>
</div>
</div>
<div id="cell-3" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's build some Polynomial features:</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>d<span class="op">=</span><span class="dv">3</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>pfeatures <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span>d,include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>pfeatures.fit_transform(X[:, np.newaxis])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>array([[1.14e-04, 1.31e-08, 1.50e-12],
       [2.74e-02, 7.50e-04, 2.05e-05],
       [3.91e-02, 1.53e-03, 5.96e-05],
       [8.50e-02, 7.23e-03, 6.15e-04],
       [9.23e-02, 8.53e-03, 7.87e-04],
       [1.40e-01, 1.97e-02, 2.77e-03],
       [1.47e-01, 2.15e-02, 3.16e-03],
       [1.70e-01, 2.88e-02, 4.90e-03],
       [1.86e-01, 3.47e-02, 6.46e-03],
       [1.98e-01, 3.92e-02, 7.77e-03],
       [2.04e-01, 4.18e-02, 8.55e-03],
       [3.02e-01, 9.14e-02, 2.76e-02],
       [3.13e-01, 9.82e-02, 3.08e-02],
       [3.46e-01, 1.19e-01, 4.13e-02],
       [3.97e-01, 1.57e-01, 6.25e-02],
       [4.17e-01, 1.74e-01, 7.25e-02],
       [4.17e-01, 1.74e-01, 7.27e-02],
       [4.19e-01, 1.76e-01, 7.37e-02],
       [5.39e-01, 2.90e-01, 1.56e-01],
       [5.59e-01, 3.12e-01, 1.74e-01],
       [6.70e-01, 4.50e-01, 3.01e-01],
       [6.85e-01, 4.70e-01, 3.22e-01],
       [6.92e-01, 4.79e-01, 3.32e-01],
       [7.20e-01, 5.19e-01, 3.74e-01],
       [8.01e-01, 6.41e-01, 5.13e-01],
       [8.76e-01, 7.68e-01, 6.73e-01],
       [8.78e-01, 7.71e-01, 6.77e-01],
       [8.78e-01, 7.71e-01, 6.77e-01],
       [8.95e-01, 8.00e-01, 7.16e-01],
       [9.68e-01, 9.38e-01, 9.08e-01]])</code></pre>
</div>
</div>
<div id="cell-4" class="cell" data-scrolled="false" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Here is a list of different degree polynomials to try out</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>degrees <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">15</span>,<span class="dv">20</span>,<span class="dv">30</span>]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate samples of the true function + noise</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.sort(np.random.rand(n_samples))</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>noise_amount <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> true_fun(X) <span class="op">+</span> np.random.randn(n_samples) <span class="op">*</span> noise_amount</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># For each of the different polynomial degrees we listed above</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> degrees:</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>)) <span class="co"># Make a new figure</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Construct the polynomial features</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    polynomial_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span>d,</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>                                             include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Construct linear regression model</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    linear_regression <span class="op">=</span> LinearRegression()</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    pipeline <span class="op">=</span> Pipeline([(<span class="st">"polynomial_features"</span>, polynomial_features),</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>                         (<span class="st">"linear_regression"</span>, linear_regression)])</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Now fit the data first through the </span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># polynomial basis, then do regression</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    pipeline.fit(X[:, np.newaxis], y)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the accuracy score of the trained model</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># on the original training data</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> pipeline.score(X[:, np.newaxis],y)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the results</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    X_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label<span class="op">=</span><span class="st">"Model"</span>)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    plt.plot(X_plot, true_fun(X_plot), <span class="st">'--'</span>,label<span class="op">=</span><span class="st">"True function"</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X, y, label<span class="op">=</span><span class="st">"Samples"</span>)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    plt.xlim((<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    plt.ylim((<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    plt.legend(loc<span class="op">=</span><span class="st">"best"</span>)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print the polynomial degree and the training</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># accuracy in the title of the graph</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Degree </span><span class="sc">{}</span><span class="ch">\n</span><span class="st">Train score = </span><span class="sc">{:.3f}</span><span class="st">"</span>.<span class="bu">format</span>(</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>        d, score))</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    plt.show()   </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-5-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-5-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-5-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-5-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-5-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-5-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-5-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can take a look at the learned weight coefficients, by accessing the <code>coef_</code> attribute of the fitted model:</p>
<div id="cell-6" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>linear_regression.coef_</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>array([ 4.29e+04, -6.41e+05,  4.46e+06, -5.47e+06, -1.61e+08,  1.50e+09,
       -7.03e+09,  2.01e+10, -3.46e+10,  2.83e+10,  9.52e+09, -3.50e+10,
        5.36e+08,  3.61e+10,  3.50e+09, -3.63e+10, -1.74e+10,  2.94e+10,
        3.42e+10, -1.02e+10, -4.27e+10, -1.74e+10,  3.46e+10,  4.11e+10,
       -1.48e+10, -5.44e+10,  1.25e+09,  6.96e+10, -5.11e+10,  1.14e+10])</code></pre>
</div>
</div>
<p>What is going on here? To understand this, we need to understand something about how the model is optimizing error.</p>
</section>
<section id="how-do-we-deal-with-this-tradeoff-regularization" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="how-do-we-deal-with-this-tradeoff-regularization"><span class="header-section-number">2.2</span> How do we deal with this tradeoff? Regularization!</h2>
<p><span class="math display">\[
Cost = Loss(w,D) + \Omega(w)
\]</span> where <span class="math inline">\(\Omega(w)\)</span> represents what we call a “Regularization” of the function or a “Penalty Term” The purpose of <span class="math inline">\(\Omega(w)\)</span> is to help us prevent the (otherwise complex) model from being overly complicated, by penalizing this complexity. There are many ways to do this that we will see later on, but one common way to do this for linear models is to penalize the <strong>total weight</strong> that you allow all of the <span class="math inline">\(w_i\)</span> to have. Specifically how one calculates this total weight turns out to matter a lot, and we shall see it return in future weeks. But to get us started in un-packing how to do this, we first need to talk about what a <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">Norm</a> is, how it relates to Linear Regression weights, and how it helps us perform Regularization.</p>
</section>
<section id="norms-and-their-relationship-to-regularization" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="norms-and-their-relationship-to-regularization"><span class="header-section-number">2.3</span> Norms and their relationship to Regularization</h2>
<p>A Norm is a concept in mathematics that allows us to essentially measure length or size, typically of vectors. Any time you have tried to compute the distance between two points in space (say, by using the Pythagorean Theorem), or the magnitude of an applied Force vector, you have been using a Norm – most likely the Euclidean Norm or Euclidean Distance. For example, for a vector <span class="math inline">\(\mathbf{x}\)</span> with <span class="math inline">\(n\)</span> dimensions, the Euclidean Norm looks like this: <span class="math display">\[
||\mathbf{x}||_2 = \sqrt{ x_1^2 + x_2^2 + \cdots x_n^2 }
\]</span> If you have ever had to compute the total Force Magnitude given its x and y components (for example, in Statics class), you have used the Euclidean Norm to do so. In that context, it served to take multiple components of a Force aggregate them in such a way as to tell you something about the <strong>total force</strong> – by analogy, we will do the same thing here with linear regression, where each weight is like a component and we can use the Euclidean Norm to compute the total weight.</p>
<p>While Euclidean Norms may be quite useful or familiar to Engineers, it turns out that they are a special case of a much wider <em>family</em> of Norms called <a href="https://en.wikipedia.org/wiki/Lp_space#The_p-norm_in_finite_dimensions"><em>p-norms</em></a>, which are defined as: <span class="math display">\[
||\mathbf{x}||_p = \left(|x_1|^p + |x_2|^p+\cdots + |x_n|^p\right)^{1/p} = \left(\sum_{i=1}^n \left| x_i \right|^p \right)^p
\]</span></p>
<p>Specifically, the Euclidean Norm is called the L2-norm, or sometimes just the 2-norm. To see why this is, just set <span class="math inline">\(p=2\)</span> in the above, and note how it corresponds to the Euclidean Norm that we all know and love. So, by setting <span class="math inline">\(p\)</span> to a number between <span class="math inline">\(0\)</span> and <span class="math inline">\(\infty\)</span>, we can modify what the <em>total weight</em> means, and setting <span class="math inline">\(p=2\)</span> is the setting which we are all most familiar with. To get a visual sense of how norms vary, see below, which visualizes a line of “circle” of radius 1, but where the length of the line is determined by the p-norm. You will see that when p=2 this corresponds to what we are familiar with, but when p goes up or down things change.</p>
<div id="cell-9" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ipywidgets <span class="im">import</span> interact, FloatSlider</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to compute and plot the p-norm unit ball in 2D</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_p_norm(p<span class="op">=</span><span class="fl">2.0</span>):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Avoid invalid p values</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p <span class="op">&lt;=</span> <span class="dv">0</span>:</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"p must be &gt; 0"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, <span class="dv">400</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Parametric form of p-norm unit circle</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.cos(theta)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.sin(theta)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normalize to p-norm = 1</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    denom <span class="op">=</span> (np.<span class="bu">abs</span>(x)<span class="op">**</span>p <span class="op">+</span> np.<span class="bu">abs</span>(y)<span class="op">**</span>p)<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span>p)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    x_unit <span class="op">=</span> x <span class="op">/</span> denom</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    y_unit <span class="op">=</span> y <span class="op">/</span> denom</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    plt.plot(x_unit, y_unit, label<span class="op">=</span><span class="ss">f'p = </span><span class="sc">{</span>p<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    plt.gca().set_aspect(<span class="st">'equal'</span>, adjustable<span class="op">=</span><span class="st">'box'</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f'Unit Ball in p-norm (p=</span><span class="sc">{</span>p<span class="sc">:.2f}</span><span class="ss">)'</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Interactive slider for p</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>interact(</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    plot_p_norm,</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    p<span class="op">=</span>FloatSlider(value<span class="op">=</span><span class="fl">2.0</span>, <span class="bu">min</span><span class="op">=</span><span class="fl">0.1</span>, <span class="bu">max</span><span class="op">=</span><span class="fl">10.0</span>, step<span class="op">=</span><span class="fl">0.1</span>, description<span class="op">=</span><span class="st">'p'</span>)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"dbeeb275e71449d7b0c93c768e93c2ac","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>&lt;function __main__.plot_p_norm(p=2.0)&gt;</code></pre>
</div>
</div>
<p>For today, we will just focus on the L2-Norm, however we will revist norms again later where we will see how changing the one we are using can have positive or negative effects in certain circumstances.</p>
<p>For today’s purpose, we will use the L2-Norm to help us penalize having linear regression models with really large weights, by essentially putting a cost on the total weight of the weight vector, where the total is measured by the L2-Norm. That is: <span class="math display">\[
Cost = \sum_{n=1}^{N}||y-w\cdot x||^2 + \alpha \cdot ||w||^2
\]</span> Where <span class="math inline">\(\alpha\)</span> is the price we pay for including more weight in the linear model. If it reduces our error cost enough to offset the cost of the increased weight, then that may be worth it to us. Otherwise, we would err on the side of using less weight overall.</p>
<p>This Regularization (i.e., increasing <span class="math inline">\(\alpha\)</span>) essentially allows you to trade off bias and variance.</p>
<div id="cell-11" class="cell" data-scrolled="false" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import Ridge Regression</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># alpha determines how much of </span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># a penalty the weights incur</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">1e-20</span>, <span class="fl">1e-10</span>, <span class="fl">1e-7</span>, <span class="fl">1e-5</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>]</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># For the below example, let's</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># just consider a 15-degree polynomial</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>d<span class="op">=</span><span class="dv">15</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">100</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a <span class="kw">in</span> alphas:</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>))</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    polynomial_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span>d,</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>                                             include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">#linear_regression = LinearRegression() #&lt;- Note difference with next line</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    linear_regression <span class="op">=</span> Ridge(alpha<span class="op">=</span>a)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    pipeline <span class="op">=</span> Pipeline([(<span class="st">"polynomial_features"</span>, polynomial_features),</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>                         (<span class="st">"linear_regression"</span>, linear_regression)])</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit model</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    pipeline.fit(X[:, np.newaxis], y)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get Training Accuracy</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> pipeline.score(X[:, np.newaxis],y)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot things</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    X_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label<span class="op">=</span><span class="st">"Model"</span>)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    plt.plot(X_plot, true_fun(X_plot), <span class="st">'--'</span>,label<span class="op">=</span><span class="st">"True function"</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X, y, label<span class="op">=</span><span class="st">"Samples"</span>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    plt.xlim((<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    plt.ylim((<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>    plt.legend(loc<span class="op">=</span><span class="st">"best"</span>)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Degree=</span><span class="sc">{}</span><span class="st">, $</span><span class="ch">\\</span><span class="st">alpha$=</span><span class="sc">{}</span><span class="ch">\n</span><span class="st">Train score = </span><span class="sc">{:.3f}</span><span class="st">"</span>.<span class="bu">format</span>(</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        d, a, score))</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-8-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-8-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-8-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-8-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-8-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-8-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-8-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="can-we-evaluate-this-more-rigorously" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="can-we-evaluate-this-more-rigorously"><span class="header-section-number">2.4</span> Can we evaluate this more rigorously?</h2>
<p>Yes, we can use data we have not seen yet to get an estimate of the generalization error. One popular way to do this is through Cross-Validation:</p>
<div id="cell-13" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Now let's split the data into training and test data:</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>     X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Let’s take a look at what the above has actually done.</p>
<div id="cell-15" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'X_train</span><span class="ch">\n</span><span class="st">'</span>,X_train,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'X_test</span><span class="ch">\n</span><span class="st">'</span>,X_test,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'y_train</span><span class="ch">\n</span><span class="st">'</span>,y_train,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'y_test</span><span class="ch">\n</span><span class="st">'</span>,y_test,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>plt.plot(X_plot, true_fun(X_plot), <span class="st">'--'</span>,label<span class="op">=</span><span class="st">"True function"</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the training and testing points in colors</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_train,y_train, label<span class="op">=</span><span class="st">"Training data"</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test,y_test, label<span class="op">=</span><span class="st">"Testing data"</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"best"</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>X_train
 [0.94 0.45 0.66 0.81 0.27 0.62 0.4  0.59 0.88 0.75 0.14 0.96 0.35 0.24
 0.66 0.73 0.41 0.35 0.9  0.17 0.11 0.75 0.62 0.51] 

X_test
 [0.14 0.95 0.58 0.43 0.93 0.9 ] 

y_train
 [-0.19 -0.48 -0.89 -0.75  0.38 -0.97 -0.32 -1.   -0.51 -0.96  0.89 -0.04
 -0.04  0.41 -0.88 -0.94 -0.37  0.   -0.44  0.51  0.87 -0.99 -0.94 -0.74] 

y_test
 [ 0.82 -0.28 -0.9  -0.41 -0.4  -0.51] 
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-10-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Key idea in cross validation is to test the model on data that was separate from the data you trained on.</p>
<div id="cell-17" class="cell" data-scrolled="false" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">1e-20</span>, <span class="fl">1e-10</span>, <span class="fl">1e-7</span>, <span class="fl">1e-5</span>, <span class="dv">1</span>,<span class="dv">10</span>]</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>d<span class="op">=</span><span class="dv">15</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a <span class="kw">in</span> alphas:</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>))</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">#plt.setp(ax, xticks=(), yticks=())</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    polynomial_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span>d,</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>                                             include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    linear_regression <span class="op">=</span> LinearRegression()</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    linear_regression <span class="op">=</span> Ridge(alpha<span class="op">=</span>a)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    pipeline <span class="op">=</span> Pipeline([(<span class="st">"polynomial_features"</span>, polynomial_features),</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>                         (<span class="st">"linear_regression"</span>, linear_regression)])</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">#pipeline.fit(X[:, np.newaxis], y)</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    pipeline.fit(X_train[:, np.newaxis], y_train)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate the models using crossvalidation</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">#scores = cross_validation.cross_val_score(pipeline,</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    X[:, np.newaxis], y, scoring="mean_squared_error", cv=10)</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    testing_score <span class="op">=</span> pipeline.score(X_test[:, np.newaxis],y_test)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    training_score <span class="op">=</span> pipeline.score(X_train[:, np.newaxis],y_train)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    X_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label<span class="op">=</span><span class="st">"Model"</span>)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>    plt.plot(X_plot, true_fun(X_plot), <span class="st">'--'</span>,label<span class="op">=</span><span class="st">"True function"</span>)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X, y, label<span class="op">=</span><span class="st">"Samples"</span>)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    plt.xlim((<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    plt.ylim((<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    plt.legend(loc<span class="op">=</span><span class="st">"best"</span>)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Degree </span><span class="sc">{}</span><span class="st">, Alpha </span><span class="sc">{}</span><span class="ch">\n</span><span class="st">Test score = </span><span class="sc">{:.3f}</span><span class="ch">\n</span><span class="st">Training score = </span><span class="sc">{:.3f}</span><span class="st">"</span>.<span class="bu">format</span>(</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>        d, a, testing_score,training_score))</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-11-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-11-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-11-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-11-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-cross_validation_linear_regression-cell-11-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This is a simplified type of cross-validation often referred to as “Shuffle Splitting” and is one of the most common, but it is useful to review other types of cross-validation via <a href="https://scikit-learn.org/stable/modules/cross_validation.html">this nice summary page from the SKLearn library</a>, which covers a variety of important variants including:</p>
<ol type="1">
<li>K-Fold Cross Validation</li>
<li>Leave-One-Out Cross Validation</li>
<li>Stratified Cross Validation</li>
<li>Group-wise Cross Validation</li>
<li>Time Series Split Cross Validation</li>
</ol>
<p><strong>Discussion Point</strong>: Under what conditions or situations would using each type of cross-validation above be appropriate versus inappropriate?</p>
</section>
</div>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/ML4ME_Textbook/ML4ME_Textbook/notebooks/derivative_free_hyper_parameter_optimization.ipynb" data-notebook-title="Overview of Derivative Free Optimization Methods" data-notebook-cellid="cell-0">
<section id="overview-of-derivative-free-optimization-methods" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="overview-of-derivative-free-optimization-methods"><span class="header-section-number">2.5</span> Overview of Derivative Free Optimization Methods</h2>
<p>Now that we have introduce the usage of hyper-parameters and cross-validation, a natural question arises: How do we choose the hyper-parameters? There are many ways to do this, and this section will describe the most common and basic ones, while leaving more advanced techniques (like Implicit Differentiation) for later. Specifically, this section will: 1. Define the concepts of Grid and Random Hyper-parameter search. 2. Use Grid and Random search to optimize hyper-parameters of a model. 2. Distinguish when Randomized Search is much better than grid search. 3. Describe how Global Optimization procedures such as Bayesian Optimization work. 4. Recognize why none of those at all work in High Dimensions and describe the “Curse of Dimensionality”</p>
<div id="cell-1" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>sns.set_context(<span class="st">'poster'</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>pal <span class="op">=</span> sns.color_palette(<span class="st">"Paired"</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> sns.blend_palette(pal,as_cmap<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="co"># True Function we want to estimate</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>true_fun <span class="op">=</span> <span class="kw">lambda</span> X: np.cos(<span class="fl">1.5</span> <span class="op">*</span> np.pi <span class="op">*</span> X)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Noisy Samples from the true function</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.sort(np.random.rand(n_samples))[:, np.newaxis]</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> true_fun(X) <span class="op">+</span> np.random.randn(n_samples)[:, np.newaxis] <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">7</span>))</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the true function:</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>X_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)[:, np.newaxis]</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>plt.plot(X_plot, true_fun(X_plot), <span class="st">'--'</span>,label<span class="op">=</span><span class="st">"True function"</span>)</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data samples</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>plt.scatter(X,y, label<span class="op">=</span><span class="st">"Samples"</span>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"best"</span>)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-2-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="fitting-data-using-polynomial-regression" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="fitting-data-using-polynomial-regression"><span class="header-section-number">2.6</span> Fitting Data using Polynomial Regression</h2>
<p>Unlike where we used K Nearest Neighbors, this time we will fit the data using Polynomial Regression (<span class="math inline">\(y=a+bx+cx^2 + \cdots\)</span>)</p>
<p>In this particular case, we will use something called “Ridge regression” though the specific details of what this is and why it works we will get into next week. For today, the key things to note are that the models we build below have two knobs or “hyper-parameters” that dictate model complexity:</p>
<ol type="1">
<li>The degree of the polynomial curve fit. That is d=1 corresponds to <span class="math inline">\(f(x) = w_1\cdot x^1 = w_1\cdot x\)</span>. d=2 corresponds to <span class="math inline">\(f(x) = w_1\cdot x^1 + w_2\cdot x^2\)</span>, and d=D corresponds to <span class="math inline">\(f(x) = w_1\cdot x^1 + w_2\cdot x^2 + \cdots + w_D\cdot x^D\)</span>. (Note that you could also have an intercept term such as <span class="math inline">\(w_0 + w_1\cdot x\)</span>, but for the below example we have disabled this addition.)</li>
<li>A penalty term called <span class="math inline">\(\alpha\)</span> (more on this next week) which essentially penalizes large weights (<span class="math inline">\(w_1,\cdots, w_D\)</span>). When <span class="math inline">\(\alpha\)</span> is small, the weights can be whatever they want (high complexity function), and when <span class="math inline">\(\alpha\)</span> is high, the weights have to be close to zero (low complexity function–e.g., just a flat line)</li>
</ol>
<p>What we are going to see today is how to pick <span class="math inline">\(d\)</span> and <span class="math inline">\(\alpha\)</span> such that we maximize cross-validation performance. Next week we will dive more into the details of various linear regression models.</p>
<div id="cell-3" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-4" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> model_selection</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's plot the behavior of a fixed degree polynomial</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>degree <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># (i.e., f(x) = w_1*x + w_2*x^2 + ... + w_15*x^15)</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># but where we change alpha.</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.logspace(start<span class="op">=-</span><span class="dv">13</span>,stop<span class="op">=</span><span class="dv">4</span>,num<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>polynomial_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span>degree,</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>                                         include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> []</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a <span class="kw">in</span> alphas:</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    linear_regression <span class="op">=</span> Ridge(alpha<span class="op">=</span>a)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    pipeline <span class="op">=</span> Pipeline([(<span class="st">"polynomial_features"</span>, polynomial_features),</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>                         (<span class="st">"linear_regression"</span>, linear_regression)])</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    cv_scores <span class="op">=</span> model_selection.cross_val_score(pipeline,</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>        X, y, scoring<span class="op">=</span><span class="st">"neg_mean_squared_error"</span>, cv<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    scores.append(cv_scores)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> np.array(scores)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">3</span>))</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>plt.semilogx(alphas,<span class="op">-</span>np.mean(scores,axis<span class="op">=</span><span class="dv">1</span>),<span class="st">'-'</span>)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Test MSE'</span>)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Alpha ($</span><span class="ch">\\</span><span class="st">alpha$)'</span>)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg ansi-bold">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg ansi-bold">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg ansi-bold">In[4], line 7</span>
<span class="ansi-green-fg">      4</span> degree <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">15</span>
<span class="ansi-green-fg">      5</span> <span style="font-style:italic;color:rgb(95,135,135)"># (i.e., f(x) = w_1*x + w_2*x^2 + ... + w_15*x^15)</span>
<span class="ansi-green-fg">      6</span> <span style="font-style:italic;color:rgb(95,135,135)"># but where we change alpha.</span>
<span class="ansi-green-fg ansi-bold">----&gt; 7</span> alphas <span style="color:rgb(98,98,98)">=</span> np<span style="color:rgb(98,98,98)">.</span>logspace(start<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">-</span><span style="color:rgb(98,98,98)">13</span>,stop<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">4</span>,num<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">20</span>)
<span class="ansi-green-fg">      8</span> polynomial_features <span style="color:rgb(98,98,98)">=</span> PolynomialFeatures(degree<span style="color:rgb(98,98,98)">=</span>degree,
<span class="ansi-green-fg">      9</span>                                          include_bias<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">False</span>)
<span class="ansi-green-fg">     10</span> scores <span style="color:rgb(98,98,98)">=</span> []

<span class="ansi-red-fg ansi-bold">NameError</span>: name 'np' is not defined</pre>
</div>
</div>
</div>
</section>
<section id="what-if-we-have-more-than-one-variable" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="what-if-we-have-more-than-one-variable"><span class="header-section-number">2.7</span> What if we have more than one variable?</h2>
<p>Let’s look at both polynomial degree and regularization weight</p>
<div id="cell-6" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> []</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.logspace(start<span class="op">=-</span><span class="dv">13</span>, <span class="co"># Start at 1e-13</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>                     stop<span class="op">=</span><span class="dv">4</span>,    <span class="co"># Stop at 1e4</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>                     num<span class="op">=</span><span class="dv">40</span>)    <span class="co"># Split that into 40 pieces</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>degrees <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">16</span>) <span class="co"># This will only go to 15, due to how range works</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> np.zeros(shape<span class="op">=</span>(<span class="bu">len</span>(degrees), <span class="co"># i.e., 15</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>                         <span class="bu">len</span>(alphas))) <span class="co"># i.e., 20</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, degree <span class="kw">in</span> <span class="bu">enumerate</span>(degrees): <span class="co"># For each degree</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    polynomial_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span>degree,</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>                                             include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j,a <span class="kw">in</span> <span class="bu">enumerate</span>(alphas):    <span class="co"># For each alpha</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        linear_regression <span class="op">=</span> Ridge(alpha<span class="op">=</span>a)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        pipeline <span class="op">=</span> Pipeline([(<span class="st">"polynomial_features"</span>, polynomial_features),</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>                             (<span class="st">"linear_regression"</span>, linear_regression)])</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        cv_scores <span class="op">=</span> model_selection.cross_val_score(pipeline,</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>            X, y, scoring<span class="op">=</span><span class="st">"neg_mean_squared_error"</span>, cv<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        scores[i][j] <span class="op">=</span> <span class="op">-</span>np.mean(cv_scores)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-7" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">7</span>))</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> cm</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>Xs, Ys <span class="op">=</span> np.meshgrid(<span class="bu">range</span>(<span class="bu">len</span>(degrees)), <span class="bu">range</span>(<span class="bu">len</span>(alphas)))</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>zs <span class="op">=</span> np.array([scores[i,j] <span class="cf">for</span> i,j <span class="kw">in</span> <span class="bu">zip</span>(np.ravel(Xs), np.ravel(Ys))])</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>Zs <span class="op">=</span> zs.reshape(Xs.shape)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>Xs, Ys <span class="op">=</span> np.meshgrid(degrees, np.log(alphas))</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>ax.plot_surface(Xs, Ys, Zs, rstride<span class="op">=</span><span class="dv">1</span>, cstride<span class="op">=</span><span class="dv">1</span>, cmap<span class="op">=</span>cm.coolwarm,</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    linewidth<span class="op">=</span><span class="dv">0</span>, antialiased<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Label the Axes</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Degree'</span>)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Regularization'</span>)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>ax.set_zlabel(<span class="st">'MSE'</span>)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Rotate the image</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>ax.view_init(<span class="dv">30</span>, <span class="co"># larger # goes "higher"</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>             <span class="dv">30</span>) <span class="co"># larger # "circles around"</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-8" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">10</span>))</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(Zs,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>           cmap<span class="op">=</span>cm.coolwarm, <span class="co"># Allows you to set the color</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>           vmin<span class="op">=</span>Zs.<span class="bu">min</span>(), vmax<span class="op">=</span><span class="fl">0.2</span>, <span class="co"># The min and max Z-Values (for coloring purposes)</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>           extent<span class="op">=</span>[Xs.<span class="bu">min</span>(), Xs.<span class="bu">max</span>(),   <span class="co"># How far on X-Axis you want to plot</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>                   Ys.<span class="bu">min</span>(), Ys.<span class="bu">max</span>()],  <span class="co"># How far on Y-Axis</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>           interpolation<span class="op">=</span><span class="st">'spline16'</span>,      <span class="co"># How do you want to interpolate values between data?</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>           origin<span class="op">=</span><span class="st">'lower'</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Mean Squared Error'</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Degree'</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Regularization'</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="optimization" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="optimization"><span class="header-section-number">2.8</span> Optimization</h2>
<p>At the end of the day, all this is doing is optimization/search over different parameters.</p>
<p>How should we go about automating this?</p>
<p>Most common: Grid Search.</p>
<div id="cell-10" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'parameters we could change:'</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> pipeline.get_params().keys():</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" "</span>,k)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>parameters we could change:
  memory
  steps
  verbose
  polynomial_features
  linear_regression
  polynomial_features__degree
  polynomial_features__include_bias
  polynomial_features__interaction_only
  polynomial_features__order
  linear_regression__alpha
  linear_regression__copy_X
  linear_regression__fit_intercept
  linear_regression__max_iter
  linear_regression__normalize
  linear_regression__random_state
  linear_regression__solver
  linear_regression__tol</code></pre>
</div>
</div>
<div id="cell-11" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> {<span class="st">'polynomial_features__degree'</span>: <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">16</span>)), <span class="co"># 15 possible</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">'linear_regression__alpha'</span>: np.logspace(start<span class="op">=-</span><span class="dv">13</span>,stop<span class="op">=</span><span class="dv">4</span>,num<span class="op">=</span><span class="dv">10</span>),</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>              <span class="st">'polynomial_features__include_bias'</span>:[<span class="st">'True'</span>, <span class="st">'False'</span>]}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-12" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># How do we want to do cross-validation?</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> model_selection</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>num_data_points <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 4-fold CV</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>kfold_cv <span class="op">=</span> model_selection.KFold(n_splits <span class="op">=</span> <span class="dv">4</span>) </span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Or maybe you want randomized splits?</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>shuffle_cv <span class="op">=</span> model_selection.ShuffleSplit(n_splits <span class="op">=</span> <span class="dv">20</span>,     <span class="co"># How many iterations?</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>                                          test_size<span class="op">=</span><span class="fl">0.2</span>    <span class="co"># What % should we keep for test?</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>                                         )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-13" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(pipeline,    <span class="co"># The thing we want to optimize</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>                           parameters,  <span class="co"># The parameters we will change</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>                           cv<span class="op">=</span>shuffle_cv, <span class="co"># How do you want to cross-validate?</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>                           scoring <span class="op">=</span> <span class="st">'neg_mean_squared_error'</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>                          )</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X, y) <span class="co"># This runs the cross-validation</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>GridSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),
             estimator=Pipeline(steps=[('polynomial_features',
                                        PolynomialFeatures(degree=15,
                                                           include_bias=False)),
                                       ('linear_regression',
                                        Ridge(alpha=10000.0))]),
             param_grid={'linear_regression__alpha': array([1.00000000e-13, 7.74263683e-12, 5.99484250e-10, 4.64158883e-08,
       3.59381366e-06, 2.78255940e-04, 2.15443469e-02, 1.66810054e+00,
       1.29154967e+02, 1.00000000e+04]),
                         'polynomial_features__degree': [1, 2, 3, 4, 5, 6, 7, 8,
                                                         9, 10, 11, 12, 13, 14,
                                                         15],
                         'polynomial_features__include_bias': ['True',
                                                               'False']},
             scoring='neg_mean_squared_error')</code></pre>
</div>
</div>
<div id="cell-14" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>grid_search.best_params_ <span class="co"># Once finished, you can see what the best parameters are</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>{'linear_regression__alpha': 3.5938136638046257e-06,
 'polynomial_features__degree': 5,
 'polynomial_features__include_bias': 'True'}</code></pre>
</div>
</div>
<div id="cell-15" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best MSE for Grid Search: </span><span class="sc">{:.2e}</span><span class="st">"</span>.<span class="bu">format</span>(<span class="op">-</span>grid_search.best_score_))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best MSE for Grid Search: 1.66e-02</code></pre>
</div>
</div>
<div id="cell-16" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>grid_search.predict(X)  <span class="co"># You can also use the best model directly (in sklearn)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>array([[ 1.06097211],
       [ 0.97399126],
       [ 0.9385276 ],
       [ 0.85921625],
       [ 0.78547323],
       [-0.20945195],
       [-0.34742376],
       [-0.38613125],
       [-0.4449367 ],
       [-0.54188637],
       [-0.75804255],
       [-0.78001026],
       [-0.82694219],
       [-0.83783579],
       [-0.88728228],
       [-0.95921482],
       [-1.00926296],
       [-1.01455458],
       [-1.01707599],
       [-0.92313959],
       [-0.91783245],
       [-0.89110789],
       [-0.87185274],
       [-0.7708306 ],
       [-0.63240106],
       [-0.54107778],
       [-0.38653015],
       [-0.29425304],
       [-0.19997607],
       [-0.12483163]])</code></pre>
</div>
</div>
<div id="cell-17" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>best_degree <span class="op">=</span> grid_search.best_params_[<span class="st">'polynomial_features__degree'</span>]</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> grid_search.best_params_[<span class="st">'linear_regression__alpha'</span>]</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>))</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>plt.plot(X_plot, grid_search.predict(X_plot),<span class="st">'-'</span>,label<span class="op">=</span><span class="st">"Model"</span>,alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>plt.plot(X_plot, true_fun(X_plot), <span class="st">':'</span>,label<span class="op">=</span><span class="st">"True function"</span>,alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>plt.scatter(X,y, c<span class="op">=</span><span class="st">'Blue'</span>, s<span class="op">=</span><span class="dv">20</span>, edgecolors<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>plt.xlim((<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Degree </span><span class="sc">{}</span><span class="st">, Alpha </span><span class="sc">{:.1e}</span><span class="st">"</span>.<span class="bu">format</span>(best_degree,best_alpha))</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="randomized-search" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="randomized-search"><span class="header-section-number">2.9</span> Randomized Search</h2>
<p>In reality, grid search is wasteful and not easy to control. A better (and still easy way) is to randomize the search.</p>
<div id="cell-19" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Now, instead of specifying exact which points to test, we instead</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="co"># have to specify a distribution to sample from.</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co"># For example, things from http://docs.scipy.org/doc/scipy/reference/stats.html</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> randint <span class="im">as</span> sp_randint</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> lognorm <span class="im">as</span> sp_lognorm</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV, RandomizedSearchCV</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> {<span class="st">'polynomial_features__degree'</span>: sp_randint(<span class="dv">1</span>,<span class="dv">20</span>), <span class="co"># We want an integer</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>              <span class="st">'linear_regression__alpha'</span>: sp_lognorm(<span class="dv">1</span>),</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>              <span class="st">'polynomial_features__include_bias'</span>:[<span class="st">'True'</span>, <span class="st">'False'</span>]} <span class="co"># Selecting from two is fine</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Need something whose logarithmic distribution we can control. How about a lognormal? <span class="math display">\[
\mathcal{N}(\ln x;\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left[-\frac {(\ln x - \mu)^2} {2\sigma^2}\right].
\]</span></p>
<div id="cell-21" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>sigma<span class="op">=</span><span class="dv">6</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>rv <span class="op">=</span> sp_lognorm(sigma,scale<span class="op">=</span><span class="fl">1e-7</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>plt.hist(rv.rvs(size<span class="op">=</span><span class="dv">1000</span>),bins<span class="op">=</span>np.logspace(<span class="op">-</span><span class="dv">20</span>, <span class="dv">2</span>, <span class="dv">22</span>))</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-22" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> {<span class="st">'polynomial_features__degree'</span>: sp_randint(<span class="dv">1</span>,<span class="dv">20</span>), <span class="co"># We want an integer</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>              <span class="st">'linear_regression__alpha'</span>: sp_lognorm(sigma,scale<span class="op">=</span><span class="fl">1e-7</span>),</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">'polynomial_features__include_bias'</span>:[<span class="st">'True'</span>, <span class="st">'False'</span>]} <span class="co"># Selecting from two is fine</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-23" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fitting the high degree polynomial makes the linear system almost</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># singular, which makes Numpy issue a Runtime warning.</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co"># This is not a problem here, except that it pops up the warning box</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co"># So I will disable it just for pedagogical purposes</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>warnings.simplefilter(<span class="st">'ignore'</span>,<span class="pp">RuntimeWarning</span>)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="co"># specify parameters and distributions to sample from</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> RandomizedSearchCV</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="co"># run randomized search</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="co">#n_iter_search = 300 # How many random parameter settings should we try?</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>n_iter_search <span class="op">=</span> <span class="bu">len</span>(grid_search.cv_results_[<span class="st">'params'</span>]) <span class="co"># Give it same # as grid search, to be fair</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>random_search <span class="op">=</span> RandomizedSearchCV(pipeline,</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>                                   param_distributions<span class="op">=</span>parameters,</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>                                   n_iter<span class="op">=</span>n_iter_search, </span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>                                   cv<span class="op">=</span>shuffle_cv, <span class="co"># How do you want to cross-validate?</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>                                   scoring <span class="op">=</span> <span class="st">'neg_mean_squared_error'</span>)</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>random_search.fit(X, y)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>RandomizedSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),
                   estimator=Pipeline(steps=[('polynomial_features',
                                              PolynomialFeatures(degree=15,
                                                                 include_bias=False)),
                                             ('linear_regression',
                                              Ridge(alpha=10000.0))]),
                   n_iter=300,
                   param_distributions={'linear_regression__alpha': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001C8A6C0E1C8&gt;,
                                        'polynomial_features__degree': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001C8A69A3E08&gt;,
                                        'polynomial_features__include_bias': ['True',
                                                                              'False']},
                   scoring='neg_mean_squared_error')</code></pre>
</div>
</div>
<div id="cell-24" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>random_search.best_params_ <span class="co"># Once finished, you can see what the best parameters are</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>{'linear_regression__alpha': 0.00042015290705956784,
 'polynomial_features__degree': 7,
 'polynomial_features__include_bias': 'False'}</code></pre>
</div>
</div>
<div id="cell-25" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best MSE for Random Search: </span><span class="sc">{:.2e}</span><span class="st">"</span>.<span class="bu">format</span>(<span class="op">-</span>random_search.best_score_))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best MSE for Random Search: 1.56e-02</code></pre>
</div>
</div>
<div id="cell-26" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>best_degree <span class="op">=</span> random_search.best_params_[<span class="st">'polynomial_features__degree'</span>]</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> random_search.best_params_[<span class="st">'linear_regression__alpha'</span>]</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>))</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>plt.plot(X_plot, random_search.predict(X_plot),<span class="st">'-'</span>,label<span class="op">=</span><span class="st">"Model"</span>,alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>plt.plot(X_plot, true_fun(X_plot), <span class="st">':'</span>,label<span class="op">=</span><span class="st">"True function"</span>,alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>plt.scatter(X,y, c<span class="op">=</span><span class="st">'Blue'</span>, s<span class="op">=</span><span class="dv">20</span>, edgecolors<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>plt.xlim((<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Degree </span><span class="sc">{}</span><span class="st">, Alpha </span><span class="sc">{:.1e}</span><span class="st">"</span>.<span class="bu">format</span>(best_degree,best_alpha))</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-22-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="global-bayesian-optimization" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="global-bayesian-optimization"><span class="header-section-number">2.10</span> Global Bayesian Optimization</h2>
<p>Surely, since we are essentially doing optimization, we could approach hyper-parameter selection as an optimization problem as well, right?</p>
<p>Enter techniques like Global Bayesian Optimization below:</p>
<div id="cell-28" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""The function to predict."""</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="op">*</span> np.sin(x)</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Try others!</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">#return 5 * np.sinc(x)</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">#return x</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.atleast_2d(np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>)).T</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Observations</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> f(X).ravel()</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>plt.plot(X,y)</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-29" class="cell" data-scrolled="false" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co">########################################################</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="co"># This is just a helper function, no need to worry about</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="co"># The internals.</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># We will return to this example in Week 14</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="co">########################################################</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.gaussian_process <span class="im">import</span> GaussianProcessRegressor</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.gaussian_process.kernels <span class="im">import</span> RBF, ConstantKernel <span class="im">as</span> C</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Mesh the input space for evaluations of the real function, the prediction and</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="co"># its MSE</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.atleast_2d(np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">1000</span>)).T</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Gaussian Process model</span></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a><span class="co">#kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>kernel <span class="op">=</span> C(<span class="fl">1.0</span>, (<span class="fl">1e-3</span>, <span class="fl">1e3</span>)) <span class="op">*</span> RBF(<span class="dv">10</span>, (<span class="fl">1e-2</span>, <span class="fl">1e2</span>))</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a><span class="co">#gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>kernel <span class="op">=</span> C(<span class="fl">3.0</span>)<span class="op">*</span>RBF(<span class="fl">1.5</span>)</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>gp <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kernel,alpha<span class="op">=</span><span class="fl">1e-6</span>,optimizer<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a><span class="co">#gp = GaussianProcess(corr='cubic', theta0=1e-2, thetaL=1e-4, thetaU=1e-1,random_start=100)</span></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Now, ready to begin learning:</span></span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>train_ind <span class="op">=</span>{</span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Upper CB'</span>:   np.zeros(<span class="bu">len</span>(X),dtype<span class="op">=</span><span class="bu">bool</span>),</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Random'</span>:np.zeros(<span class="bu">len</span>(X),dtype<span class="op">=</span><span class="bu">bool</span>)</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>options <span class="op">=</span> train_ind.keys()</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>possible_points <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(X))))</span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Possible Initialization options</span></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Select different points randomly</span></span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a><span class="co">#for i in range(2):</span></span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a><span class="co">#    for o in options:</span></span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a><span class="co">#        ind = np.random.choice(possible_points[~train_ind[o]],1)</span></span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a><span class="co">#        train_ind[o][ind] = True</span></span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Start with end-points</span></span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a><span class="co">#for o in options:</span></span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a><span class="co">#    train_ind[o][0] = True</span></span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a><span class="co">#    train_ind[o][-1] = True</span></span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Start with same random points</span></span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ind <span class="kw">in</span> np.random.choice(possible_points,<span class="dv">2</span>):</span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> o <span class="kw">in</span> options:</span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a>        train_ind[o][ind] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a>plot_list <span class="op">=</span> np.array([<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">20</span>,<span class="dv">30</span>,<span class="dv">40</span>,<span class="dv">50</span>,<span class="bu">len</span>(X)])</span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># As i increases, we increase the number of points</span></span>
<span id="cb46-51"><a href="#cb46-51" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">6</span>))</span>
<span id="cb46-52"><a href="#cb46-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j,o <span class="kw">in</span> <span class="bu">enumerate</span>(options):</span>
<span id="cb46-53"><a href="#cb46-53" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">1</span>,<span class="dv">2</span>,j<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb46-54"><a href="#cb46-54" aria-hidden="true" tabindex="-1"></a>        gp.fit(X[train_ind[o],:],y[train_ind[o]])</span>
<span id="cb46-55"><a href="#cb46-55" aria-hidden="true" tabindex="-1"></a>        yp,sigma <span class="op">=</span> gp.predict(X[<span class="op">~</span>train_ind[o],:], return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-56"><a href="#cb46-56" aria-hidden="true" tabindex="-1"></a>        ucb <span class="op">=</span> yp <span class="op">+</span> <span class="fl">1.96</span><span class="op">*</span>sigma</span>
<span id="cb46-57"><a href="#cb46-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> o <span class="kw">is</span> <span class="st">'Upper CB'</span>:</span>
<span id="cb46-58"><a href="#cb46-58" aria-hidden="true" tabindex="-1"></a>            <span class="co">#candidates = np.extract(MSE == np.amax(MSE),X[~train_ind[o],:])</span></span>
<span id="cb46-59"><a href="#cb46-59" aria-hidden="true" tabindex="-1"></a>            candidates <span class="op">=</span> np.extract(ucb <span class="op">==</span> np.amax(ucb),X[<span class="op">~</span>train_ind[o],:])</span>
<span id="cb46-60"><a href="#cb46-60" aria-hidden="true" tabindex="-1"></a>            next_point <span class="op">=</span> np.random.choice(candidates.flatten())</span>
<span id="cb46-61"><a href="#cb46-61" aria-hidden="true" tabindex="-1"></a>            next_ind <span class="op">=</span> np.argwhere(X.flatten() <span class="op">==</span> next_point)</span>
<span id="cb46-62"><a href="#cb46-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> o <span class="kw">is</span> <span class="st">'Random'</span>:</span>
<span id="cb46-63"><a href="#cb46-63" aria-hidden="true" tabindex="-1"></a>            next_ind <span class="op">=</span> np.random.choice(possible_points[<span class="op">~</span>train_ind[o]],<span class="dv">1</span>)</span>
<span id="cb46-64"><a href="#cb46-64" aria-hidden="true" tabindex="-1"></a>        train_ind[o][next_ind] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb46-65"><a href="#cb46-65" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-66"><a href="#cb46-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Plot intermediate results</span></span>
<span id="cb46-67"><a href="#cb46-67" aria-hidden="true" tabindex="-1"></a>        yp,sigma <span class="op">=</span> gp.predict(x, return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-68"><a href="#cb46-68" aria-hidden="true" tabindex="-1"></a>        plt.fill(np.concatenate([x, x[::<span class="op">-</span><span class="dv">1</span>]]),</span>
<span id="cb46-69"><a href="#cb46-69" aria-hidden="true" tabindex="-1"></a>                np.concatenate([yp <span class="op">-</span> <span class="fl">1.9600</span> <span class="op">*</span> sigma,</span>
<span id="cb46-70"><a href="#cb46-70" aria-hidden="true" tabindex="-1"></a>                               (yp <span class="op">+</span> <span class="fl">1.9600</span> <span class="op">*</span> sigma)[::<span class="op">-</span><span class="dv">1</span>]]),<span class="st">'b'</span>,</span>
<span id="cb46-71"><a href="#cb46-71" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.05</span>,  ec<span class="op">=</span><span class="st">'g'</span>, label<span class="op">=</span><span class="st">'95</span><span class="sc">% c</span><span class="st">onfidence interval'</span>)</span>
<span id="cb46-72"><a href="#cb46-72" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-73"><a href="#cb46-73" aria-hidden="true" tabindex="-1"></a>        n_train <span class="op">=</span> np.count_nonzero(train_ind[o])</span>
<span id="cb46-74"><a href="#cb46-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-75"><a href="#cb46-75" aria-hidden="true" tabindex="-1"></a>        gp.fit(X[train_ind[o],:],y[train_ind[o]])</span>
<span id="cb46-76"><a href="#cb46-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Show progress</span></span>
<span id="cb46-77"><a href="#cb46-77" aria-hidden="true" tabindex="-1"></a>        yp,sigma <span class="op">=</span> gp.predict(x, return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-78"><a href="#cb46-78" aria-hidden="true" tabindex="-1"></a>        yt <span class="op">=</span> f(x)</span>
<span id="cb46-79"><a href="#cb46-79" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> np.linalg.norm(yp<span class="op">-</span>yt.flatten())</span>
<span id="cb46-80"><a href="#cb46-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-81"><a href="#cb46-81" aria-hidden="true" tabindex="-1"></a>        plt.fill(np.concatenate([x, x[::<span class="op">-</span><span class="dv">1</span>]]),</span>
<span id="cb46-82"><a href="#cb46-82" aria-hidden="true" tabindex="-1"></a>                np.concatenate([yp <span class="op">-</span> <span class="fl">1.9600</span> <span class="op">*</span> sigma,</span>
<span id="cb46-83"><a href="#cb46-83" aria-hidden="true" tabindex="-1"></a>                               (yp <span class="op">+</span> <span class="fl">1.9600</span> <span class="op">*</span> sigma)[::<span class="op">-</span><span class="dv">1</span>]]),<span class="st">'b'</span>,</span>
<span id="cb46-84"><a href="#cb46-84" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.3</span>,  ec<span class="op">=</span><span class="st">'None'</span>, label<span class="op">=</span><span class="st">'95</span><span class="sc">% c</span><span class="st">onfidence interval'</span>)</span>
<span id="cb46-85"><a href="#cb46-85" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-86"><a href="#cb46-86" aria-hidden="true" tabindex="-1"></a>        plt.plot(x,yt,<span class="st">'k--'</span>,alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb46-87"><a href="#cb46-87" aria-hidden="true" tabindex="-1"></a>        plt.plot(x,yp,<span class="st">'r-'</span>,alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb46-88"><a href="#cb46-88" aria-hidden="true" tabindex="-1"></a>        plt.scatter(X[train_ind[o],:],y[train_ind[o]],color<span class="op">=</span><span class="st">'g'</span>,s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb46-89"><a href="#cb46-89" aria-hidden="true" tabindex="-1"></a>        plt.scatter(X[next_ind,:].flatten(),y[next_ind].flatten(),color<span class="op">=</span><span class="st">'r'</span>,s<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb46-90"><a href="#cb46-90" aria-hidden="true" tabindex="-1"></a>        plt.ylim([<span class="op">-</span><span class="dv">10</span>,<span class="dv">15</span>])</span>
<span id="cb46-91"><a href="#cb46-91" aria-hidden="true" tabindex="-1"></a>        plt.xlim([<span class="dv">0</span>,<span class="dv">10</span>])</span>
<span id="cb46-92"><a href="#cb46-92" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"</span><span class="sc">%s</span><span class="ch">\n</span><span class="sc">%d</span><span class="st"> training points</span><span class="ch">\n</span><span class="sc">%.2f</span><span class="st"> error"</span><span class="op">%</span>(o,n_train,error))</span>
<span id="cb46-93"><a href="#cb46-93" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-24-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-24-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-24-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-24-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-24-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-24-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-24-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-24-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-24-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-24-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-curse-of-dimensionality" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="the-curse-of-dimensionality"><span class="header-section-number">2.11</span> The Curse of Dimensionality</h2>
<p>Discuss on board examples of the Curse of Dimensionality and how it affects algorithms dependent on calculating distances.</p>
<ul>
<li>Space-filling properties of inscribed hyper-cube</li>
<li>Distance ratio between min and max distances</li>
<li>Effects on nearest neighbor graphs</li>
<li>Effects on Gaussian Density</li>
</ul>
<div id="cell-31" class="cell" data-execution_count="27">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> math <span class="im">import</span> gamma</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>V_sphere <span class="op">=</span> <span class="kw">lambda</span> d: np.pi<span class="op">**</span>(d<span class="op">/</span><span class="fl">2.0</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>V_cube <span class="op">=</span> <span class="kw">lambda</span> d: d<span class="op">*</span><span class="dv">2</span><span class="op">**</span>(d<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>gamma(d<span class="op">/</span><span class="fl">2.0</span>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>volume_ratio <span class="op">=</span> <span class="kw">lambda</span> d: V_sphere(d)<span class="op">/</span>V_cube(d)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">50</span>)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>ratio <span class="op">=</span> [volume_ratio(i) <span class="cf">for</span> i <span class="kw">in</span> d]</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">10</span>))</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>plt.plot(d,ratio)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>plt.semilogy(d,ratio)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Ratio of Hyper-Sphere Vol. to Hyper-Cube Vol."</span>)</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of Dimensions"</span>)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co">: Add distance min/max example</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="evaluating_models_files/figure-html/..-notebooks-derivative_free_hyper_parameter_optimization-cell-25-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</div>
<section id="sec-diagonstic-plots" class="level2" data-number="2.12">
<h2 data-number="2.12" class="anchored" data-anchor-id="sec-diagonstic-plots"><span class="header-section-number">2.12</span> Useful Plots and Diagnostics for ML Models</h2>
<p>(To Be Written)</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part1/part1.html" class="pagination-link" aria-label="Foundational Skills">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Foundational Skills</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notebooks/supervised_linear_models.html" class="pagination-link" aria-label="Introduction to Linear Regression and Gradient Descent">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Linear Regression and Gradient Descent</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Machine Learning for Mechanical Engineers © 2025 by <a href="./index.qmd#sec-contributors">Mark Fuge and IDEAL Lab Contributors</a> is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>