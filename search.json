[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning for Mechanical Engineering",
    "section": "",
    "text": "Preface\nThis is an open textbook to accompany my course notes for “Machine Learning for Mechanical Engineering” at ETH Zürich in the Department of Mechanical and Process Engineering (D-MAVT).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#runnable-and-interactive-code-elements",
    "href": "index.html#runnable-and-interactive-code-elements",
    "title": "Machine Learning for Mechanical Engineering",
    "section": "Runnable and Interactive Code Elements",
    "text": "Runnable and Interactive Code Elements\nMany of the elements of this book involve runnable code elements in Python, or interactive experiments that you can conduct or visualize while you are reading. These will appear largely static in the rendered book, since some may have long run-times, but you can download and run the corresponding notebook in a browser-based environment (e.g., CoLab) to be able to run the experiments. Most experiments in the book are designed to be run in class in the span of a few minutes, so browser-based environments should work well for this.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#callouts-with-notes-and-experiments",
    "href": "index.html#callouts-with-notes-and-experiments",
    "title": "Machine Learning for Mechanical Engineering",
    "section": "Callouts with Notes and Experiments",
    "text": "Callouts with Notes and Experiments\nThrough the book, I will occasionally use callouts like the one demonstrated below for a couple of use cases:\n\n\n\n\n\n\nNoteExample Collapsed Note, e.g., for a Derivation\n\n\n\n\n\nHere is what a note with a collapsable block structure will look like. We will also use this type of structure to hide details of long derivations, either so you can work them out on your own first, or because the details are not immediately central to following what comes next in the chapter or lecture.\n\n\n\n\n\n\n\n\n\nTipExample Experiment\n\n\n\nHere is an example where we might pose an experiment or task for you to do on your own or in class. Often wrestling with these experiments is a good way to build intuitive understanding and integrate some of the material into practice. When you see this type of callout, you should pause and do the exercise at this point in the chapter, rather than reading ahead, as it may build intuition which is useful later in the chapter.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part1/part1.html",
    "href": "part1/part1.html",
    "title": "Foundational Skills",
    "section": "",
    "text": "This part of the book covers useful foundational skills that should serve you well regardless of which model is State-of-the-Art at the time. We will use them throughout the book, but it is helpful to have some material and exercises together in a cohesive whole.",
    "crumbs": [
      "Foundational Skills"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html",
    "href": "part1/reviewing_supervised_linear_models.html",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "",
    "text": "1.1 What is a Linear Model?\nWe will start by reviewing Linear Models, since they are likely already familiar to you from prior coursework, are widely used, and will serve as a useful (if simple) launching off point for later discussions of more advanced techniques. So, while some of what we will explore in this section might seem pretty basic at first glance, do not let it’s simplicity fool you, as we will revisit similar concepts throughout the rest of the notes, as these concepts will help you form a strong foundation that will serve us well once things get more complex.\nThis chapter will do this in three parts:\nWith this as a baseline model, the next chapter will review the concept of Cross-Validation and how we evaluate whether an Machine Learning model is “good”, and then the subsequent chapter will review (Stochastic) Gradient Descent, in the Linear Model context.\nLet’s start by trying to fit a model to the (admittedly simple) dataset below, where I have just sampled some (noisy) points from a periodic function:\nCode\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import SGDClassifier\nfrom ipywidgets import interact,interact_manual, FloatSlider\nfrom sklearn.datasets import make_regression\nimport seaborn as sns\nsns.set_context('poster')\nnp.random.seed(1)\n\n# Number of data points\nn_samples = 30\n\n# True Function we want to estimate\ntrue_fun = lambda X: np.cos(1.5 * np.pi * X)\n\n# Noisy Samples from the true function\nX = np.sort(np.random.rand(n_samples))\ny = true_fun(X) + np.random.randn(n_samples) * 0.1\n\nplt.figure(figsize=(10,10))\n# Plot the true function:\nX_plot = np.linspace(0, 1, 100)\nplt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n# Plot the data samples\nplt.scatter(X,y, label=\"Samples\")\nplt.legend(loc=\"best\")\nplt.show()\nIf we wanted to fit a line to this data, we would use what is called a linear model:\n\\[\ny = w_0+w_1\\cdot x\n\\]\nwhere \\(w_0\\) is the intercept and \\(w_1\\) is the slope of the line. We can write this more compactly using vector notation as: \\[\ny = \\mathbf{w}^T \\mathbf{x}\n\\] where w is the weight vector [\\(w_0\\), \\(w_1\\)] and \\(x\\) is the feature vector [1, x]. We can see here that taking the dot product between \\(w\\) and \\(x\\) is equivalent to the equation above. Importantly, even though the above equation represents a straight line with respect to x, we are not limited to using linear models only for this. For example, we could make:\n\\[\n\\mathbf{w} = [w_0, w_1, w_2, w_3];\\quad \\mathbf{x} = [1, x, x^2, x^3]\n\\]\nand in this way, we can model y as a cubic function of x, while \\(y = \\mathbf{w}^T \\mathbf{x}\\) remains a “linear model”, since it is still linear with respect to the weights \\(w\\). This is quite powerful, since by adding features (i.e., additional concatentated entries) to \\(\\mathbf{x}\\), we can fit functions that are apparently non-linear with respect to the original input variable \\(x\\), but will possess many useful properties of linear models that we will discuss later (e.g., convexity with respect to \\(w\\)).\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\n\n# Here is a list of different degree polynomials to try out\ndegrees = [1,2,3,5,10,15,20,30]\n\n# Generate samples of the true function + noise\nX = np.sort(np.random.rand(n_samples))\nnoise_amount = 0.1\ny = true_fun(X) + np.random.randn(n_samples) * noise_amount\n\n# For each of the different polynomial degrees we listed above\nfor d in degrees:\n    plt.figure(figsize=(7, 7)) # Make a new figure\n    # Construct the polynomial features\n    polynomial_features = PolynomialFeatures(degree=d,\n                                             include_bias=False)\n    # Construct linear regression model\n    linear_regression = LinearRegression()\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    # Now fit the data first through the \n    # polynomial basis, then do regression\n    pipeline.fit(X[:, np.newaxis], y)\n    \n    # Get the accuracy score of the trained model\n    # on the original training data\n    score = pipeline.score(X[:, np.newaxis],y)\n\n    # Plot the results\n    X_plot = np.linspace(0, 1, 100)\n    plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label=\"Model\")\n    plt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n    plt.scatter(X, y, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    \n    # Print the polynomial degree and the training\n    # accuracy in the title of the graph\n    plt.title(\"Degree {}\\nTrain score = {:.3f}\".format(\n        d, score))\n    plt.show()\nWe can see that even though we are fitting a linear model every time, the behavior with respect to x is markedly non-linear. Moreover, we see some strange behavior as we increase the polynomial degree. What is going on here and why is it behaving in this way? To build some intuition, we can take a look at the learned weight coefficients, by accessing the coef_ attribute of the fitted model:\nCode\nlinear_regression.coef_\n\n\narray([ 4.29e+04, -6.41e+05,  4.46e+06, -5.47e+06, -1.61e+08,  1.50e+09,\n       -7.03e+09,  2.01e+10, -3.46e+10,  2.83e+10,  9.52e+09, -3.50e+10,\n        5.36e+08,  3.61e+10,  3.50e+09, -3.63e+10, -1.74e+10,  2.94e+10,\n        3.42e+10, -1.02e+10, -4.27e+10, -1.74e+10,  3.46e+10,  4.11e+10,\n       -1.48e+10, -5.44e+10,  1.25e+09,  6.96e+10, -5.11e+10,  1.14e+10])\nWhat is going on here? To understand this, we need to understand something about how the model is optimizing error and how we might control this behavior.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#what-is-a-linear-model",
    "href": "part1/reviewing_supervised_linear_models.html#what-is-a-linear-model",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "",
    "text": "TipExperiment: Effect of Polynomial Degree on Model Fit\n\n\n\nWe can see this in the below experiment, where we fit progressively higher order polynomial functions to the above training data. In each case we are fitting a linear model, by virtue of appending additional polynomial features to x. When you do this experiment, ask yourself:\n\nHow does the model fit change as we increase the polynomial degree?\nWhat happens to the training score (\\(R^2\\)) as we increase the polynomial degree?\nDoes this increase in the training score reflect what you intuitively expect? Why or why not?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#regularization-and-controlling-model-complexity",
    "href": "part1/reviewing_supervised_linear_models.html#regularization-and-controlling-model-complexity",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.2 Regularization and Controlling Model Complexity",
    "text": "1.2 Regularization and Controlling Model Complexity\nIn the base case above, all the model is trying to do is minimize the Mean Squared Error (MSE) with respect to the training error, and with a sufficient number of parameters (polynomial features, in this case), it becomes possible to always achieve perfect accuracy (i.e., fit every point) in the training data. Unfortunately, as we can see, fitting the training data perfectly does not necessarily lead to a model that generalizes well to new data. To help us control this behavior, we can introduce the concept of Regularization, which is a way of penalizing overly complex models. Specifically, we can modify our cost function to be something like:\n\\[\nCost = Loss(w,D) + \\Omega(w)\n\\]\nwhere \\(\\Omega(w)\\) represents what we call a “Regularization” of the function or a “Penalty Term” The purpose of \\(\\Omega(w)\\) is to help us prevent the (otherwise complex) model from being overly complicated, by penalizing this complexity. There are many ways to do this that we will see later on, but one common way to do this for linear models is to penalize the total weight that you allow all of the \\(w_i\\) to have. Specifically how one calculates this total weight turns out to matter a lot, and we shall see it return in later chapters and sections. But to get us started in un-packing how to do this, we first need to talk about what a Norm is, how it relates to Linear Regression weights, and how it helps us perform Regularization.\n\n1.2.1 Norms and their relationship to Regularization\nA Norm is a concept in mathematics that allows us to essentially measure length or size, typically of vectors. Any time you have tried to compute the distance between two points in space (say, by using the Pythagorean Theorem), or the magnitude of an applied Force vector, you have been using a Norm — most likely the Euclidean Norm or Euclidean Distance. For example, for a vector \\(\\mathbf{x}\\) with \\(n\\) dimensions, the Euclidean Norm looks like this: \\[\n||\\mathbf{x}||_2 = \\sqrt{ x_1^2 + x_2^2 + \\cdots x_n^2 }\n\\] If you have ever had to compute the total Force Magnitude given its x and y components (for example, in Statics class), you have used the Euclidean Norm to do so. In that context, it served to take multiple components of a Force aggregate them in such a way as to tell you something about the total force – by analogy, we will do the same thing here with linear regression, where each weight is like a component and we can use the Euclidean Norm to compute the total weight.\nWhile Euclidean Norms may be quite useful or familiar to Engineers, it turns out that they are a special case of a much wider family of Norms called p-norms, which are defined as: \\[\n||\\mathbf{x}||_p = \\left(|x_1|^p + |x_2|^p+\\cdots + |x_n|^p\\right)^{1/p} = \\left(\\sum_{i=1}^n \\left| x_i \\right|^p \\right)^p\n\\]\nSpecifically, the Euclidean Norm is called the L2-norm, or sometimes just the 2-norm. To see why this is, just set \\(p=2\\) in the above, and note how it corresponds to the Euclidean Norm that we all know and love. So, by setting \\(p\\) to a number between \\(0\\) and \\(\\infty\\), we can modify what the total weight means, and setting \\(p=2\\) is the setting which we are all most familiar with. To get a visual sense of how norms vary, see below, which visualizes a line of “circle” of radius 1, but where the length of the line is determined by the p-norm. You will see that when p=2 this corresponds to what we are familiar with, but when p goes up or down things change.\n\n\nCode\nfrom ipywidgets import interact, FloatSlider\n\n# Define a function to compute and plot the p-norm unit ball in 2D\ndef plot_p_norm(p=2.0):\n    # Avoid invalid p values\n    if p &lt;= 0:\n        print(\"p must be &gt; 0\")\n        return\n    \n    theta = np.linspace(0, 2*np.pi, 400)\n    # Parametric form of p-norm unit circle\n    x = np.cos(theta)\n    y = np.sin(theta)\n    \n    # Normalize to p-norm = 1\n    denom = (np.abs(x)**p + np.abs(y)**p)**(1/p)\n    x_unit = x / denom\n    y_unit = y / denom\n    \n    plt.figure(figsize=(5,5))\n    plt.plot(x_unit, y_unit, label=f'p = {p:.2f}')\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.title(f'Unit Ball in p-norm (p={p:.2f})')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Interactive slider for p\ninteract(\n    plot_p_norm,\n    p=FloatSlider(value=2.0, min=0.1, max=10.0, step=0.1, description='p')\n)\n\n\n\n\n\n&lt;function __main__.plot_p_norm(p=2.0)&gt;\n\n\nFor today, we will just focus on the L2-Norm, however we will revist norms again later where we will see how changing the one we are using can have positive or negative effects in certain circumstances.\nWe will use the L2-Norm to help us penalize having linear regression models with really large weights, by essentially putting a cost on the total weight of the weight vector, where the total is measured by the L2-Norm. That is: \\[\nCost = \\sum_{n=1}^{N}||y-w\\cdot x||^2 + \\alpha \\cdot ||w||^2\n\\] Where \\(\\alpha\\) is the price we pay for including more weight in the linear model. If it reduces our error cost enough to offset the cost of the increased weight, then that may be worth it to us. Otherwise, we would err on the side of using less weight overall.\nThis Regularization (i.e., increasing \\(\\alpha\\)) essentially allows you to trade off bias and variance, as we will see below.\n\n\n\n\n\n\nTipExperiment: Effect of Increasing Regularization Strength on Polynomial Fit\n\n\n\nIn the below experiment, we will increase the regularization strength (\\(\\alpha\\)) for a fixed 15-degree polynomial, and observe its effect on the overfitting problem before. Consider the following questions as you observe the experiment below:\n\nWhat happens when we set \\(\\alpha\\) to a low (close to zero) value?\nWhat happens when we set \\(\\alpha\\) to a high value?\n\n\n\n\n\nCode\n# Import Ridge Regression\nfrom sklearn.linear_model import Ridge\n\n# alpha determines how much of \n# a penalty the weights incur\nalphas = [0, 1e-20, 1e-10, 1e-7, 1e-5, 1, 10, 100]\n\n# For the below example, let's\n# just consider a 15-degree polynomial\nd=15\nnp.random.seed(100)\n\nfor a in alphas:\n    plt.figure(figsize=(7, 7))\n    polynomial_features = PolynomialFeatures(degree=d,\n                                             include_bias=False)\n    #linear_regression = LinearRegression() #&lt;- Note difference with next line\n    linear_regression = Ridge(alpha=a)\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    # Fit model\n    pipeline.fit(X[:, np.newaxis], y)\n    # Get Training Accuracy\n    score = pipeline.score(X[:, np.newaxis],y)\n\n    # Plot things\n    X_plot = np.linspace(0, 1, 100)\n    plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label=\"Model\")\n    plt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n    plt.scatter(X, y, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    \n    plt.title(\"Degree={}, $\\\\alpha$={}\\nTrain score = {:.3f}\".format(\n        d, a, score))\n    plt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#examples-of-other-commonly-used-loss-functions-in-linear-models",
    "href": "part1/reviewing_supervised_linear_models.html#examples-of-other-commonly-used-loss-functions-in-linear-models",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.3 Examples of Other Commonly used Loss Functions in Linear Models",
    "text": "1.3 Examples of Other Commonly used Loss Functions in Linear Models\nThus far we have been discussing Linear Models in their most familiar context — minimizing the Mean Squared Error (MSE) with respect to the training data, optionally with an L2 regularization on the weight vector:\n\\[\n\\mathcal{L}(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 + \\alpha ||\\mathbf{w}||_2^2\n\\]\nFor now, let us ignore the regularization term, and just focus on the Loss term. Why should we minimize the squared error?1 Why not the absolute error or other possible loss functions? Let’s explore a few of those options and then see, in practice, how they affect the learned linear model.\n1 It turns out that there are good theoretical reasons for this, for example, that a Linear Model trained via an L2/MSE Loss is the Best Linear Unbiased Estimate (BLUE) of the Linear Model, according to the Gauss-Markov Theorem, but, as we will see, there are other reasons to forgo these advantages.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#loss-functions-for-regression",
    "href": "part1/reviewing_supervised_linear_models.html#loss-functions-for-regression",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.4 Loss Functions for Regression",
    "text": "1.4 Loss Functions for Regression\nBeyond classical MSE, there are two main types of variants that are commonly used for regression problems:\n\nRobust Loss Functions, that minimize the quadratic effect of the MSE loss for very large errors. These are typically used to make the trained model less sensitive to outliers in the training data.\nEpsilon-Insensitive Loss functions, that ignore errors that are sufficiently small (within an \\(\\epsilon\\) margin). These are typically used to incur some advantages in terms of sparsity in the learned model (for example, in Support Vector Methods, which we will see later).\n\nIn reality, these two variants can be combined in different ways, which we will see reflected below, but as a summary, these are:\n\nAbsolute Loss: \\(|y - \\hat{y}|\\)\nHuber Loss: A squared loss for small errors, and then transitioning to an absolute loss for large errors.\nEpsilon-Insensitive Loss: A loss that is zero for errors up to \\(\\epsilon\\) and then uses absolute loss for larger errors.\nSquared Epsilon-Insensitive Loss: Similar to Epsilon-Insensitive Loss, but uses squared loss for errors larger than \\(\\epsilon\\).\n\nOf course, you could imagine more complex variants and combinations of these properties, but these capture the main properties and benefits that we will see below.\n\n\nCode\ndef modified_huber_loss(y):\n    if(abs(y)&lt;1):\n        return y**2\n    else:\n        return 2*abs(y)-1\nmhuber = np.vectorize(modified_huber_loss)\n\neps = 0.7\ndef sq_esp_insensitive(y):\n    if(abs(y)&lt;eps):\n        return 0\n    else:\n        return (abs(y)-eps)**2\nsq_eps_ins = np.vectorize(sq_esp_insensitive)\n\n\n\n\nCode\nxmin, xmax = -4, 4\nxx = np.linspace(xmin, xmax, 100)\n\nplt.figure(figsize=(10,5))\nplt.plot(xx, xx**2, 'g-',\n         label=\"Squared Loss\")\nplt.plot(xx, abs(xx), 'g--',\n         label=\"Absolute Loss\")\n\nplt.plot(xx, abs(xx)-eps, 'b--',\n         label=\"Epsilon-Insensitive Loss\")\n\nplt.plot(xx, sq_eps_ins(xx), 'b-',\n         label=\"Sq-Epsilon-Insensitive Loss\")\nplt.plot(xx, mhuber(xx), 'r-',\n         label=\"Modified-Huber Loss\")\n\nplt.ylim((0, 8))\nplt.legend(loc=\"upper center\")\nplt.xlabel(\"Error\")\nplt.ylabel(\"$L(y, f(x))$\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.4.1 Handling Outliers using Robust Loss Functions\n\n\nCode\n# Create some data with Outliers\nn_samples = 1000\nn_outliers = 50\n\nXr, yr, coef = make_regression(n_samples=n_samples, n_features=1,\n                              n_informative=1, noise=10,\n                              coef=True, random_state=0)\n# Add outlier data\nnp.random.seed(0)\nXr[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))\nyr[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)\nyr/=10\nyr += 10\n\nline_X = np.arange(-5, 5)\nfigure = plt.figure(figsize=(9, 9))\n\nplt.scatter(Xr, yr,facecolors='None',edgecolors='k',alpha=0.5)\n\n# Loss Options: huber, squared_error, epsilon_insensitive, squared_epsilon_insensitive\nlosses = ['huber', 'squared_error']\nfor loss in losses:\n    model = SGDRegressor(loss=loss, fit_intercept=True, max_iter = 2000,\n                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\n    model.fit(Xr, yr)\n\n    # Predict data of estimated models\n    line_y = model.predict(line_X[:, np.newaxis])\n    plt.plot(line_X, line_y, '-', label=loss,alpha=1)\n\nplt.axis('tight')\nplt.legend(loc='best')\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#loss-functions-for-linear-classification",
    "href": "part1/reviewing_supervised_linear_models.html#loss-functions-for-linear-classification",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.5 Loss Functions for Linear Classification",
    "text": "1.5 Loss Functions for Linear Classification\nThus far we have only discussed Regression problems, where we are modeling a continuous output (e.g., \\(y=w^T\\cdot x\\)). However, Linear Models can also be used for Classification problems, where the output is discrete (e.g., \\(y \\in \\{0,1\\}\\) or \\(y \\in \\{-1,1\\}\\)). A naive approach to handling this, would be to just train a regression model as per before, but then just threshold the output at some value to derive the class label. For example, we can take the below binary data:\n\n\nCode\nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_features=1, n_redundant=0, \n                           n_informative=1,\n                           random_state=1,\n                           n_clusters_per_class=1,\n                           flip_y=0.0, class_sep=1)\nplt.figure()\nplt.xlabel('x')\nplt.ylabel('True or False')\nplt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\nplt.show()\n\n\n\n\n\n\n\n\n\nand then fit a regular linear model to this:\n\n\nCode\nmodel = SGDRegressor(loss='squared_error', fit_intercept=True, max_iter = 2000,\n                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\nmodel.fit(X, y)\nXp = np.linspace(X.min(),X.max(),100)\nXp = Xp[:, np.newaxis]\nplt.figure()\nplt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\nplt.plot(Xp,model.predict(Xp))\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see from the above that this is not entirely what we want – for example, just setting the class cutoff at 0.5 would not produce a classification boundary that would be optimal. In contrast, we can modify the loss function to more accurately project the \\(w^T\\cdot x\\) linear model into a classification context.\nWe do this by modifying the loss function from Mean Squared Error to something like: \\[\ny_i\\cdot(w\\cdot x_i)\n\\] where \\(y_i = \\pm 1\\) such that if \\(y_i\\) and \\(w\\cdot x_i\\) point have similar signs, then the decision function is positive, otherwise it is negative.\nWith this change, we are now interested primarily with how the behavior of the loss function when we are in the negative regime (i.e., misclassified points):\n\n\nCode\n# From: http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html\ndef modified_huber_loss(y_true, y_pred):\n    z = y_pred * y_true\n    loss = -4 * z\n    loss[z &gt;= -1] = (1 - z[z &gt;= -1]) ** 2\n    loss[z &gt;= 1.] = 0\n    return loss\n\n\nxmin, xmax = -4, 4\nxx = np.linspace(xmin, xmax, 100)\nlw = 2\nfig = plt.figure(figsize=(15,8))\nplt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], color='gold', lw=lw,\n         label=\"Zero-one loss\")\nplt.plot(xx, -np.minimum(xx, 0), color='yellowgreen', lw=lw,\n         label=\"Perceptron loss\")\nplt.plot(xx, modified_huber_loss(xx, 1), color='darkorchid', lw=lw,\n         linestyle='--', label=\"Modified Huber loss\")\nplt.plot(xx, np.log2(1 + np.exp(-xx)), color='cornflowerblue', lw=lw,\n         label=\"Log loss\")\nplt.plot(xx, np.where(xx &lt; 1, 1 - xx, 0), color='teal', lw=lw,\n         label=\"Hinge loss\")\nplt.plot(xx, np.where(xx &lt; 1, 1 - xx, 0) ** 2, color='orange', lw=lw,\n         label=\"Squared hinge loss\")\nplt.ylim((0, 8))\nplt.legend(loc=\"upper right\")\nplt.xlabel(r\"Decision function $f(x)$\")\nplt.ylabel(\"$L(y, f(x))$\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n####### Try Changing the Below ######\n#loss = 'squared_error'\nloss = 'log_loss'\n#loss = 'hinge'\n#####################################\n\nmodel = SGDClassifier(loss=loss, fit_intercept=True, max_iter = 2000,\n                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\nmodel.fit(X, y)\nXp = np.linspace(X.min(),X.max(),100)\nXp = Xp[:, np.newaxis]\nplt.figure()\nplt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\ntry:\n    plt.plot(Xp,model.predict_proba(Xp)[:,1],label='probability')\nexcept:\n    pass\nplt.plot(Xp,model.predict(Xp))\nplt.title(\"Classifier with loss = {}\".format(loss))\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also try this with 2D data to get a better sense of how some of the other loss functions behave:\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\n\n\nX, y = make_classification(n_features=2, n_redundant=0, \n                           n_informative=2,\n                           random_state=1,\n                           n_clusters_per_class=1,\n                           flip_y=0.0, class_sep=0.7)\n\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\n# Change: try 0,1, or 2\nds = datasets[2]\n\nX, y = ds\nX = StandardScaler().fit_transform(X)\n\nplt.figure(figsize=(9, 9))\nplt.scatter(X[y==1,0],X[y==1,1],marker='+')\nplt.scatter(X[y==0,0],X[y==0,1],marker='o',facecolors='none',edgecolors='k')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Effect of Linear Model Classification Losses\n\n\n\nIn the below experiment, try modifying the different classification loss functions and re-running the model. You will see a dark solid line representing the decision boundary, and dashed lines representing where the decision boundary is +1 or -1. What do you notice?:\n\nFor the perceptron loss, what behavior do you observe if you re-run this model multiple times? Why do you observe this behavior?\nComparing the hinge loss to the perceptron loss, the loss functions look remarkably similar, yet they have very different behavior in the model. Why do you think this is?\nComparing the squared error versus log loss versus hinge, what sorts of differences in behavior do you observe? Thinking about the location of the decision boundary and the shape of the loss functions, why do you think they behave differently?\n\n\n\n\n\nCode\nfrom sklearn.svm import LinearSVC\n\n# Try modifying these:\n#====================\nloss = 'squared_error'\n#loss = 'perceptron'\n#loss = 'log_loss'\n#loss = 'hinge'\n#loss = 'modified_huber'\n#loss = 'squared_hinge'\n\n# Also try the effect of Alpha:\n# e.g., between ranges 1e-20 and 1e0\n#=============================\nalpha=1e-3\n\n# You can also try other models by commenting out the below:\nmodel = SGDClassifier(loss=loss, fit_intercept=True,\n                      max_iter=2000,tol=1e-5, n_iter_no_change =100,\n                      penalty='l2',alpha=alpha) \n# If you would like to compare the SGDClassifier with hinge loss to LinearSVC, you can uncomment the below:\n#model = LinearSVC(loss='hinge',C=1e3)\nmodel.fit(X, y)\n\nplt.figure(figsize=(9, 9))\n\nh=0.01\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nif hasattr(model, \"decision_function\"):\n    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\nelse:\n    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\nZ = Z.reshape(xx.shape)\n\nvmax = max(abs(Z.min()),abs(Z.max()))\ncm = plt.cm.RdBu\nplt.contourf(xx, yy, Z, cmap=cm, alpha=.5, vmax = vmax, vmin = -vmax)\nlevels = [-1.0, 0.0, 1.0]\nlinestyles = ['dashed', 'solid', 'dashed']\ncolors = 'k'\nplt.contour(xx, yy, Z, levels, colors=colors, linestyles=linestyles)\nplt.scatter(X[y==1,0],X[y==1,1],marker='+')\nplt.scatter(X[y==0,0],X[y==0,1],marker='o',facecolors='none',edgecolors='k')\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#penalty-functions-example-of-how-l_p-penalty-changes-loss-optima",
    "href": "part1/reviewing_supervised_linear_models.html#penalty-functions-example-of-how-l_p-penalty-changes-loss-optima",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.6 Penalty Functions: Example of How \\(L_p\\) Penalty Changes Loss Optima",
    "text": "1.6 Penalty Functions: Example of How \\(L_p\\) Penalty Changes Loss Optima\nOK, so we have seen how different loss functions can affect the learned linear model. But what about the penalty function? How does changing the penalty function affect the learned model? Let’s explore this with a simple example by again returning to fitting a line. In this case, we will fit an actual “line” to the data, by which I mean:\n\\[\ny= w_0 + w_1 \\cdot x = \\mathbf{w}^T \\cdot \\mathbf{x}\n\\]\n\n\nCode\n# Generate noisy data from a line\n\n######################\n# Change Me!\nw1_true = 5\nw0_true = 2\nnoise = 0.001\n#################\n\ntrue_func = lambda x: w1_true*x+w0_true\n\nnum_samples = 50\nx = (np.random.rand(num_samples)-0.5)*20\ny = true_func(x)+np.random.normal(scale=noise,size=num_samples)\n\n\n\n\nCode\nplt.figure()\nplt.scatter(x,y)\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this case, since we have very little noise (although you can play with this if you would like), you can see that if we put in the true intercept and slope, we get zero (or close to zero) training objective.\n\nfrom numpy.linalg import norm\ndef loss(a,b,alpha,order=2):\n    return np.average((y - (a*x+b))**2) + alpha*norm([a,b],ord=order)\n\n#example\nprint(f\"Objective: {loss(a=5,b=2,alpha=0)}\")\n\nObjective: 1.3389235788421472e-06\n\n\nThis tells us the objective at the true parameters, but now let’s visualize how the total objective (training error + penalty) varies as we change the parameters. To do this, we can compute the total objective over a grid of possible values for \\(w_0\\) and \\(w_1\\), and then plot the contours of this objective function:\n\n\nCode\nA, B = np.meshgrid(np.linspace(-10, 10, 201), np.linspace(-10, 10, 201))\nN,M = A.shape\nfloss = np.vectorize(loss)\n\ndef generate_new_data(a=5,b=5):\n    x = (np.random.rand(num_samples)-0.5)*20\n    #y = true_func(x)+np.random.normal(scale=1,size=num_samples)\n    y = a*x+b+np.random.normal(scale=1,size=num_samples)\n    return x,y\n\ndef generate_z_grid(alpha,order):\n    Z_noalpha = floss(A.flatten(),B.flatten(),0).reshape((N,M))\n    alpha=alpha\n    Z = floss(A.flatten(),B.flatten(),alpha,order)\n    min_ind = np.argmin(Z)\n    Amin = A.flatten()[min_ind]\n    Bmin = B.flatten()[min_ind]\n    Z = Z.reshape((N,M))\n    return Z_noalpha, Z, Amin, Bmin\n\nget_levels = lambda z: [np.percentile(z.flatten(),i) for i in [0,0.5,1,2,5,10,15,30,40,50,75,95,100]]\n#levels = [np.percentile(allz,i) for i in [0,0.5,1,2,5,10,15,30,40,50,75,95,100]]\n#levels = [np.percentile(allz,i) for i in np.logspace(-2,3,10,base=3)]\n\ndef plot_objective(alpha=0,order=2):\n    Z_noalpha, Z, Amin, Bmin = generate_z_grid(alpha,order)\n    plt.figure(figsize=(7,7))\n    plt.vlines(0,-10,10,alpha=0.25,colors='r',linestyles='solid')\n    plt.hlines(0,-10,10,alpha=0.25,colors='r',linestyles='solid')\n    plt.contour(A,B,Z_noalpha,10,levels=get_levels(Z_noalpha),linestyles='solid',cmap='Greys_r',alpha=0.05)\n    plt.contour(A,B,Z,10,levels=get_levels(Z),linestyles='solid',cmap='Greys_r',alpha=0.5)\n    plt.scatter([0],[0],marker='D',s=50,c='r')\n    plt.scatter([w1_true],[w0_true],marker='*',s=400)\n    plt.scatter([Amin],[Bmin])\n    plt.xlabel('a')\n    plt.ylabel('b')\n    plt.title('Optima: a={:.2f}, b={:.2f}'.format(Amin,Bmin))\n    plt.show()\n\n\n\ninteract(plot_objective,\n         alpha=np.logspace(-2,5,8),\n         order=FloatSlider(min=0,max=10,step=0.1,continuous_update=False,value=2.0))\n\n\n\n\n&lt;function __main__.plot_objective(alpha=0, order=2)&gt;\n\n\nHere we can see the true optimal parameters as a blue star, the objective function contours as dark gray lines, and the red diamond shows the point where both weights are zero. The orange circle shows the minimum point of the objective function. If you increase the value of alpha in the drop down, this will increase the penalty weight, and you can see how both the objective and the optimal point change. Moreover, you can change the p-norm order in the penalty to see what effect this has on the total objective function.\n\n\n\n\n\n\nTipExperiment: How do the different penalty terms affect the Objective Function and the optimal weight?\n\n\n\nTry modifying the following:\n\nAs you increase the penalty weight under the L2 norm, how does the objective landscape change?\nIf you use a p-order that is less than 2, how does this alter the objective landscape?\nThe L1 norm is known to induce sparsity in the optimal weights. Do you observe this in the objective landscape? Why or why not?\nDifferent types of regularization within the p-norm family are often referred to as “Shrinkage” operators. Why do you think this is the case, based on what you observe?\nIf my goal is to induce sparsity, it would make sense to consider the L0 norm, which just counts the number of non-zero entries in a vector. Based on the plots below, why won’t this work?\n\n\n\nTo help visualize the experiment above, let’s try plotting, for different norm orders, the path that the coefficients take as we set alpha = 0 (no regularization) to a large number (essentially fully regularized). Now the light green contour represents just the contribution to the objective from the training error (no regularization), and the light gray contour shows the contribution from the penalty term (no training error). The blue line shows how the optimal weight changes as we increase the penalty weight from 0 to a large number.2\n2 Note how I can visually find the optimal weight by fixing a given iso-contour of the regularization term (the gray lines) and then finding the point along that iso-contour where the green contour is minimized.\n\nCode\nfrom scipy.optimize import fmin\nimport warnings\nwarnings.simplefilter('ignore', RuntimeWarning)\n\nalpha_range = np.logspace(-1,5,14)\norder_range = [0,0.25,.5,.75,1,1.5,2,3,5,10,20,100]\n\nAl = len(alpha_range)\nOl = len(order_range)\nresults = np.zeros((Al,Ol,2))\n\nfor j,o in enumerate(order_range):\n    prev_opt = [5,5]\n    for i,a in enumerate(alpha_range):\n        f = lambda x: loss(x[0],x[1],alpha=a,order=o)\n        x_opt = fmin(f,prev_opt,disp=False)\n        results[i,j] = x_opt\n        prev_opt = x_opt\n\n\n\n\nCode\nfor j,o in enumerate(order_range):\n    fig = plt.figure(figsize=(12,7))\n    Z_noalpha, Z, Amin, Bmin = generate_z_grid(10000,o)\n    plt.contour(A,B,Z_noalpha,10,levels=get_levels(Z_noalpha),linestyles='solid',alpha=0.2,colors='g')\n    plt.contour(A,B,Z,10,levels=get_levels(Z),linestyles='solid',alpha=0.2,colors='k')\n    plt.plot(results[:,j,0],results[:,j,1],marker='o',alpha=0.75)\n    plt.title('L-{} Norm'.format(o))\n    plt.axis('equal')\n    plt.xlim([-1,6])\n    plt.ylim([-1,3])\n    plt.show()\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#summary-of-how-to-select-a-loss-or-penalty-function",
    "href": "part1/reviewing_supervised_linear_models.html#summary-of-how-to-select-a-loss-or-penalty-function",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.7 Summary of How to Select a Loss or Penalty function",
    "text": "1.7 Summary of How to Select a Loss or Penalty function\nIn class, we reviewed a couple of different forms of loss functions and penalty functions and talked a bit about the criteria for selecting them. Below is a very short summary of these.\n\n1.7.1 Loss Functions for Regression:\n\nIf you have no prior knowledge of the function or data, then selecting an L2 type loss (like the Mean Squared Error) is reasonable. When data nicely behaves w.r.t. a linear model (e.g., features are uncorrelated, errors in the linear model are uncorrelated, have equal variances, and expected error of zero around the linear model, etc.) then a Linear Model with an L2 Loss is the Best Linear Unbiased Estimate (BLUE) according to the Gauss-Markov Theorem.\nIf you have reason to believe that the data will have outliers or otherwise need to be robust to spurious large samples, then L2 loss will not be robust to this (as we saw in Lecture). For this, moving to an L1 type loss (like an Absolute Loss or Huber loss) will make the model less sensitive to outliers. It is one approach to handling Robust Regression.\nIf you need to have the model’s loss be dominated only by a handful of points/data as opposed to all of the data, then epsilon-insensitive loss is appropriate since many points well-fit by the model will have “zero” loss. For things like Linear Models, this has limited usefulness right now. However, when we “kernalize” Linear models in “Kernels” week, you will see that this is a big deal, and it is what gives rise to the “Support Vector” part of “Support Vector Machines”. Specifically, decreasing epsilon towards zero increases the number of needed Support Vectors (can be a bad thing), and increasing epsilon can decrease the number of needed Support Vectors (can be a good thing). This will make more sense in a few week’s time.\n\n\n\n1.7.2 Loss Functions for Classification:\n\nZero-One loss sounds nice, but is not useful in practice, since it is not differentiable.\nPerceptron loss, while of historical importance, is not terribly useful in practice, since it does not converge to a unique solution and an SVM (i.e., Hinge Loss below) has all of the same benefits.\nIf you need a simple linear model which outputs actual probabilities (like, 95% sure the component has failed), then the log-loss does this, via Logistic Regression and allows you to calculate classification probabilities in closed form.\nIf you want something that maximizes the margin of the classifier, then the Hinge Loss can get close to this. It is the basis of Linear Support Vector Machines\n\n\n\n1.7.3 Penalty Terms (Lp Norms) for Linear Models:\n\n\\(L_2\\) Norm penalties on the weight vector essentially “shrink” the weights towards zero as you increase the penalty weight. Adding this kind of penalty to a linear model has different names, depending on which community of people you are talking with. Some of these other names are: (1) Tikhonov regularization, (2) Ridge Regression, (3) \\(L_2\\) Shrinkage Estimators, or (4) Gaussian Weight Priors. I find looking at the penalty term itself more helpful at understanding the effects rather than memorizing the different names.\n\\(L_0\\) Norm penalties, while conceptually interesting since they essentially “count” entries in the weight vector, are not practically useful since they are not differentiable and are thus difficult to optimize.\n\\(L_1\\) Norm penalties are a compromise between \\(L_2\\) and \\(L_0\\) in that they promote sparsity in the weights (some weights will become zero) but are (largely) differentiable, meaning that you can meaningfully optimize them (unlike \\(L_0\\)). Shrinking certain weights to zero in this way can be useful when (1) you are in the \\(n \\ll p\\) regime (many more features than data points) where the model is underdetermined and (2) you want some degree of model interpretability (it sets many weights to zero). Some of these other names for this kind of linear regression with this penalty are the LASSO (least absolute shrinkage and selection operator). \n\\(L_\\infty\\) (where p is really large) essentially penalize the size of the biggest element of the weight vector (w). While there are some niche instances where this kind of norm is useful for a Loss function (e.g., Chebyshev Regression), I have rarely seen meaningful use cases in practice where this makes sense as a penalty term for the weight vector.\nCombinations of penalties. For example, a common combination is combining both an \\(L_2\\) and \\(L_1\\) penalty on the weights, as in: \\(\\Omega(w) = \\alpha ||w||_2 + \\beta ||w||_1\\). This particular combination is called “Elastic Net” and exhibits some of the good properties of \\(L_2\\) penalities with the sparsity inducing properties of \\(L_1\\) regularization.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html",
    "href": "notebooks/cross_validation_linear_regression.html",
    "title": "2  Evaluating Machine Learning Models",
    "section": "",
    "text": "2.1 Getting an Unbiased Estimate of Out-of-Sample Performance\nIn the prior chapter, we covered how different loss functions and regularization terms affected a linear model, in terms of the model’s qualitative performance and its affect on the training score. However, as we saw, the training score an be a misleading performance indicator. How might we judge the model’s performance more rigorously? This chapter addresses this by reviewing the why and how of performing Cross Validation, and also what this means regarding optimizing the hyperparameters of a model.\nWhen we train a machine learning model, we are typically interested in how well the model will perform on data it has not seen before. This is often referred to as the model’s generalization performance. However, if we evaluate the model’s performance on the same data it was trained on, we may get an overly optimistic estimate of its true performance. This is because the model may have simply memorized the training data, rather than learning the underlying patterns. One popular way to assess this is through Cross Validation:\n# Now let's split the data into training and test data:\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.2, random_state=0)\nLet’s take a look at what the above has actually done.\nCode\nprint('X_train\\n',X_train,'\\n')\nprint('X_test\\n',X_test,'\\n')\nprint('y_train\\n',y_train,'\\n')\nprint('y_test\\n',y_test,'\\n')\n\nplt.figure(figsize=(8,8))\nplt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n# plot the training and testing points in colors\nplt.scatter(X_train,y_train, label=\"Training data\")\nplt.scatter(X_test,y_test, label=\"Testing data\")\nplt.legend(loc=\"best\")\nplt.show()\n\n\nX_train\n [8.78142503e-01 3.02332573e-01 4.19194514e-01 6.92322616e-01\n 1.40386939e-01 4.17304802e-01 1.86260211e-01 3.96767474e-01\n 7.20324493e-01 6.70467510e-01 2.73875932e-02 9.68261576e-01\n 1.46755891e-01 9.23385948e-02 5.38816734e-01 5.58689828e-01\n 1.98101489e-01 1.69830420e-01 8.76389152e-01 8.50442114e-02\n 1.14374817e-04 6.85219500e-01 4.17022005e-01 3.13424178e-01] \n\nX_test\n [0.03905478 0.89460666 0.34556073 0.20445225 0.87811744 0.80074457] \n\ny_train\n [-0.37395132  0.05199163 -0.47818202 -0.82672018  0.90350849 -0.45417721\n  0.72898424 -0.36366041 -0.89399733 -1.11157064  0.90389734 -0.21270638\n  0.86040424  0.79675107 -0.89105816 -0.87458209  0.52662674  0.74673588\n -0.63887828  0.97904574  0.98275703 -0.97273902 -0.42390528  0.0668933 ] \n\ny_test\n [ 0.98733352 -0.47140604 -0.00455281  0.55839434 -0.61801179 -0.82613321]\nThe key idea in cross validation is to test the model on data that was separate from the data you trained on, therefore establishing two datasets: a training set and a test set. The training set is used to fit the model, while the test set is used to evaluate its performance. This way, we can get a more realistic estimate of how well the model will perform on new, unseen data.\nCode\nalphas = [0, 1e-20, 1e-10, 1e-7, 1e-5, 1,10]\nd=15\nfor a in alphas:\n    plt.figure(figsize=(7, 7))\n    #plt.setp(ax, xticks=(), yticks=())\n    polynomial_features = PolynomialFeatures(degree=d,\n                                             include_bias=False)\n    linear_regression = LinearRegression()\n    linear_regression = Ridge(alpha=a)\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    #pipeline.fit(X[:, np.newaxis], y)\n    pipeline.fit(X_train[:, np.newaxis], y_train)\n    # Evaluate the models using crossvalidation\n    #scores = cross_validation.cross_val_score(pipeline,\n    #    X[:, np.newaxis], y, scoring=\"mean_squared_error\", cv=10)\n    \n    testing_score = pipeline.score(X_test[:, np.newaxis],y_test)\n    training_score = pipeline.score(X_train[:, np.newaxis],y_train)\n\n    X_plot = np.linspace(0, 1, 100)\n    plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label=\"Model\")\n    plt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n    plt.scatter(X, y, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    \n    plt.title(\"Degree {}, Alpha {}\\nTest score = {:.3f}\\nTraining score = {:.3f}\".format(\n        d, a, testing_score,training_score))\n    plt.show()\nThis is a simplified type of cross-validation often referred to as “Shuffle Splitting” and is one of the most common, but it is useful to review other types of cross-validation via this nice summary page from the SKLearn library, which covers a variety of important variants including:\nDiscussion Point: Under what conditions or situations would using each type of cross-validation above be appropriate versus inappropriate?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#getting-an-unbiased-estimate-of-out-of-sample-performance",
    "href": "notebooks/cross_validation_linear_regression.html#getting-an-unbiased-estimate-of-out-of-sample-performance",
    "title": "2  Evaluating Machine Learning Models",
    "section": "",
    "text": "K-Fold Cross Validation\nLeave-One-Out Cross Validation\nStratified Cross Validation\nGroup-wise Cross Validation\nTime Series Split Cross Validation",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#finding-the-optimal-hyper-parameters",
    "href": "notebooks/cross_validation_linear_regression.html#finding-the-optimal-hyper-parameters",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.2 Finding the Optimal Hyper-parameters",
    "text": "2.2 Finding the Optimal Hyper-parameters\nNow that we have introduced the usage of hyper-parameters and cross-validation, a natural question arises: How do we choose the hyper-parameters? There are many ways to do this, and this section will describe the most common and basic ones, while leaving more advanced techniques (like Implicit Differentiation) for later. Specifically, this section will: 1. Define the concepts of Grid and Random Hyper-parameter search. 2. Use Grid and Random search to optimize hyper-parameters of a model. 2. Distinguish when Randomized Search is much better than grid search. 3. Describe how Global Optimization procedures such as Bayesian Optimization work. 4. Recognize why none of those at all work in High Dimensions and describe the “Curse of Dimensionality”\nIn future chapters once we cover more advanced derivative methods, we can discuss how to use tools like Implicit Differentiation to directly compute the gradient of the cross-validation score with respect to hyper-parameters, and then use this gradient to optimize the hyper-parameters using standard gradient-based optimization methods. However, for now, let’s focus on more basic derivative-free methods, since they are more widely used and easier to understand.\nLet’s start by returning to our Polynomial example, and this time focus on finding the best combination of degree and penalty weight for a linear model.\n\n\nCode\n# from sklearn.preprocessing import PolynomialFeatures\n# from sklearn.pipeline import Pipeline\n# from sklearn.linear_model import Ridge\n# from sklearn.metrics import mean_squared_error\n\n\n\n\nCode\nfrom sklearn import model_selection\n\n# Let's plot the behavior of a fixed degree polynomial\ndegree = 15\n# (i.e., f(x) = w_1*x + w_2*x^2 + ... + w_15*x^15)\n# but where we change alpha.\nalphas = np.logspace(start=-13,stop=4,num=20)\npolynomial_features = PolynomialFeatures(degree=degree,\n                                         include_bias=False)\nscores = []\nfor a in alphas:\n    linear_regression = Ridge(alpha=a)\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    cv_scores = model_selection.cross_val_score(pipeline,\n        X[:,np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=20)\n    scores.append(cv_scores)\n\nscores = np.array(scores)\n\nplt.figure(figsize=(7,3))\nplt.semilogx(alphas,-np.mean(scores,axis=1),'-')\nplt.ylabel('Test MSE')\nplt.xlabel('Alpha ($\\\\alpha$)')\nsns.despine()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#what-if-we-have-more-than-one-variable",
    "href": "notebooks/cross_validation_linear_regression.html#what-if-we-have-more-than-one-variable",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.3 What if we have more than one variable?",
    "text": "2.3 What if we have more than one variable?\nLet’s look at both polynomial degree and regularization weight\n\n\nCode\nscores = []\nalphas = np.logspace(start=-13, # Start at 1e-13\n                     stop=4,    # Stop at 1e4\n                     num=40)    # Split that into 40 pieces\ndegrees = range(1,16) # This will only go to 15, due to how range works\n\nscores = np.zeros(shape=(len(degrees), # i.e., 15\n                         len(alphas))) # i.e., 20\n\nfor i, degree in enumerate(degrees): # For each degree\n    polynomial_features = PolynomialFeatures(degree=degree,\n                                             include_bias=False)\n    \n    for j,a in enumerate(alphas):    # For each alpha\n        linear_regression = Ridge(alpha=a)\n        pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                             (\"linear_regression\", linear_regression)])\n        cv_scores = model_selection.cross_val_score(pipeline,\n            X[:,np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=20)\n        scores[i][j] = -np.mean(cv_scores)\n\n\n\n\nCode\nfig = plt.figure(figsize=(10,7))\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nax = fig.add_subplot(111, projection='3d')\n\nXs, Ys = np.meshgrid(range(len(degrees)), range(len(alphas)))\nzs = np.array([scores[i,j] for i,j in zip(np.ravel(Xs), np.ravel(Ys))])\nZs = zs.reshape(Xs.shape)\n\nXs, Ys = np.meshgrid(degrees, np.log(alphas))\n\nax.plot_surface(Xs, Ys, Zs, rstride=1, cstride=1, cmap=cm.coolwarm,\n    linewidth=0, antialiased=False)\n\n# Label the Axes\nax.set_xlabel('Degree')\nax.set_ylabel('Regularization')\nax.set_zlabel('MSE')\n\n# Rotate the image\nax.view_init(30, # larger # goes \"higher\"\n             30) # larger # \"circles around\"\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(5,10))\nplt.imshow(Zs,\n           cmap=cm.coolwarm, # Allows you to set the color\n           vmin=Zs.min(), vmax=0.2, # The min and max Z-Values (for coloring purposes)\n           extent=[Xs.min(), Xs.max(),   # How far on X-Axis you want to plot\n                   Ys.min(), Ys.max()],  # How far on Y-Axis\n           interpolation='spline16',      # How do you want to interpolate values between data?\n           origin='lower')\nplt.title('Mean Squared Error')\nplt.xlabel('Degree')\nplt.ylabel('Regularization')\nplt.colorbar()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#optimization",
    "href": "notebooks/cross_validation_linear_regression.html#optimization",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.4 Optimization",
    "text": "2.4 Optimization\nAt the end of the day, all this is doing is optimization/search over different parameters.\nHow should we go about automating this?\nMost common: Grid Search.\n\n\nCode\nprint('parameters we could change:')\nfor k in pipeline.get_params().keys():\n    print(\" \",k)\n\n\nparameters we could change:\n  memory\n  steps\n  transform_input\n  verbose\n  polynomial_features\n  linear_regression\n  polynomial_features__degree\n  polynomial_features__include_bias\n  polynomial_features__interaction_only\n  polynomial_features__order\n  linear_regression__alpha\n  linear_regression__copy_X\n  linear_regression__fit_intercept\n  linear_regression__max_iter\n  linear_regression__positive\n  linear_regression__random_state\n  linear_regression__solver\n  linear_regression__tol\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'polynomial_features__degree': list(range(1,16)), # 15 possible\n              'linear_regression__alpha': np.logspace(start=-13,stop=4,num=10),\n              'polynomial_features__include_bias':[True, False]}\n\n\n\n\nCode\n# How do we want to do cross-validation?\nfrom sklearn import model_selection\nnum_data_points = len(y)\n\n# 4-fold CV\nkfold_cv = model_selection.KFold(n_splits = 4) \n\n# Or maybe you want randomized splits?\nshuffle_cv = model_selection.ShuffleSplit(n_splits = 20,     # How many iterations?\n                                          test_size=0.2    # What % should we keep for test?\n                                         )\n\n\n\n\nCode\nX=X[:,np.newaxis]\ngrid_search = GridSearchCV(pipeline,    # The thing we want to optimize\n                           parameters,  # The parameters we will change\n                           cv=shuffle_cv, # How do you want to cross-validate?\n                           scoring = 'neg_mean_squared_error'\n                          )\ngrid_search.fit(X, y) # This runs the cross-validation\n\n\nGridSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),\n             estimator=Pipeline(steps=[('polynomial_features',\n                                        PolynomialFeatures(degree=15,\n                                                           include_bias=False)),\n                                       ('linear_regression',\n                                        Ridge(alpha=np.float64(10000.0)))]),\n             param_grid={'linear_regression__alpha': array([1.00000000e-13, 7.74263683e-12, 5.99484250e-10, 4.64158883e-08,\n       3.59381366e-06, 2.78255940e-04, 2.15443469e-02, 1.66810054e+00,\n       1.29154967e+02, 1.00000000e+04]),\n                         'polynomial_features__degree': [1, 2, 3, 4, 5, 6, 7, 8,\n                                                         9, 10, 11, 12, 13, 14,\n                                                         15],\n                         'polynomial_features__include_bias': [True, False]},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),\n             estimator=Pipeline(steps=[('polynomial_features',\n                                        PolynomialFeatures(degree=15,\n                                                           include_bias=False)),\n                                       ('linear_regression',\n                                        Ridge(alpha=np.float64(10000.0)))]),\n             param_grid={'linear_regression__alpha': array([1.00000000e-13, 7.74263683e-12, 5.99484250e-10, 4.64158883e-08,\n       3.59381366e-06, 2.78255940e-04, 2.15443469e-02, 1.66810054e+00,\n       1.29154967e+02, 1.00000000e+04]),\n                         'polynomial_features__degree': [1, 2, 3, 4, 5, 6, 7, 8,\n                                                         9, 10, 11, 12, 13, 14,\n                                                         15],\n                         'polynomial_features__include_bias': [True, False]},\n             scoring='neg_mean_squared_error') best_estimator_: PipelinePipeline(steps=[('polynomial_features', PolynomialFeatures(degree=4)),\n                ('linear_regression',\n                 Ridge(alpha=np.float64(3.5938136638046257e-06)))]) PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=4) Ridge?Documentation for RidgeRidge(alpha=np.float64(3.5938136638046257e-06)) \n\n\n\n\nCode\ngrid_search.best_params_ # Once finished, you can see what the best parameters are\n\n\n{'linear_regression__alpha': np.float64(3.5938136638046257e-06),\n 'polynomial_features__degree': 4,\n 'polynomial_features__include_bias': True}\n\n\n\n\nCode\nprint(\"Best MSE for Grid Search: {:.2e}\".format(-grid_search.best_score_))\n\n\nBest MSE for Grid Search: 6.93e-03\n\n\n\n\nCode\ngrid_search.predict(X)  # You can also use the best model directly (in sklearn)\n\n\narray([ 0.92597995,  0.97112185,  0.97915236,  0.95343228,  0.94185444,\n        0.8235348 ,  0.80306679,  0.72117767,  0.65636685,  0.60679754,\n        0.57933728,  0.10937232,  0.05389104, -0.10522154, -0.34633312,\n       -0.43501599, -0.43622117, -0.44424987, -0.84518448, -0.88774347,\n       -0.98056731, -0.97399224, -0.96929348, -0.94126767, -0.78277808,\n       -0.54827761, -0.54221084, -0.54212267, -0.48316009, -0.20435414])\n\n\n\n\nCode\nbest_degree = grid_search.best_params_['polynomial_features__degree']\nbest_alpha = grid_search.best_params_['linear_regression__alpha']\nX_plot = X_plot[:,np.newaxis]\nplt.figure(figsize=(7, 7))\nplt.plot(X_plot, grid_search.predict(X_plot),'-',label=\"Model\",alpha=0.5)\nplt.plot(X_plot, true_fun(X_plot), ':',label=\"True function\",alpha=1)\nplt.scatter(X,y, c='Blue', s=20, edgecolors='none')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xlim((0, 1))\nplt.ylim((-2, 2))\nsns.despine()\nplt.title(\"Degree {}, Alpha {:.1e}\".format(best_degree,best_alpha))\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#randomized-search",
    "href": "notebooks/cross_validation_linear_regression.html#randomized-search",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.5 Randomized Search",
    "text": "2.5 Randomized Search\nIn reality, grid search is wasteful and not easy to control. A better (and still easy way) is to randomize the search.\n\n\nCode\n# Now, instead of specifying exact which points to test, we instead\n# have to specify a distribution to sample from.\n# For example, things from http://docs.scipy.org/doc/scipy/reference/stats.html\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import lognorm as sp_lognorm\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nparameters = {'polynomial_features__degree': sp_randint(1,20), # We want an integer\n              'linear_regression__alpha': sp_lognorm(1),\n              'polynomial_features__include_bias':[True, False]} # Selecting from two is fine\n\n\nNeed something whose logarithmic distribution we can control. How about a lognormal? \\[\n\\mathcal{N}(\\ln x;\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left[-\\frac {(\\ln x - \\mu)^2} {2\\sigma^2}\\right].\n\\]\n\n\nCode\nsigma=6\nrv = sp_lognorm(sigma,scale=1e-7)\n\nplt.figure()\nplt.hist(rv.rvs(size=1000),bins=np.logspace(-20, 2, 22))\nplt.xscale('log')\nplt.xlabel('x')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nparameters = {'polynomial_features__degree': sp_randint(1,20), # We want an integer\n              'linear_regression__alpha': sp_lognorm(sigma,scale=1e-7),\n              'polynomial_features__include_bias':[True, False]} # Selecting from two is fine\n\n\n\n\nCode\n# Fitting the high degree polynomial makes the linear system almost\n# singular, which makes Numpy issue a Runtime warning.\n# This is not a problem here, except that it pops up the warning box\n# So I will disable it just for pedagogical purposes\nimport warnings\nwarnings.simplefilter('ignore',RuntimeWarning)\n\n# specify parameters and distributions to sample from\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# run randomized search\n#n_iter_search = 300 # How many random parameter settings should we try?\nn_iter_search = len(grid_search.cv_results_['params']) # Give it same # as grid search, to be fair\nrandom_search = RandomizedSearchCV(pipeline,\n                                   param_distributions=parameters,\n                                   n_iter=n_iter_search, \n                                   cv=shuffle_cv, # How do you want to cross-validate?\n                                   scoring = 'neg_mean_squared_error')\nrandom_search.fit(X, y)\n\n\nRandomizedSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),\n                   estimator=Pipeline(steps=[('polynomial_features',\n                                              PolynomialFeatures(degree=15,\n                                                                 include_bias=False)),\n                                             ('linear_regression',\n                                              Ridge(alpha=np.float64(10000.0)))]),\n                   n_iter=300,\n                   param_distributions={'linear_regression__alpha': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000023215096120&gt;,\n                                        'polynomial_features__degree': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000002320FEBFF80&gt;,\n                                        'polynomial_features__include_bias': [True,\n                                                                              False]},\n                   scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),\n                   estimator=Pipeline(steps=[('polynomial_features',\n                                              PolynomialFeatures(degree=15,\n                                                                 include_bias=False)),\n                                             ('linear_regression',\n                                              Ridge(alpha=np.float64(10000.0)))]),\n                   n_iter=300,\n                   param_distributions={'linear_regression__alpha': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000023215096120&gt;,\n                                        'polynomial_features__degree': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000002320FEBFF80&gt;,\n                                        'polynomial_features__include_bias': [True,\n                                                                              False]},\n                   scoring='neg_mean_squared_error') best_estimator_: PipelinePipeline(steps=[('polynomial_features', PolynomialFeatures(degree=4)),\n                ('linear_regression',\n                 Ridge(alpha=np.float64(3.3921813538938043e-06)))]) PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=4) Ridge?Documentation for RidgeRidge(alpha=np.float64(3.3921813538938043e-06)) \n\n\n\n\nCode\nrandom_search.best_params_ # Once finished, you can see what the best parameters are\n\n\n{'linear_regression__alpha': np.float64(3.3921813538938043e-06),\n 'polynomial_features__degree': 4,\n 'polynomial_features__include_bias': True}\n\n\n\n\nCode\nprint(\"Best MSE for Random Search: {:.2e}\".format(-random_search.best_score_))\n\n\nBest MSE for Random Search: 7.48e-03\n\n\n\n\nCode\nbest_degree = random_search.best_params_['polynomial_features__degree']\nbest_alpha = random_search.best_params_['linear_regression__alpha']\n\nplt.figure(figsize=(7, 7))\nplt.plot(X_plot, random_search.predict(X_plot),'-',label=\"Model\",alpha=0.5)\nplt.plot(X_plot, true_fun(X_plot), ':',label=\"True function\",alpha=1)\nplt.scatter(X,y, c='Blue', s=20, edgecolors='none')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xlim((0, 1))\nplt.ylim((-2, 2))\nsns.despine()\nplt.title(\"Degree {}, Alpha {:.1e}\".format(best_degree,best_alpha))\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#global-bayesian-optimization",
    "href": "notebooks/cross_validation_linear_regression.html#global-bayesian-optimization",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.6 Global Bayesian Optimization",
    "text": "2.6 Global Bayesian Optimization\nSurely, since we are essentially doing optimization, we could approach hyper-parameter selection as an optimization problem as well, right?\nEnter techniques like Global Bayesian Optimization below:\n\n\nCode\ndef f(x):\n    \"\"\"The function to predict.\"\"\"\n    return x * np.sin(x)\n    # Try others!\n    #return 5 * np.sinc(x)\n    #return x\n    \nX = np.atleast_2d(np.linspace(0, 10, 200)).T\n\n# Observations\ny = f(X).ravel()\n\nplt.figure()\nplt.plot(X,y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n########################################################\n# This is just a helper function, no need to worry about\n# The internals.\n# We will return to this example in Week 14\n########################################################\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nnp.random.seed(1)\n\n# Mesh the input space for evaluations of the real function, the prediction and\n# its MSE\nx = np.atleast_2d(np.linspace(0, 10, 1000)).T\n\n# Create a Gaussian Process model\n#kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\nkernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n#gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\nkernel = C(3.0)*RBF(1.5)\ngp = GaussianProcessRegressor(kernel=kernel,alpha=1e-6,optimizer=None)\n#gp = GaussianProcess(corr='cubic', theta0=1e-2, thetaL=1e-4, thetaU=1e-1,random_start=100)\n\n# Now, ready to begin learning:\ntrain_ind ={\n    'Upper CB':   np.zeros(len(X),dtype=bool),\n    'Random':np.zeros(len(X),dtype=bool)\n}\noptions = train_ind.keys()\n\npossible_points = np.array(list(range(len(X))))\n# Possible Initialization options\n# 1. Select different points randomly\n#for i in range(2):\n#    for o in options:\n#        ind = np.random.choice(possible_points[~train_ind[o]],1)\n#        train_ind[o][ind] = True\n\n# 2. Start with end-points\n#for o in options:\n#    train_ind[o][0] = True\n#    train_ind[o][-1] = True\n\n# 3. Start with same random points\nfor ind in np.random.choice(possible_points,2):\n    for o in options:\n        train_ind[o][ind] = True\n\nplot_list = np.array([5,10,20,30,40,50,len(X)])\nfor i in range(10):\n    # As i increases, we increase the number of points\n    plt.figure(figsize=(16,6))\n    for j,o in enumerate(options):\n        plt.subplot(1,2,j+1)\n        gp.fit(X[train_ind[o],:],y[train_ind[o]])\n        yp,sigma = gp.predict(X[~train_ind[o],:], return_std=True)\n        ucb = yp + 1.96*sigma\n        if o == 'Upper CB':\n            #candidates = np.extract(MSE == np.amax(MSE),X[~train_ind[o],:])\n            candidates = np.extract(ucb == np.amax(ucb),X[~train_ind[o],:])\n            next_point = np.random.choice(candidates.flatten())\n            next_ind = np.argwhere(X.flatten() == next_point)\n        elif o == 'Random':\n            next_ind = np.random.choice(possible_points[~train_ind[o]],1)\n        train_ind[o][next_ind] = True\n        \n        # Plot intermediate results\n        yp,sigma = gp.predict(x, return_std=True)\n        plt.fill(np.concatenate([x, x[::-1]]),\n                np.concatenate([yp - 1.9600 * sigma,\n                               (yp + 1.9600 * sigma)[::-1]]),'b',\n                alpha=0.05,  ec='g', label='95% confidence interval')\n    \n        n_train = np.count_nonzero(train_ind[o])\n\n        gp.fit(X[train_ind[o],:],y[train_ind[o]])\n        # Show progress\n        yp,sigma = gp.predict(x, return_std=True)\n        yt = f(x)\n        error = np.linalg.norm(yp-yt.flatten())\n\n        plt.fill(np.concatenate([x, x[::-1]]),\n                np.concatenate([yp - 1.9600 * sigma,\n                               (yp + 1.9600 * sigma)[::-1]]),'b',\n                alpha=0.3,  ec='None', label='95% confidence interval')\n        \n        plt.plot(x,yt,'k--',alpha=1)\n        plt.plot(x,yp,'r-',alpha=1)\n        plt.scatter(X[train_ind[o],:],y[train_ind[o]],color='g',s=100)\n        plt.scatter(X[next_ind,:].flatten(),y[next_ind].flatten(),color='r',s=150)\n        plt.ylim([-10,15])\n        plt.xlim([0,10])\n        plt.title(\"%s\\n%d training points\\n%.2f error\"%(o,n_train,error))\n    plt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#the-curse-of-dimensionality",
    "href": "notebooks/cross_validation_linear_regression.html#the-curse-of-dimensionality",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.7 The Curse of Dimensionality",
    "text": "2.7 The Curse of Dimensionality\nDiscuss on board examples of the Curse of Dimensionality and how it affects algorithms dependent on calculating distances.\n\nSpace-filling properties of inscribed hyper-cube\nDistance ratio between min and max distances\nEffects on nearest neighbor graphs\nEffects on Gaussian Density\n\n\n\nCode\nfrom math import gamma\nV_sphere = lambda d: np.pi**(d/2.0)\nV_cube = lambda d: d*2**(d-1)*gamma(d/2.0)\nvolume_ratio = lambda d: V_sphere(d)/V_cube(d)\n\nd = range(2,50)\nratio = [volume_ratio(i) for i in d]\nplt.figure(figsize=(10,10))\nplt.plot(d,ratio)\nplt.semilogy(d,ratio)\nplt.ylabel(\"Ratio of Hyper-Sphere Vol. to Hyper-Cube Vol.\")\nplt.xlabel(\"Number of Dimensions\")\nplt.show()\n\n# TODO: Add distance min/max example",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/supervised_linear_models.html",
    "href": "notebooks/supervised_linear_models.html",
    "title": "3  Introduction to Gradient Descent",
    "section": "",
    "text": "3.1 Experiment:\nWe will now review Linear Regression from the standpoint of Gradient Descent (instead of the normal equations), so as to build our intuition about how Gradient Descent works, and also introduce the concept of Stochastic Gradient Descent (SGD).\nLet’s first set up our notation for the problem: \\[\ny = w\\cdot x + b + \\epsilon\n\\] Or, if we consider \\(x = [1, x]\\) then: \\[\ny = \\mathbf{w^T\\cdot x} + \\epsilon\n\\]\nFrom your earlier statistics classes, you likely learned how to solve for the linear regression weights using the Normal Equations: \\[\n\\hat{w} = (X^T X)^{-1}X^T y\n\\]\nThere are ways of solving the normal equations directly without needing to take the inverse (such as using the Cholesky decomposition), however today we are going to focus on a different kind of solver that has more broader applications: Gradient Descent and it’s cousin Stochastic Gradient Descent (SGD).\nWe first need to start with some sort of Cost function that we wish to minimize. In general, we will consider costs of the form: \\[\nLoss = Error + \\alpha\\cdot Penalty\n\\]\nSpecifically for Linear Models, we will talk about costs (which I’ll call \\(J\\)) of the form:\n\\[\nJ(w,X) = \\frac{1}{N}\\Sigma_{i=1}^N \\left(\\mathbf{y}_i - f(\\mathbf{w},\\mathbf{x}_i)\\right)^2 + \\alpha\\cdot\\Omega(\\mathbf{w})\n\\]\nwhere for Linear Models \\(f(w,X) = \\mathbf{w\\cdot X}\\), so that our overall cost becomes:\n\\[\nJ(w,X) = \\frac{1}{N}\\Sigma_{i=1}^N \\left(\\mathbf{y}_i - \\mathbf{w\\cdot \\mathbf{x}_i}\\right)^2 + \\alpha\\cdot\\Omega(\\mathbf{w})\n\\]\nWe’ll consider the no-penalty case (\\(\\alpha=0\\)), so that our loss is just:\n\\[\nJ(w,X) = \\frac{1}{N}\\Sigma_{i=1}^N \\left(\\mathbf{y}_i - \\mathbf{w\\cdot \\mathbf{x}_i}\\right)^2\n\\]\nLet’s plot this cost as a function of the line slope, just to get an idea of what it looks like:\nWhile it might seem clear to us, visually, where the lowest cost is, actually finding this point automatically via a computer with minimal effort is another story. This is essentially what the field of Optimization tries to do. One simple (but powerful) method of optimization is Gradient Descent. It works by taking a (possibly random) starting point (e.g., w=60), and then computing the gradient of the function at that point. Since gradients will point upwards, and we want to minimize the cost, we will instead walk in the negative gradient direction, which should move us closer to the bottom. Let’s see this on an example, by computing the gradient of our cost function above with respect to the slope (w):\n\\[\n\\begin{aligned}\n\\frac{\\partial J}{\\partial w} &=& \\frac{\\partial}{\\partial w} \\left( \\frac{1}{N}\\Sigma_{i=1}^N \\left(\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i\\right)^2 \\right) \\\\\n&=&\\frac{1}{N}\\Sigma_{i=1}^N  \\frac{\\partial}{\\partial w} \\left(\\left(\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i\\right)^2 \\right) \\\\\n&=&\\frac{2}{N}\\Sigma_{i=1}^N (\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i) \\frac{\\partial}{\\partial w} \\left(\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i \\right) \\\\\n&=&-\\frac{2}{N}\\Sigma_{i=1}^N (\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i) \\cdot \\mathbf{x}_i\n\\end{aligned}\n\\]\nLet’s plot this:\nOnce we have 1) a starting point, and 2) the gradient at a point, the idea with gradient descent is to take a small step (\\(\\alpha\\)) in the direction of the negative gradient:\n\\[\nw_{t+1} = w_t - \\alpha \\frac{\\partial J}{\\partial w}\n\\]\nNote: here we are just considering a single parameter (the slope, w), but this method extends to multiple parameters (\\(\\mathbf{\\theta}\\)), via the gradient operator:\n\\[\n\\mathbf{\\theta}_{t+1} = \\mathbf{\\theta}_t - \\alpha \\nabla_\\theta J\n\\]\nTry modifying the initial guess wg and the step size alpha and re-running the below cells. What do you observe?\n######################\n# Try changing the below\nwg = 80 # Initial guess at slope; Try changing this\nalpha = 0.1  # Try changing alpha (both big and small)\n# What do you notice?\n###########################\nnum_steps = 20 # Take 20 steps\nweights = np.zeros(num_steps)\nweights[0] = wg  # Set the initial weight\nfor i in range(1,num_steps): \n    weights[i] = grad_step(weights[i-1], X, alpha)\nprint(\"Final weight from Gradient Descent is {:.2f}\".format(weights[i]))\nprint(\"Compared to {:.2f} from the Normal Equations\".format(wn))\n\nFinal weight from Gradient Descent is 43.06\nCompared to 42.57 from the Normal Equations\nweight_cost = [loss(w) for w in weights]\nplt.figure(figsize=(10,10))\nplt.plot(wp,cost)\nplt.scatter(weights,weight_cost,facecolors='none', edgecolors='r',linewidth=1)\nax = plt.gca()\nfor i,w in enumerate(weights):\n    ax.annotate('{}'.format(i), xy=(w, weight_cost[i]-10), \n                xytext=(w+1e-8, 10+50*np.random.rand()),\n                ha='center',fontsize=8,\n                arrowprops=dict(facecolor='white', edgecolor='grey',\n                                shrink=0.05,\n                            width=1, headwidth=1)\n               )\nplt.ylabel('Cost')\nplt.xlabel('slope (w)')\nplt.show()\n# Plot how the weights progress\nplt.figure(figsize=(10,5))\nplt.plot(range(len(weights)),weights, label='GD')\nplt.hlines(wn, 0, len(weights), \n           label = \"Optimal Weight\", \n           color='k', linestyle=\"--\")\nplt.xlabel('Gradient Descent Iteration')\nplt.ylabel('Weight')\nplt.legend()\nplt.show()\n# Copying for comparison later\ngd_weights = weights",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Gradient Descent</span>"
    ]
  },
  {
    "objectID": "notebooks/supervised_linear_models.html#experiment-1",
    "href": "notebooks/supervised_linear_models.html#experiment-1",
    "title": "3  Introduction to Gradient Descent",
    "section": "4.1 Experiment:",
    "text": "4.1 Experiment:\nWhat do you notice about the behavior of SGD? What happens when alpha is small vs large? What happens when you take multiple passes through the data? If I keep doing more passes, do I eventually converge?\nWhat you are seeing is a result of Stochastic Approximation (trying to approximate a gradient of a function using noisy estimates of that gradient (where the noise here comes from evaluating the gradient using only one data point).\nThis behavior was studied by multiple people in the 1950s and 60s, with one key result coming Herbert Robbins and Sutton Monro, in what is now called the Robbins-Monro Algorithm. The central idea is rather than defining a single step size, we should let the step size decrease over time. Initially, we need to move the weights a lot, but as we get closer to the goal, they exert less influence so that we settle at some point. What they showed was that SGD would converge to the right estimator so long as the sequence satisfies the following properties:\n\\[\n\\begin{aligned}\n\\sum_{n=1}^{\\infty} a_n &= \\infty \\\\\n\\sum_{n=1}^{\\infty} a_n^2 &&lt; \\infty\n\\end{aligned}\n\\]\nFor sequences where \\(a_n&gt;0~\\forall~n&gt;0\\), Robbins and Monro recommended the \\(a_n = a/n\\), however this rate is based on some assumptions about smoothness and convexity which sometimes don’t work well in practice. People generally use decay rates on the order of \\(O(1/\\sqrt(n))\\), however there are entire fields of researchers working on this “optimal step size” problem for SGD and there are many great alternative procedures out there if you know certain things about the function (e.g., can compute things like hessians, etc.)\n\n#################################\n# Try Changing the below\nwg = 80 # Initial guess at slope\nalpha_i = 0.5  # Initial Step Size\n#alpha = lambda n: alpha_i\n#alpha = lambda n: alpha_i/n\nalpha = lambda n: alpha_i/np.sqrt(n)\nnum_passes = 5  # Number of times we pass through the data\nshuffle_after_pass = True  # Whether to shuffle the data\n##########################\n# What do you find?\n\nN = len(y)\nweights = np.zeros(N*num_passes+1)\nk=0\nweights[k] = wg  # Set the initial weight\nprint('Initial weight: ',weights[0])\n\nindex = list(range(N))\nfor n in range(num_passes):\n    if shuffle_after_pass:\n        np.random.shuffle(index)\n    for i in index:\n        k+=1\n        xi = X[i,0]\n        yi = y[i]\n        weights[k] = weights[k-1] - alpha(k)*sgd_dloss(weights[k-1],yi,xi)\nprint('Final Weight from SGD: {:.2f}'.format(weights[-1]))\nprint(\"Compared to {:.2f} (Normal Equations)\".format(wn))\n\nplt.figure(figsize=(10,5))\n# Plot SGD Weights\nplt.plot(range(len(weights)), weights,\n         marker=None,\n         label = 'SGD')\n# Plot Grad. Descent Weights\nplt.plot(np.array(range(len(gd_weights)))*len(index),\n         gd_weights,\n         marker='o',\n         label = 'GD')\n# Plot True Answer\nplt.hlines(wn, 0, len(weights), \n           label = \"Optimal Weight\", \n           color='k', linestyle=\"--\")\nplt.xlabel('# Samples Used')\nplt.ylabel('Weight')\n#plt.ylim([30,55])\nplt.xlim([0,len(weights)])\nplt.legend()\nplt.show()\n\nInitial weight:  80.0\nFinal Weight from SGD: 41.30\nCompared to 42.57 (Normal Equations)\n\n\nC:\\Users\\mafuge\\AppData\\Local\\Temp\\ipykernel_5008\\3561310644.py:2: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  return float(-2*(y-w*x)*x)\n\n\n\n\n\n\n\n\n\nWhile Gradient Descent looks much better than SGD here, let’s now scale the axis by the number of model evaluations needed:\n\nplt.figure(figsize=(10,5))\nplt.plot(range(len(weights)), weights,\n         label = 'SGD')\nplt.plot(np.array(range(len(gd_weights)))*len(index),\n         gd_weights,\n         marker='o',\n         label = 'GD')\nplt.hlines(wn, 0, len(weights), \n           label = \"Optimal Weight\", \n           color='k', linestyle=\"--\")\n#plt.ylim([35,50])\nplt.xlim([0,len(weights)])\nplt.xlabel('# Samples Used')\nplt.ylabel('Weight')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nnp.array(range(len(gd_weights)))*len(index)\n\narray([   0,  100,  200,  300,  400,  500,  600,  700,  800,  900, 1000,\n       1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900])",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Gradient Descent</span>"
    ]
  },
  {
    "objectID": "notebooks/supervised_linear_models.html#existing-implementations",
    "href": "notebooks/supervised_linear_models.html#existing-implementations",
    "title": "3  Introduction to Gradient Descent",
    "section": "4.2 Existing Implementations",
    "text": "4.2 Existing Implementations\nSGD is a fairly simple and popular technique for solving many problems where you can easily express the derivatives of those functions. For certain types of loss function (like the square error/L2 norm we discussed above), many folks have written optimized libraries for just that purpose, such as Scikit-Learn’s SGD functions including SGDRegressor.\nFor reference, the SGD Regressor in ScikitLearn uses an update rule similar to: \\[\n\\eta^{(t)} = \\frac{eta_0}{t^{power_t}}\n\\]\n\nfrom sklearn.linear_model import SGDRegressor\nsgd = SGDRegressor(loss = 'squared_error',\n                   eta0 = 0.01,  # Initial Learning rate/step size\n                   power_t = 0.25, # how quickly sould eta decay?\n                   max_iter = 100,  # Max # of passes to do over the data?\n                   tol = 1e-3,     # Tolerance for change in loss\n                   fit_intercept=False # Not worrying about b term in w*x+b\n                  )\n# Here eta = eta0/(t^power_t) where t is the iteration\nX = np.asarray(X)\ny = np.array(y).reshape(len(y),) # Reshape y so that scikit doesn't complain\nsgd.fit(X,y)\nprint('Final Weight from SKLearn SGD: {:.2f}'.format(sgd.coef_[0]))\nprint(\"Compared to {:.2f} (Normal Equations)\".format(wn))\n\nFinal Weight from SKLearn SGD: 42.55\nCompared to 42.57 (Normal Equations)\n\n\n\nplt.figure(figsize=(10,5))\nplt.scatter(np.asarray(X).ravel(),\n            np.asarray(y).ravel(),\n            color='k'\n           )\n#plt.scatter(X,y)\nXp = [[-3],[3]]\nplt.plot(Xp,sgd.predict(Xp),label='SGD')\nplt.plot([-3,3],[-3*wn, 3*wn],\n         label='Normal Eqns',\n         linestyle='--' )\nplt.legend()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Gradient Descent</span>"
    ]
  },
  {
    "objectID": "notebooks/supervised_linear_models.html#advanced-techniques",
    "href": "notebooks/supervised_linear_models.html#advanced-techniques",
    "title": "3  Introduction to Gradient Descent",
    "section": "4.3 Advanced Techniques",
    "text": "4.3 Advanced Techniques\nThere are a variety of more advanced SGD techniques, most of which involve one or more of the following tricks: 1. Using “acceleration” procedures that leverage “momentum” of some type. You can read more about this phenomenon at: “Why Momentum Really Works” 2. “Batching” the SGD updates: that is, taking steps that are considering \\(N&gt;n&gt;1\\) in size (e.g., averaging the gradients of, say, 5 data points before taking a step). This can help stablize gradients and improve convergence. 3. “Normalizing” the gradient updates: that is, re-scaling the gradient updates at each step to achieve better convergence. This is commonly used in Neural Networks for things like Batch Normalization (Wikipedia) (or, for a more advanced introduction, you can read the NeurIPS paper Understanding Batch Normalization).",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Gradient Descent</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html",
    "href": "part1/linear_decompositions.html",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "",
    "text": "4.1 Principal Component Analysis\nIn this notebook, we will briefly review some of the key concepts of linear unsupervised learning, including Principal Component Analysis (PCA) and the effects of regularizations on linear decompositions, such as SparsePCA and Non-negative Matrix Factorization (NMF). We will use a set of airfoil geometries to demonstrate these effects visually. For those who need to refresh their memory on basic concepts around Matrices and how they affect data, you can see visual examples in Appendix C\nFirst, let’s load some airfoil geometry coordinates, take a look at the shape of the data matrix, and pick a random one to visualize:\nGreat, as we can see there are 1528 airfoils, expressed as 192 surface coordinates each with an x and y value. We can turn this into a matrix compatible with a linear decomposition by flattening the last two dimensions, so that each airfoil is a row vector of length 384 (2x192).\nNow let’s demonstrate how to use various dimension reduction algorithms on this example.\nMathematically, given centered data \\(X \\in R^{n×d}\\), PCA finds k orthonormal components that best reconstruct the data in least-squares sense. One convenient formulation is\n\\[\n\\min_{W,Z}\\;\\|X - Z W\\|_{F}^{2}\\quad\\text{s.t. }Z=XW^{T},\\;W W^{T}=I.\n\\]\nEquivalently PCA maximizes the projected variance:\n\\[\n\\max_{W:\\;W W^{T}=I} \\;\\mathrm{tr}(W \\Sigma W^{T}),\\quad \\Sigma=\\frac{1}{n}X^{T}X.\n\\]\nNotes:\nfrom sklearn import decomposition\n\n# We can set the maximum number of components that we want to truncate to\n# Or can just leave it as None to get all components\nn_components = 20\nestimator = decomposition.PCA(n_components=n_components)\nZ_pca = estimator.fit_transform(data)\ncomponents_ = estimator.components_\nWe see that we now possess a matrix (i.e., linear operator) that goes from the target 20 components/dimensions back to the original 382 dimensions.\nprint(f\"The shape of the components_ matrix (W) is {components_.shape}\")\n\nThe shape of the components_ matrix (W) is (20, 384)\nIf we wanted to visualize how each of these components looks like in terms of the original airfoil coordinates, we can reshape each row of the components matrix back to the original airfoil shape:\ncomponents_.reshape((n_components, -1, 2)).shape\n\n(20, 192, 2)\nLet’s go ahead and visualize the first all of the learned components:\nCode\n# Component heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(components_, aspect='auto', cmap='RdBu_r', interpolation='nearest', \n           norm=TwoSlopeNorm(vmin=components_.min(), vcenter=0, vmax=components_.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Feature index ($d \\\\in {Z_pca.shape[0]}$)\")\nplt.ylabel(f\"Latent index ($k \\\\in {Z_pca.shape[1]}$)\")\nplt.title('PCA Component Matrix (latent codes x original features)')\nplt.show()\nWe notice a kind of alternating aliasing pattern in the components, but recall, this is because of how we reshaped the original data, which had rows of x and y coordinates interleaved. To make this clearer, we can visually re-order the indices of the components so that all the x-coordinates come first (first 192 features), followed by all the y-coordinates (second 192 features):\nCode\n# make an array with all of the odd indices of components_\ndef reorder_indices(components):\n    return np.hstack([components[:,0:-1:2],components[:,1:-1:2]])\n\n# Component heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(reorder_indices(components_), aspect='auto', cmap='RdBu_r', interpolation='nearest', \n           norm=TwoSlopeNorm(vmin=components_.min(), vcenter=0, vmax=components_.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Feature index ($d \\\\in {Z_pca.shape[0]}$)\")\nplt.ylabel(f\"Latent index ($k \\\\in {Z_pca.shape[1]}$)\")\nplt.title('PCA Component Matrix (latent codes x original features) - Reordered')\nplt.show()\nOK, this now looks a little clearer. What do you notice?\nWe can next visualize how each data point is mapped to all of the latent coordinates:\nCode\n# Latent Coordinate heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(Z_pca, aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=Z_pca.min(), vcenter=0, vmax=Z_pca.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Latent index ($k \\\\in {Z_pca.shape[1]}$)\")\nplt.ylabel(f\"Data Point index ($n \\\\in {Z_pca.shape[0]}$)\")\nplt.title('Latent codes for each Data Point')\nplt.show()\nWhy do you think the first few components are more important than the later ones? We can gain some intuition here by looking at the amount of variance explained by each component:\nCode\nplt.figure()\nplt.plot(estimator.explained_variance_)\nplt.ylabel(\"Explained Variance\")\nplt.xlabel(\"Latent Dimension\")\nplt.title(\"PCA Explained Variance\")\nplt.xticks(np.arange(n_components))\nplt.show()\nWe can also visualize the cumulative explained variance to see how many components are needed to explain a certain amount of variance in the data. For example, we can plot a line at the number of dimensions we need to keep to explain 99% of the variance:\nCode\ncumulative_explained_var_ratio = np.cumsum(estimator.explained_variance_ratio_)\nnumber_of_vars_to_99 = np.argmax(cumulative_explained_var_ratio&gt;.99)\nplt.figure()\nplt.plot(cumulative_explained_var_ratio,label=\"Cumulative Explained Variance\")\nplt.ylabel(\"Explained Variance\")\nplt.xlabel(\"Latent Dimension\")\nplt.xticks(np.arange(n_components))\nplt.vlines(number_of_vars_to_99,\n           np.min(cumulative_explained_var_ratio),1.0,\n           colors=\"k\",linestyles='dashed',\n          label = \"99% Explained Variance\")\nplt.legend()\nplt.title(\"PCA Cumulative Explained Variance\")\nplt.show()\nOr plot the explained variance ratio as a function of the number of components, which tells us how much each additional component contributes to the total explained variance:\nCode\nplt.figure()\nplt.plot(estimator.explained_variance_ratio_)\nplt.ylabel(\"Explained Variance Ratio\")\nplt.xlabel(\"Latent Dimension\")\nplt.xticks(np.arange(n_components))\nplt.title(\"PCA Explained Variance Ratio\")\nplt.show()\nWe can see that this approximately follows the singular values of the data matrix:\nCode\nprint(f\"Singular Values: {estimator.singular_values_}\")\nplt.figure()\nplt.plot(estimator.singular_values_)\nplt.ylabel(\"Singular Values\")\nplt.xlabel(\"Latent Dimension\")\nplt.xticks(np.arange(n_components))\nplt.title(\"PCA Singular Values\")\nplt.show()\n\n\nSingular Values: [7.96347431 7.30793278 2.11271967 1.880563   1.43978062 1.06857152\n 0.86184436 0.79754077 0.46006406 0.40013427 0.3503857  0.33218949\n 0.28107742 0.20708862 0.19063632 0.17582911 0.14392864 0.1288838\n 0.12505431 0.10367465]\nMoving beyond just the singular values, we can also now look at the projection of each airfoil into any of the latent dimensions. It is natural to explore the first two, since those explain the largest variance:\nCode\nz = estimator.transform(data)\nplt.figure()\nplt.scatter(z[:,0],z[:,1],alpha=0.3)\nplt.xlabel(\"1st Principal Component\")\nplt.ylabel(\"2nd Principal Component\")   \nplt.title(\"PCA Projection onto First Two Principal Components\")\nplt.show()\nWe can also pull out the “latent code/coordinates/vector” for an individual airfoil:\nz[airfoil_id,:]\n\narray([ 0.26565954, -0.30692116,  0.18797174, -0.0349173 , -0.0307379 ,\n        0.02908489, -0.02403219,  0.02460046, -0.01230913, -0.0146706 ,\n        0.0124125 ,  0.00509173, -0.01270168,  0.00192036, -0.00573131,\n        0.00744313,  0.00506638,  0.00798497,  0.01136562,  0.00033123])",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#principal-component-analysis",
    "href": "part1/linear_decompositions.html#principal-component-analysis",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "",
    "text": "There is no explicit L1/L2 penalty in vanilla PCA; the constraint \\(W W^{T}=I\\) enforces orthonormality of components.\nThe analytical solution is given by the top-k eigenvectors of the covariance (or the top-k left/right singular vectors from SVD).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.1 How does the # Components affect the Airfoil Reconstruction?\nWe can see visually the effect of including an increasing number of components by looking at how this affects the reconstruction of a single airfoil:\n\n\nCode\nfor n_components in range(2,10):\n    estimator = decomposition.PCA(n_components=n_components, whiten=False)\n    \n    # Train the model\n    estimator.fit(data)\n    # Project down to the low dimensional space\n    z     = np.dot(data - estimator.mean_, estimator.components_.T)\n    # Re-Project back to the high dimensional space\n    x_hat = np.dot(z[0], estimator.components_) + estimator.mean_\n\n    # Now plot them\n    airfoil_original = make_airfoil(data[0])\n    airfoil_reconstructed = make_airfoil(x_hat)\n    airfoil_mean = make_airfoil(estimator.mean_)\n    plt.figure()\n    plt.plot(airfoil_original[:,0],\n             airfoil_original[:,1],\n             alpha=0.75,\n             label = 'Original')\n    plt.plot(airfoil_reconstructed[:,0],\n                airfoil_reconstructed[:,1],\n                alpha=0.5,\n                label='Reconstructed')\n\n    plt.title(f\"Using {n_components} Number of Components\")\n    plt.legend()\n    plt.axis('equal')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n# Interactive top-k overlay for contribution\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=1, min=1, max=9, step=1, description='top_k')\n    def show_topk(k=4):\n        # Find the first k principal components and multiply them with the corresponding latent variables\n        contrib = np.dot(z[airfoil_id,:k], components_[:k,:])+ estimator.mean_\n        plt.figure(figsize=(6,3))\n        plt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\n        plt.plot(make_airfoil(contrib)[:,0], make_airfoil(contrib)[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '-', color='g', label='Mean')\n        plt.axis('equal')\n        plt.title(f'Top-{k} component contributions (overlay)')\n        plt.legend()\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_topk, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n\n\n\n\n\n\nCode\n# Interactive visualization of each component\nif widgets is not None:\n    component_selector = widgets.IntSlider(value=1, min=1, max=n_components, step=1, description='component')\n    def show_component(k=4):\n        # Find the first k principal components and multiply them with the corresponding latent variables\n        #contrib = np.dot(z[airfoil_id,:k], components_[:k,:])+ estimator.mean_\n        plt.figure(figsize=(6,5))\n        selected_component = components_[k-1,:]+ estimator.mean_\n        plt.plot(make_airfoil(selected_component)[:,0], make_airfoil(selected_component)[:,1], label='Component', alpha=1.0)\n        plt.axis('equal')\n        plt.title(f'Component {k}')\n        plt.xlim(-0.6,0.6)\n        plt.show()\n    display(widgets.VBox([component_selector, widgets.interactive_output(show_component, {'k': component_selector})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#sparse-pca",
    "href": "part1/linear_decompositions.html#sparse-pca",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "4.2 Sparse PCA",
    "text": "4.2 Sparse PCA\nSparse PCA encourages components that have many zeros, which can make the learned components easier to interpret as localized shape features. Below we fit a SparsePCA model and visualize a small set of sparse components and their effect on reconstructing an example airfoil.\nMathematically, Sparse PCA augments a PCA-style reconstruction loss with an L1 penalty on the (unconstrained) components to encourage sparsity. A common optimization is:\n\\[\\min_{Z,W}\\;\\|X - ZW\\|_{F}^{2} + \\alpha\\|W\\|_{1}\\quad\\text{s.t. }\\|Z_{i}\\|_{2}^{2}\\le 1\\;\\forall i,\\,\\]\nwhere \\(W \\in R^{k×d}\\) holds the component vectors (rows), \\(Z \\in R^{n×k}\\) are the codes, and \\(\\alpha&gt;0\\) controls sparsity. Different implementations (e.g., the sklearn SparsePCA) solve related objectives (sometimes using a LASSO subproblem or elastic-net style updates).\nNotes:\n\nThe L1 term on W encourages many component weights to be exactly zero, producing localized/part-like components.\nIn practice one often also adds a small L2 (ridge) term to stabilize optimization (an elastic-net variant).\n\n\nfrom sklearn.decomposition import SparsePCA\n\n#### Try Changing both n_components and alpha  ########\nn_components = 12\n# Warning: setting alpha too low will take the algorithm a long time to converge\nalpha = 0.01\n##########################\nspca = SparsePCA(n_components=n_components, alpha=alpha, random_state=42)\n# fit on the flattened data matrix 'data' used above\nspca.fit(data)\nW_sp = spca.components_\nZ_sp = spca.transform(data)\n\nNow let’s take a look at what the Sparse PCA model has learned and how it reconstructs the example airfoil differently than vanilla PCA:\n\n\nCode\n# Component heatmap (components x features)\nplt.figure(figsize=(10,4))\nplt.imshow(reorder_indices(W_sp), aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=W_sp.min(), vcenter=0, vmax=W_sp.max()))\nplt.colorbar(label='weight')\nplt.ylabel(f\"Latent index ($k \\\\in {W_sp.shape[0]}$)\")\nplt.xlabel(f\"Feature index ($d \\\\in {W_sp.shape[1]}$)\")\nplt.title('SparsePCA component weights (components x features)')\nplt.show()\n\n# Latent Coordinate heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(Z_sp, aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=Z_sp.min(), vcenter=0, vmax=Z_sp.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Latent index ($k \\\\in {Z_sp.shape[1]}$)\")\nplt.ylabel(f\"Data Point index ($n \\\\in {Z_sp.shape[0]}$)\")\nplt.title('Latent codes for each Data Point')\nplt.show()\n\n# Now let's try reconstructing an airfoil from the trained model\n# Pull out the latent representation of this airfoil ID\nz = Z_sp[airfoil_id]\nx_hat = spca.inverse_transform(z.reshape(-1, 1).T)\n# Alternatively, we could do the following manual reconstruction:\n#x_hat = np.dot(z, W_sp) + data.mean(axis=0)\n\n# Interactive visualization of each component\nif widgets is not None:\n    component_selector = widgets.IntSlider(value=1, min=1, max=n_components, step=1, description='component')\n    def show_component(k=4):\n        # Find the first k principal components and multiply them with the corresponding latent variables\n        #contrib = np.dot(z[airfoil_id,:k], components_[:k,:])+ estimator.mean_\n        plt.figure(figsize=(6,5))\n        selected_component = W_sp[k-1,:] + data.mean(axis=0)\n        plt.plot(make_airfoil(selected_component)[:,0], make_airfoil(selected_component)[:,1], label='Component', alpha=1.0)\n        plt.axis('equal')\n        plt.title(f'Component {k}')\n        #plt.legend()\n        plt.xlim(-0.6,0.6)\n        #plt.ylim(-0.4,0.4)\n        plt.show()\n    display(widgets.VBox([component_selector, widgets.interactive_output(show_component, {'k': component_selector})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n# Interactive top-k overlay for contribution\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=4, min=1, max=n_components, step=1, description='top_k')\n    def show_topk(k=4):\n        top_idx = np.argsort(np.abs(z))[::-1][:k]\n        contrib = np.sum(z[top_idx][:,None] * W_sp[top_idx], axis=0)\n        plt.figure(figsize=(6,3))\n        plt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\n        plt.plot(make_airfoil(contrib + data.mean(axis=0))[:,0], make_airfoil(contrib + data.mean(axis=0))[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '--', color='g', label='Mean')\n        plt.axis('equal')\n        plt.title(f'Top-{k} component contributions (overlay)')\n        plt.legend()\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_topk, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Comparison between PCA and SparsePCA\n\n\n\nCompare both the component weight matrix and the reconstruction of a single airfoil when we used PCA and SparsePCA.\nConsider the following questions:\n\nWhat are the main differences in the component weights between PCA and SparsePCA? How does the addition of the L1 penalty within SparsePCA manifest itself in the weight matrix? How does this manifest itself in the learned components, either individually or as we add them up during reconstruction?\nAlso compare the latent codes \\(Z\\) between PCA and SparsePCA. How do they differ? What does this tell you about how the data is represented in the latent space? How about which components are most important?\nFor SparsePCA, how does changing the value of the sparsity parameter \\(\\alpha\\) affect the learned components and reconstruction? What happens when we increase or decrease \\(\\alpha\\)? Look at both the weight matrix and the components.\nIf you set \\(\\alpha\\) very low (e.g., 0.001) and increase the number of components, how does the result compare to vanilla PCA with the same number of components? What would you expect to happen, comparing the loss functions? Why are they different?\nUnder what practical circumstances or for what types of problems might SparsePCA be preferred over PCA and vice versa?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#dictionary-learning",
    "href": "part1/linear_decompositions.html#dictionary-learning",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "4.3 Dictionary Learning",
    "text": "4.3 Dictionary Learning\nDictionary learning finds a set of atoms (basis elements) and sparse codes that reconstruct the data. This is useful when localized, part-based representations are desirable. Essentially, unlike Sparse PCA (which adds an L1 penalty to the components), dictionary learning adds an L1 penalty to the codes. Below we fit a Dictionary Learning model and visualize a small set of learned atoms and their effect on reconstructing an example airfoil.\nMathematically, Dictionary learning (sparse coding) models X as the product of a dictionary \\(W \\in R^{k×d}\\) (atoms) and sparse codes \\(Z \\in R^{n×k}\\):\n\\[\n\\min_{Z,W}\\;\\|X - Z W\\|_{F}^{2} + \\alpha\\|Z\\|_{1} \\,\\quad\\text{s.t. }\\|W_{j}\\|_{2}\\le 1\\; \\forall j.\n\\]\nHere Z contains the sparse coefficients for each example and \\(\\alpha&gt;0\\) controls the sparsity of the codes. The constraint on \\(W\\) prevents trivial scaling to reduce the penalty term (i.e., just pulling weight into W and shrinking Z). Intuitively, each data point is reconstructed as a sparse linear combination of dictionary atoms, essentially selecting a few “parts” (where the parts are elements of \\(W\\)) to compose the whole as a weighted sum.\nNotes:\n\nThe L1 penalty acts on the coefficients (Z) rather than \\(W\\) to encourage sparse usage of dictionary elements.\nMany algorithms alternate between solving for Z (a LASSO-type problem) and updating W (a constrained least-squares step).\n\n\n# Warning, running this particular cell takes a long time as the sklearn implementation \n# is not particularly fast. (~4-8 mins)\nfrom sklearn.decomposition import DictionaryLearning\n# Alternatively, you can use the MiniBatchDictionaryLearning which is faster, but less accurate or stable:\n#from sklearn.decomposition import MiniBatchDictionaryLearning as DictionaryLearning\n\n\n### Try changing n_components and alpha ########\nn_components = 10\nalpha = 0.1\n##########################\n\nDL = DictionaryLearning(n_components=n_components, alpha=alpha, random_state=42)\nZ_dl = DL.fit_transform(data)\nW_dl = DL.components_\n\n\n\nCode\n# Visualize first few atoms\nplt.figure(figsize=(12,6))\nfor i in range(n_components):\n    plt.subplot(4,6,i+1)\n    xy = make_airfoil(W_dl[i])\n    plt.plot(xy[:,0], xy[:,1], lw=1)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title(f'Atom {i}')\n    plt.axis('equal')\n    plt.xticks([])\n    plt.yticks([])\nplt.suptitle(f'Dictionary atoms (n={n_components}, alpha={alpha})')\nplt.show()\n\n# Component heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(reorder_indices(W_dl), aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=W_dl.min(), vcenter=0, vmax=W_dl.max()))\nplt.colorbar(label='weight')\nplt.ylabel(f\"Latent index ($k \\\\in {n_components}$)\")\nplt.xlabel(f\"Feature index ($d \\\\in {W_dl.shape[1]}$)\")\nplt.title('Dictionary of Features (aka Component weights)')\nplt.show()\n\n# Latent Coordinate heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(Z_dl, aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=Z_dl.min(), vcenter=0, vmax=Z_dl.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Latent index ($k \\\\in {n_components}$)\")\nplt.ylabel(f\"Data Point index ($n \\\\in {Z_dl.shape[0]}$)\")\nplt.title('Latent codes for each Data Point')\nplt.show()\n\n# Reconstruction an airfoil using the learned codes\nx_hat = np.dot(Z_dl[airfoil_id], W_dl)\nplt.figure(figsize=(6,3))\nplt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\nplt.plot(make_airfoil(x_hat)[:,0], make_airfoil(x_hat)[:,1], label='Reconstruction', color='C3')\nplt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '--', color='g', label='Mean')\nplt.axis('equal')\nplt.title('Comparison of Original and Reconstructed Airfoil')\nplt.legend()\nplt.show()\n\n# Interactive top-k atoms display\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=6, min=1, max=n_components, step=1, description='top_k')\n    def show_top_atoms(k=6):\n        top_idx = np.argsort(np.abs(Z_dl[airfoil_id]))[::-1][:k]\n        plt.figure(figsize=(8,4))\n        for i, idx in enumerate(top_idx):\n            plt.subplot(1,k,i+1)\n            plt.plot(make_airfoil(W_dl[idx])[:,0], make_airfoil(W_dl[idx])[:,1])\n            plt.axis('equal')\n            plt.title(f'atom {idx}')\n            plt.xticks([])\n            plt.yticks([])\n        plt.suptitle('Top atoms used for this example')\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_top_atoms, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n# Interactive top-k overlay for contribution\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=4, min=1, max=8, step=1, description='top_k')\n    def show_topk(k=4):\n        top_idx = np.argsort(np.abs(Z_dl[airfoil_id]))[::-1][:k]\n        #contrib = np.sum(z[top_idx][:,None] * W_sp[top_idx], axis=0)\n        contrib = np.sum(Z_dl[airfoil_id,top_idx][:,None] * W_dl[top_idx], axis=0)\n        plt.figure(figsize=(6,3))\n        plt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\n        #plt.plot(make_airfoil(contrib + data.mean(axis=0))[:,0], make_airfoil(contrib + data.mean(axis=0))[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(contrib)[:,0], make_airfoil(contrib)[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '--', color='g', label='Mean')\n        plt.axis('equal')\n        plt.title(f'Top-{k} component contributions (overlay)')\n        plt.legend()\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_topk, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n# Show sparsity level of codes\nsparsity = np.mean(np.abs(Z_dl) &lt; 1e-6)\nprint(f'Average fraction of near-zero coefficients: {sparsity:.3f}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage fraction of near-zero coefficients: 0.540\n\n\n\n\n\n\n\n\nTipExperiment: Comparison between Dictionary Learning (Sparse Coding) and SparsePCA\n\n\n\nCompare both the component weight matrix and the reconstruction of a single airfoil when we used Sparse PCA and Dictionary Learning (Sparse Coding).\nConsider the following questions:\n\nBoth algorithms use the same sparsity penalty (L1), but on different matrices. How does this difference manifest itself in the learned components and reconstructions? What happens to the component weights versus the latent codes?\nFor Dictionary Learning, how does changing the value of the sparsity parameter \\(\\alpha\\) affect the learned components and reconstruction? What happens when we increase or decrease \\(\\alpha\\)? Look at both the weight matrix and the components.\nUnder what practical circumstances or for what types of problems might Dictionary Learning (Sparse Coding) be preferred over PCA and Sparse PCA, and vice versa?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#non-negative-matrix-factorization-nmf",
    "href": "part1/linear_decompositions.html#non-negative-matrix-factorization-nmf",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "4.4 Non-Negative Matrix Factorization (NMF)",
    "text": "4.4 Non-Negative Matrix Factorization (NMF)\nNMF constrains both the basis elements and coefficients to be non-negative. This often yields parts-based, additive representations which can be intuitive for shape components under certain circumstances.\nMathematically, Non-negative matrix factorization approximates \\(X\\) (with \\(X\\gt 0\\) after shift) as the product of non-negative factors \\(W\\in R^{n×k}_{+}\\) and \\(H \\in R^{k×d}_{+}\\) by minimizing reconstruction error under non-negativity constraints:\n\\[\\min_{W\\ge 0,H\\ge 0}\\;\\|X - Z W\\|_{F}^{2} + \\beta_Z\\|Z\\|_{1} + \\beta_W\\|W\\|_{1} + \\alpha_Z\\|Z\\|^{2}_{F} + \\alpha_W\\|W\\|^{2}_{F},\\]\nwhere optional L1 penalties (\\(\\beta_Z\\), \\(\\beta_W \\ge 0\\)) encourage sparse parts or sparse activations, and L2 penalties (\\(\\alpha_Z\\), \\(\\alpha_W \\ge 0\\)) encourage stability or shrinkage. The essential constraint is \\(W,Z \\ge 0\\) which induces additive, parts-based representations.\nNotes:\n\nIn this notebook we shift data by the minimum to ensure \\(X \\ge 0\\) before fitting and then undo the shift when plotting reconstructions.\nTypical solvers use multiplicative updates or alternating non-negative least squares; regularization (L1 or L2) can be added to encourage sparsity or stability.\n\n\nfrom sklearn.decomposition import NMF\n\n#### Try changing n_components ########\nn_components = 14\n# This is the L2 penalty on the components\nalpha_W  = 0.001\n# This is the L1 ratio on the components (between 0 and 1)\n# Set to 0 for pure L2, 1 for pure L1, and in between for a mix\nl1_ratio = 0.001\n##########################\n\n# NMF needs non-negative data. We'll shift by the min and remember the offset.\nshift = data.min()\nXpos = data - shift + 1e-6\n\nnmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=5000,\n          alpha_W = alpha_W, l1_ratio=l1_ratio)\nZ_nmf = nmf.fit_transform(Xpos)\nW_nmf = nmf.components_\n\n\n\nCode\n# Visualize NMF components (H) as parts\nplt.figure(figsize=(12,4))\nfor i in range(n_components):\n    plt.subplot(2, n_components//2, i+1)\n    xy = make_airfoil(W_nmf[i])\n    plt.plot(xy[:,0], xy[:,1], lw=1)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.axis('equal')\n    plt.title(f'Part {i}')\n    plt.xticks([])\n    plt.yticks([])\nplt.suptitle('NMF learned parts (components)')\nplt.show()\n\n# Component heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(reorder_indices(W_nmf), aspect='auto', cmap='Reds', interpolation='nearest')\nplt.colorbar(label='weight')\nplt.xlabel('feature index')\nplt.ylabel('part index')\nplt.title('NMF part weights (parts x features)')\nplt.show()\n\n# Latent Coordinate heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(Z_nmf, aspect='auto', cmap='Reds', interpolation='nearest')\nplt.colorbar(label='weight')\nplt.xlabel(f\"Latent index ($k \\\\in {Z_nmf.shape[1]}$)\")\nplt.ylabel(f\"Data Point index ($n \\\\in {Z_nmf.shape[0]}$)\")\nplt.title('Latent codes for each Data Point')\nplt.show()\n\n# Reconstruction the airfoil using W @ H\nx_hat_pos = np.dot(Z_nmf[airfoil_id], W_nmf)\n# undo the shift to bring back to original centered data\nx_hat = x_hat_pos + shift - 1e-6\nmean_airfoil = make_airfoil(data.mean(axis=0))\n\n# Use plot_reconstruction helper for consistent display; but NMF uses shifted data so show overlay manually\nplt.figure(figsize=(6,3))\n#plt.subplot(1,2,1)\nplt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original')\nplt.plot(mean_airfoil[:,0], mean_airfoil[:,1], color='g', label='Mean')\nplt.plot(make_airfoil(x_hat)[:,0], make_airfoil(x_hat)[:,1], label='Reconstructed')\nplt.title('Original')\nplt.axis('equal')\nplt.legend()\nplt.show()\n\n# Interactive top-k parts\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=6, min=1, max=n_components, step=1, description='top_k')\n    def show_top_parts(k=6):\n        top_idx = np.argsort(Z_nmf[airfoil_id])[::-1][:k]\n        plt.figure(figsize=(8,3))\n        for i, idx in enumerate(top_idx):\n            plt.subplot(1,k,i+1)\n            plt.plot(make_airfoil(W_nmf[idx])[:,0], make_airfoil(W_nmf[idx])[:,1])\n            plt.axis('equal')\n            plt.title(f'part {idx}')\n            plt.xlim(-0.1,0.6)\n            plt.xticks([])\n            plt.yticks([])\n        plt.suptitle('Top parts used for this example')\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_top_parts, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually')\n\n# Interactive top-k overlay for contribution\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=4, min=1, max=14, step=1, description='top_k')\n    def show_topk(k=4):\n        top_idx = np.argsort(np.abs(Z_nmf[airfoil_id]))[::-1][:k]\n        contrib = np.sum(Z_nmf[airfoil_id,top_idx][:,None] * W_nmf[top_idx], axis=0)\n        plt.figure(figsize=(6,3))\n        plt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\n        #plt.plot(make_airfoil(contrib + data.mean(axis=0))[:,0], make_airfoil(contrib + data.mean(axis=0))[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(contrib+shift)[:,0], make_airfoil(contrib+shift)[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '--', color='g', label='Mean')\n        plt.axis('equal')\n        plt.title(f'Top-{k} component contributions (overlay)')\n        plt.legend()\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_topk, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\nprint(f'Explained variance (approx): {1 - np.linalg.norm(Xpos - Z_nmf.dot(W_nmf)) / np.linalg.norm(Xpos):.3f}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplained variance (approx): 0.994",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#model-comparison",
    "href": "part1/linear_decompositions.html#model-comparison",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "4.5 Model Comparison",
    "text": "4.5 Model Comparison\nFinally, let’s compare the different models side-by-side. We will visualize the two two learned projections of the data into the latent space, as well as the component reconstruction matrix that translates those components back into the original feature space, and lastly the mapping of all data points to all latent coordinates. This is similar to plots you have seen before, but it is nice to see them all together for comparison.\n\n\nCode\n# Comparison: first-two latent projections and component heatmaps for each method\nfrom sklearn.decomposition import PCA\n\nmodels_info = {}\n# PCA (reuse estimator from above if present, otherwise fit)\npca = PCA(n_components=8, random_state=42)\nZ_pca = pca.fit_transform(data)\nmodels_info['PCA'] = {'Z': Z_pca, 'components': pca.components_}\n\n# SparsePCA (use Z_sp, W_sp computed earlier)\nmodels_info['SparsePCA'] = {'Z': Z_sp, 'components': W_sp}\n\n# DictionaryLearning: use W as Z and H as components\nmodels_info['DictionaryLearning'] = {'Z': Z_dl, 'components': W_dl}\n\n# NMF: use W as Z (already non-negative)\nmodels_info['NMF'] = {'Z': Z_nmf, 'components': W_nmf}\n\n# Plot the first-two latent projections\nplt.figure(figsize=(10,8))\nfor i, (name, info) in enumerate(models_info.items()):\n    plt.subplot(2,2,i+1)\n    Z = info['Z']\n    plt.scatter(Z[:,0], Z[:,1], alpha=0.2, s=8)\n    plt.xlabel('Latent dim 1')\n    plt.ylabel('Latent dim 2')\n    plt.title(f'{name} projection (first two dims)')\nplt.tight_layout()\nplt.show()\n\n# Plot heatmaps of component/weight matrices (showing first 120 features for clarity)\nplt.figure(figsize=(12,8))\nfor i, (name, info) in enumerate(models_info.items()):\n    comps = info['components']\n    # ensure comps is (n_components, n_features)\n    if comps.ndim == 1:\n        comps = comps[None, :]\n    plt.subplot(4,1,i+1)\n    if np.any(comps &lt; 0):\n        plt.imshow(reorder_indices(comps), aspect='auto', cmap='RdBu_r', interpolation='nearest',\n                   norm=TwoSlopeNorm(vmin=comps.min(), vcenter=0, vmax=comps.max()))\n    else:\n        plt.imshow(reorder_indices(comps), aspect='auto', cmap='Reds', interpolation='nearest')\n    plt.colorbar()\n    plt.title(f'{name} components (components x features)')\n    plt.ylabel('component')\nplt.tight_layout()\nplt.show()\n\n# Plot heatmaps of component/weight matrices (showing first 120 features for clarity)\nplt.figure(figsize=(12,8))\nfor i, (name, info) in enumerate(models_info.items()):\n    Z_map = info['Z']\n    # ensure comps is (n_components, n_features)\n    if Z_map.ndim == 1:\n        Z_map = Z_map[None, :]\n    plt.subplot(4,1,i+1)\n    if np.any(Z_map &lt; 0):\n        plt.imshow(Z_map, aspect='auto', cmap='RdBu_r', interpolation='nearest',\n                   norm=TwoSlopeNorm(vmin=Z_map.min(), vcenter=0, vmax=Z_map.max()))\n    else:\n        plt.imshow(Z_map, aspect='auto', cmap='Reds', interpolation='nearest')\n    plt.colorbar()\n    plt.title(f'{name} components (components x features)')\n    plt.ylabel('component')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html",
    "href": "part1/taking_derivatives.html",
    "title": "5  Taking Derivatives with Automatic Differentiation",
    "section": "",
    "text": "5.1 Automatic Differentiation\nTaking derivatives with respect to functions or parameters is one of the most common and fundamental operations that we will need for Machine Learning (and in fact for Scientific Computing, in general). Throughout your life so far, you have probably learned about three main ways to compute derivatives:\nThis chapter won’t cover those approaches in detail, as its main goal is to introduce you to a fourth way to compute derivatives called Automatic Differentiation (AD). This approach inherits some of the benefits of analytical and symbolic differentiation, in that it computes exact derivatives (unlike numerical differentiation). It’s main drawback is that the effort used to compute the derivatives will be only useful for a single evaluation point (similar to numerical differentiation). Unlike symbolic differentiation, AD is very efficient and does not suffer from expression swell, although it does require extra bookkeeping to keep track of intermediate values and derivatives (as we will see), which can add some memory and computational overhead.\nWith this overview in mind, we can now introduce Automatic Differentiation using a simple example, and then later demonstrate how to use it via some practical examples.\nTo demonstrate how Automatic Differentiation works, let’s take a simple example function, for which we can easily compute the derivatives manually/analytically, so that we can check out results. Let’s consider the function:\n\\[\nf(x_1, x_2) = \\ln(x_1) + x_1 \\cdot x_2 - \\sin(x_2)\n\\]\nevaluated at \\(x_1 = 2, x_2 = 5\\).\nAnalytically, this function is simple enough that we could actually compute the partial derivatives manually: \\[\\frac{\\partial y}{\\partial x_0} = \\frac{1}{x_0} + x_1 = 0.5 + 5 = 5.5\\]\n\\[\\frac{\\partial y}{\\partial x_1} = x_0 - cos(x_1) = 2 - 0.284 = 1.716\\]\nHowever, for the sake of this example, we will use Automatic Differentiation to compute these derivatives instead. The first step of Automatic Differentiation is to build up a Computational Graph of the function using a library of easy to compute derivatives (e.g., \\(\\frac{\\partial}{\\partial x} x^n \\rightarrow n\\cdot x^{n-1}\\))",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives with Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html#automatic-differentiation",
    "href": "part1/taking_derivatives.html#automatic-differentiation",
    "title": "5  Taking Derivatives with Automatic Differentiation",
    "section": "",
    "text": "5.1.1 Build the Computational Graph\nTo build the computational graph, we break the function down into its elementary operations step by step, starting from the beginning (i.e., the inputs to the function). We will introduce intermediate variables (\\(V_\\#\\)) to represent the outputs of these intermediate operations. Let’s define:\n\n\n\n\n\ngraph LR\n    %% Input nodes\n    V0((V0 = x1))\n    V1((V1 = x2))\n    %% Intermediate nodes\n    V2((V2 = ln V0))\n    V3((V3 = V0 x V1))\n    V4((V4 = -sin V1))\n    V5((V5 = V2 + V3))\n    V6((V6 = V5 + V4 = f))\n\n    %% Operations\n    V0 --&gt; V2\n    V0 --&gt; V3\n    V1 --&gt; V3\n    V1 --&gt; V4\n\n    %% Sum nodes\n    V2 --&gt; V5\n    V3 --&gt; V5\n    V5 --&gt; V6\n    V4 --&gt; V6\n\n\n\n\n\n\nNow with the graph in place, we can compute the first stage of Automatic Differentiation, which is the forward pass through the function.\n\n\n5.1.2 Compute the Forward Pass\nThe Forward Pass is simply evaluating the function at the given input values, but we will do this step by step, following the computational graph we just built. It will be useful to keep track of the intermediate values in a table for reference later. (In reality, the computer will do this for us, but we are doing it by hand here to illustrate the process.)\n\n\n\n\n\n\n\n\nNode\nDefinition\nValue\n\n\n\n\n\\(V0\\)\n\\(x_1\\)\n2\n\n\n\\(V1\\)\n\\(x_2\\)\n5\n\n\n\\(V2\\)\n\\(\\ln(V0)\\)\n\\(\\ln(2) \\approx 0.6931\\)\n\n\n\\(V3\\)\n\\(V0 \\cdot V1\\)\n\\(2 \\cdot 5 = 10\\)\n\n\n\\(V4\\)\n\\(-\\sin(V1)\\)\n\\(-\\sin(5) \\approx 0.9589\\)\n\n\n\\(V5\\)\n\\(V2 + V3\\)\n\\(0.6931 + 10 = 10.6931\\)\n\n\n\\(V6\\)\n\\(V5 + V4\\)\n\\(10.6931 + 0.9589 \\approx 11.6520\\)\n\n\n\nIf we pass in our initial points (x1, x2) through this forward pass, we can look at the final node (V6) to get the answer: \\[\nf(2, 5) \\approx 11.6520\n\\]\nGreat, this matches what we would expect. At this point, we have just evaluated the function forward, and we don’t yet have any derivatives. From here, things get interesting and bifurcate into two main types of Automatic Differentiation: Forward Mode AD and Backward Mode AD. Each of these has important but different uses for reasons that will become clear as we work through the example. Let’s start with Forward Mode AD. In both cases, we will start with the initial work we already did with the forward pass above.\n\n\n5.1.3 Computing Forward Mode AD (tangent propagation)\nForward Mode AD allows us to compute directional derivatives of the function, as well as that same directional derivative at any intermediate node in the computational graph. This is useful for computing derivatives of functions that have a small number of inputs (e.g., 1-10), but potentially a large number of outputs that we might be interested in. For example, if we were computing a trajectory of a dynamical system, we might want to know how the final state of the system changes with respect to some initial condition. In this case, the initial condition is the input, and the final state is the output. There might be many intermediate states along the way that we also want to know how they change with respect to the initial condition, such as the state or total energy at each time step. Forward Mode AD allows us to compute all of these derivatives in only a single pass through the computational graph.\nTo see how this works, we will introduce a new variable \\(\\dot{V}\\) to represent the derivative of each node with respect to some input direction. We will use the notation \\(\\dot{V} = \\frac{dV}{dx}\\), where \\(x\\) is some input variable.\nCase A: derivative wrt \\(x_1\\) (\\(\\dot{x}_1 = 1, \\dot{x}_2 = 0\\))\n\n\n\n\n\n\n\n\nNode\nDefinition\nValue\n\n\n\n\n\\(\\dot{V_0}\\)\n\\(\\dot{x_1}\\)\n1\n\n\n\\(\\dot{V_1}\\)\n\\(\\dot{x_2}\\)\n0\n\n\n\\(\\dot{V_2}\\)\n\\(d(\\ln V_0)/dV_0 = 1/V_0 \\dot{V_0}\\)\n\\((1/2)\\cdot 1 = 0.5\\)\n\n\n\\(\\dot{V_3}\\)\n\\(d(V_0 \\cdot V_1)/dV_0 = \\dot{V_0} V_1 + V_0 \\dot{V_1}\\)\n\\(5\\cdot1+2\\cdot0=5\\)\n\n\n\\(\\dot{V_4}\\)\n\\(-\\cos(V_1)\\dot{V_1}\\)\n\\(-\\cos(5)\\cdot 0 = 0\\)\n\n\n\\(\\dot{V_5}\\)\n\\(\\dot{V_3} + \\dot{V_2}\\)\n\\(0.5 + 5 = 5.5\\)\n\n\n\\(\\dot{V_6}\\)\n\\(\\dot{V_5} + \\dot{V_4}\\)\n\\(5.5 + 0 = 5.5\\)\n\n\n\n\\[\n\\frac{\\partial f}{\\partial x_1} = 5.5\n\\]\nCase B: derivative wrt \\(x_2\\) (\\(\\dot{x}_1 = 0, \\dot{x}_2 = 1\\))\nExercise: Fill in a similar table as above to compute the derivative of the function with respect to \\(x_2\\) using Forward Mode AD.\nYou can check your answer using the known analytical derivative of the function that you can compute by hand: \\[\n\\frac{\\partial f}{\\partial x_2} \\approx 1.7163\n\\]\nOK, great, we see that with some bookkeeping, we have correctly computed the derivatives of the function with respect to each input variable. Note that we had to do two passes through the computational graph to get both derivatives, since they were different directional derivatives. If we had a third input variable, we would need a third pass, and so on. This is why Forward Mode AD is best suited for functions with a small number of inputs.\nOn the flip side, if we were interested in the derivative of some intermediate node in the computational graph with respect to an input variable, we would have that information available as well. For example, if we wanted to know how \\(V3\\) changes with respect to \\(x_1\\), we can see from the table above that \\(\\frac{\\partial V3}{\\partial x_1} = 5\\). We got this in the process of computing the final function, so this derivative comes “along for the ride” without additional cost on our part. This is a powerful feature of Forward Mode AD that we will see is not available in Backward Mode AD: we can get a specific directional derivatives of any intermediate node from the computational graph via the same Forward Mode pass, but the cost of this scales with the number of input variables or number of directional derivatives we want to compute.\n\n\n5.1.4 Computing Reverse Mode AD (backpropagation)\nWhile Forward Mode AD efficiently found directional derivatives, if we wanted to compute the full gradient of the output with respect to all of the input variables, we would need to do a separate pass for each input variable. This means that the cost of computing the full gradient scales with the number of input variables. If we have a function with a large number of input variables (e.g., 1000s or more), this can be very expensive. To address this, we can use Reverse Mode AD, which will allow us to compute the full gradient of every input variable to a function using a single “backward pass” through the computational graph. This is particularly useful for functions that have a small number of outputs (e.g., 1-10), but a very large number of inputs, such as a Neural Network in Machine Learning or mesh coordinates in a Finite Element or Computational Fluid Dynamics simulation.\nTo see how this works, we will introduce a new variable \\(\\bar{V}\\) to represent the adjoint of each node with respect to the output. We will use the notation \\(\\bar{V} = \\frac{\\partial f}{\\partial V}\\), where \\(f\\) is the final output of the function. The adjoint represents how much a small change in that intermediate node would affect the final output of the function.\nSimilarly to Forward Mode AD, we will have to pick a specific output that we wish to compute the gradient with respect to. In this case, since we are computing the gradient of the output with respect to all input variables, we can set the final output node with a value of 1, i.e., \\(\\bar{V6} = 1\\). From there, we will propagate each adjoint backward through the computational graph using the chain rule.\n\n\n\n\n\n\n\n\nNode\nEquation for the adjoint\nValue\n\n\n\n\n\\(\\bar{V_6}\\)\n\\(\\frac{\\partial f}{\\partial V_6}\\)\n1\n\n\n\\(\\bar{V_5}\\)\n\\(\\frac{\\partial f}{\\partial V_6} \\frac{\\partial V_6}{\\partial V_5} = \\bar{V_6}1\\)\n1\n\n\n\\(\\bar{V_4}\\)\n\\(\\frac{\\partial f}{\\partial V_6} \\frac{\\partial V_6}{\\partial V_4} = \\bar{V_6}1\\)\n1\n\n\n\\(\\bar{V_3}\\)\n\\(\\frac{\\partial f}{\\partial V_5} \\frac{\\partial V_5}{\\partial V_3} = \\bar{V_5}1\\)\n1\n\n\n\\(\\bar{V_2}\\)\n\\(\\frac{\\partial f}{\\partial V_5} \\frac{\\partial V_5}{\\partial V_2} = \\bar{V_5}1\\)\n1\n\n\n\\(\\bar{V_1}\\)\nfrom V3: \\(\\bar{V_3}\\cdot V_1=2\\)  + from V4: \\(-\\cos(5)\\cdot \\bar{V_4} \\approx -0.2837\\)\n1.7163\n\n\n\\(\\bar{V_0}\\)\nfrom V2: \\(\\bar{V_2}(1/V_0)\\cdot1=0.5\\)  + from V3: \\(\\bar{V_3}\\cdot V_1=5\\)\n5.5\n\n\n\nWe can now verify the final gradients, which match our analytical solution:\n\\[\n\\nabla f(2,5) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2} \\right) = (5.5, \\, 1.7163)\n\\]\nHowever, unlike Forward Mode AD, we see that we now have access not only to the gradients of the input variables, but also to the gradients of all intermediate nodes in the computational graph, and we received all of them via the same amount of work/computation!",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives with Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html#pytorch-autograd-example-with-simple-function",
    "href": "part1/taking_derivatives.html#pytorch-autograd-example-with-simple-function",
    "title": "5  Taking Derivatives with Automatic Differentiation",
    "section": "5.2 PyTorch Autograd Example with Simple Function",
    "text": "5.2 PyTorch Autograd Example with Simple Function\nOk let’s use automatic differentiation to compute a simple derivative of our earlier analytical function:\n\\[y = f(x_0,x_1) = \\ln(x_0) + x_0 \\cdot x_1 - \\sin(x_1)\\]\nAnd as with before, we’ll evaluate the derivative of this function at \\[x_0 = 2, x_1=5\\]\nWe can analytically compute the derivative and code it up so that we can verify accuracy later:\n\nimport numpy as np\ndef true_grad(x0,x1):\n    return np.array([\n        1/x0 + x1,\n        x0 - np.cos(x1)\n    ])\ntrue_grad(2,5)\n\narray([5.5       , 1.71633781])\n\n\nBut now let’s see how to use PyTorch to get this using Automatic Differentiation:\n\n5.2.1 Automatic Differentiation using PyTorch\n\nimport torch\nx = torch.tensor([2.0, 5.0], requires_grad=True)\nprint(x)\n\ntensor([2., 5.], requires_grad=True)\n\n\n\ndef f(x):\n    return torch.log(x[0]) + x[0]*x[1] - torch.sin(x[1])\ny = f(x)\nprint(y)\n\ntensor(11.6521, grad_fn=&lt;SubBackward0&gt;)\n\n\n\nx.grad\n\n\n# Now call the backward AD pass so that we can compute gradients\ny.backward()\n# Now we can ask for the gradient:\nx.grad\n\ntensor([5.5000, 1.7163])\n\n\nLet’s see how well it approximated the true gradient:\n\ntrue_grad(2,5) - x.grad.numpy()\n\narray([0.00000000e+00, 1.45108339e-08])\n\n\n\n\n5.2.2 Finite Differences using SciPy\nNow let’s compare this to computing the same gradient, but using Numerical Differentiation (specifically, Central Finite Differences):\n\nfrom scipy import optimize\nx_np = np.array([2.0, 5.0])\ndef f_np(x):\n    return np.log(x[0]) + x[0]*x[1] - np.sin(x[1])\n#y_np = f_np(x_np)\n# This computes finite differences opf f_np at x_np:\noptimize.approx_fprime(x_np, f_np, epsilon=1e-4)\n\narray([5.4999875 , 1.71628987])\n\n\nLet’s see how well it approximated the true gradient:\n\ntrue_grad(2,5) - optimize.approx_fprime(x_np, f_np, epsilon=1e-4)\n\narray([1.24995903e-05, 4.79457300e-05])\n\n\n\ndef numerical_error(e):\n    return true_grad(2,5) - optimize.approx_fprime(x_np, f_np, epsilon=e)\n#error = lambda e: true_grad(2,5) - optimize.approx_fprime(x_np, y_np, epsilon=e)\nepsilons = np.logspace(-13,1,num=21)\nerrors = [np.linalg.norm(numerical_error(e)) for e in epsilons]\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_context('poster')\nnp.random.seed(1)\n\nplt.figure()\nplt.loglog(epsilons,errors,marker='o')\nplt.xlabel('$\\epsilon$')\nplt.ylabel('Error')\nplt.title('Finite Difference Approximation')\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives with Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html#pytorch-autograd-example-with-optimization",
    "href": "part1/taking_derivatives.html#pytorch-autograd-example-with-optimization",
    "title": "5  Taking Derivatives with Automatic Differentiation",
    "section": "5.3 PyTorch Autograd Example with Optimization",
    "text": "5.3 PyTorch Autograd Example with Optimization\nThis example shows how to use AD and PyTorch to perform gradient based optimization on a simple test function\n\n# Example of the McCormick Function\ndef mccormick(x):\n    return torch.sin(x[0]+x[1]) + (x[0]-x[1])**2 - 1.5*x[0]+2.5*x[1]+1\nf = mccormick\n\nx = torch.tensor([-4.0, 4.0], requires_grad=True)\ny = f(x)\n\nWhat does this function look like?\n\nX_plot = torch.meshgrid(torch.linspace(-5,4,100),torch.linspace(-5,4,100))\nx_plot,y_plot = X_plot\nplt.figure()\nplt.contour(x_plot,y_plot,f(X_plot))\nplt.show()\n\nc:\\Users\\fuge\\AppData\\Local\\miniforge3\\envs\\ml4me-student\\Lib\\site-packages\\torch\\functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4316.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\n\n\n\n\n\n\n\n\nfig = plt.figure()\nax = plt.axes(projection='3d')\nax.contour3D(x_plot, y_plot, f(X_plot), 50, cmap='binary_r')\nax.view_init(40, 90)\nplt.show()\n\n\n\n\n\n\n\n\nNow let’s say I want to optimize this. I could compute the analytical derivative. Or, I could compute the backward-mode AD on the inputs:\n\n# Pick a starting point:\nx = torch.tensor([-4.0, 4.0], requires_grad=True)\n# Evaluate y\ny = f(x)\n# Now call the backward AD pass so that we can compute gradients\ny.backward()\n# Now we can get the gradient\nx.grad\n\ntensor([-16.5000,  19.5000])\n\n\nNow we just stick it in a loop and run SGD on it:\n\n# Take an initial guess at the optimum:\nx = torch.tensor([-4.0, 4.0], requires_grad=True)\n# Note that the true answer should be x_opt = [5, 5]\n# Initialize the optimizer\noptimizer = torch.optim.AdamW([x], lr=1)\nnum_steps = 50\nsteps = [np.array(x.detach().numpy())]\n# Take 10 steps\nfor i in range(num_steps):\n    optimizer.zero_grad()\n    y = f(x)\n    y.backward()\n    optimizer.step()\n    with torch.no_grad():\n        steps.append(np.array(x.detach().numpy()))\n        print(x)\nsteps = np.array(steps)\n\ntensor([-2.9600,  2.9600], requires_grad=True)\ntensor([-1.9481,  1.9436], requires_grad=True)\ntensor([-0.9857,  0.9653], requires_grad=True)\ntensor([-0.1051,  0.0453], requires_grad=True)\ntensor([ 0.6508, -0.7906], requires_grad=True)\ntensor([ 1.2368, -1.5138], requires_grad=True)\ntensor([ 1.6219, -2.0992], requires_grad=True)\ntensor([ 1.8020, -2.5325], requires_grad=True)\ntensor([ 1.7979, -2.8129], requires_grad=True)\ntensor([ 1.6423, -2.9514], requires_grad=True)\ntensor([ 1.3704, -2.9664], requires_grad=True)\ntensor([ 1.0147, -2.8804], requires_grad=True)\ntensor([ 0.6043, -2.7165], requires_grad=True)\ntensor([ 0.1659, -2.4983], requires_grad=True)\ntensor([-0.2759, -2.2482], requires_grad=True)\ntensor([-0.6981, -1.9879], requires_grad=True)\ntensor([-1.0796, -1.7375], requires_grad=True)\ntensor([-1.4026, -1.5143], requires_grad=True)\ntensor([-1.6534, -1.3326], requires_grad=True)\ntensor([-1.8238, -1.2020], requires_grad=True)\ntensor([-1.9113, -1.1271], requires_grad=True)\ntensor([-1.9188, -1.1078], requires_grad=True)\ntensor([-1.8539, -1.1396], requires_grad=True)\ntensor([-1.7274, -1.2146], requires_grad=True)\ntensor([-1.5524, -1.3224], requires_grad=True)\ntensor([-1.3433, -1.4512], requires_grad=True)\ntensor([-1.1152, -1.5883], requires_grad=True)\ntensor([-0.8830, -1.7215], requires_grad=True)\ntensor([-0.6606, -1.8395], requires_grad=True)\ntensor([-0.4605, -1.9328], requires_grad=True)\ntensor([-0.2926, -1.9947], requires_grad=True)\ntensor([-0.1641, -2.0210], requires_grad=True)\ntensor([-0.0788, -2.0109], requires_grad=True)\ntensor([-0.0372, -1.9664], requires_grad=True)\ntensor([-0.0366, -1.8921], requires_grad=True)\ntensor([-0.0720, -1.7947], requires_grad=True)\ntensor([-0.1359, -1.6823], requires_grad=True)\ntensor([-0.2198, -1.5637], requires_grad=True)\ntensor([-0.3146, -1.4478], requires_grad=True)\ntensor([-0.4113, -1.3430], requires_grad=True)\ntensor([-0.5017, -1.2561], requires_grad=True)\ntensor([-0.5791, -1.1926], requires_grad=True)\ntensor([-0.6384, -1.1558], requires_grad=True)\ntensor([-0.6769, -1.1466], requires_grad=True)\ntensor([-0.6937, -1.1639], requires_grad=True)\ntensor([-0.6901, -1.2045], requires_grad=True)\ntensor([-0.6693, -1.2638], requires_grad=True)\ntensor([-0.6353, -1.3361], requires_grad=True)\ntensor([-0.5934, -1.4148], requires_grad=True)\ntensor([-0.5490, -1.4935], requires_grad=True)\n\n\n\nplt.figure()\nplt.contour(x_plot,y_plot,f(X_plot))\nplt.plot(steps[:,0],steps[:,1],marker='+',label = \"Adam\")\nplt.legend()\nplt.title(\"Adam optimizer\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Take an initial guess at the optimum:\nx = torch.tensor([-4.0, 4.0], requires_grad=True)\n# Note that the true answer should be x_opt = [5, 5]\n# Initialize the optimizer\n# Here using LBFGS, which is much faster convergence on small problems\noptimizer = torch.optim.LBFGS([x],lr=0.05)\nnum_steps = 5\nsteps = [np.array(x.detach().numpy())]\n# Take 10 steps\nfor i in range(num_steps):\n    def closure():\n        optimizer.zero_grad()\n        y = f(x)\n        y.backward()\n        return y\n    optimizer.step(closure)\n    with torch.no_grad():\n            steps.append(np.array(x.detach().numpy()))\n            print(x)\nsteps = np.array(steps)\n\ntensor([-1.9382,  0.4356], requires_grad=True)\ntensor([-1.0402, -0.8307], requires_grad=True)\ntensor([-0.7223, -1.2888], requires_grad=True)\ntensor([-0.6097, -1.4543], requires_grad=True)\ntensor([-0.5696, -1.5139], requires_grad=True)\n\n\n\nplt.figure()\nplt.contour(x_plot,y_plot,f(X_plot))\nplt.scatter([-0.54719],[-1.54719],marker='*',label=\"Optima\")\nplt.plot(steps[:,0],steps[:,1],marker='+',label = \"LFBGS\")\nplt.legend()\nplt.title(\"LFBGS optimizer\")\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives with Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html#demonstration-of-ad-on-verlet-integration",
    "href": "part1/taking_derivatives.html#demonstration-of-ad-on-verlet-integration",
    "title": "5  Taking Derivatives with Automatic Differentiation",
    "section": "5.4 Demonstration of AD on Verlet Integration",
    "text": "5.4 Demonstration of AD on Verlet Integration\nThis notebook demonstrates how to use Automatic Differentiation to determine the gradients of the initial conditions of a dynamical system (in this case a damped oscillator). To do this, we will define a numerical routine (Verlet Integration) and then use Automatic Differentiation to back propagate the gradient information from the output (Total system energy) to the initial conditions.\nBelow is a typical simulation script that is not current set up for automatic differentiation:\n\n\n###############################################################################\nN = 1000\nt = np.linspace(0,10,N)\ndt = t[1] - t[0]\n\n###############################################################################\n# functions\ndef integrate_original(F,x0,v0,gamma):\n    ###########################################################################\n    # arrays are allocated and filled with zeros\n    Ef = 0\n    x = np.zeros(N)\n    v = np.zeros(N)\n    E = np.zeros(N)\n    \n    ###########################################################################    \n    # initial conditions\n    x[0] = x0\n    v[0] = v0\n    \n    ###########################################################################\n    # Do the Verlet Integration\n    fac1 = 1.0 - 0.5*gamma*dt\n    fac2 = 1.0/(1.0 + 0.5*gamma*dt)\n    \n    for i in range(N-1):\n        vn = fac1*fac2*v0 - fac2*dt*x0 + fac2*dt*F[i]\n        xn = x0 + dt*vn\n        Ef = 0.5*(x0**2 + ((v0 + vn)/2.0)**2)\n        v0 = vn\n        x0 = xn\n        # For Plotting/Debug\n\n        v[i + 1] = vn\n        x[i + 1] = xn\n        E[i] = Ef\n\n    Ef = 0.5*(x0**2 + v0**2)\n    \n    E[-1] = Ef\n    \n    ###########################################################################\n    # return solution\n    return ( (x0,v0,Ef) , (x,v,E) )\n\nNow note how we modify the code to include AD via PyTorch (see comments and references to torch):\n\n\n###############################################################################\nN = 1000\nt = np.linspace(0,10,N)\ndt = t[1] - t[0]\n\n###############################################################################\n# functions\ndef integrate(F,x0,v0,gamma):\n    ###########################################################################\n    # arrays are allocated and filled with zeros\n    #x = torch.tensor([0.0],requires_grad=True)\n    #v = torch.zeros(N)\n    Ef = torch.tensor([0.0],requires_grad=True)\n    x = np.zeros(N)\n    v = np.zeros(N)\n    E = np.zeros(N)\n    \n    ###########################################################################    \n    # initial conditions\n    with torch.no_grad():\n        x[0] = x0\n        v[0] = v0\n    \n    ###########################################################################\n    # Do the Verlet Integration\n    fac1 = 1.0 - 0.5*gamma*dt\n    fac2 = 1.0/(1.0 + 0.5*gamma*dt)\n    \n    for i in range(N-1):\n        vn = fac1*fac2*v0 - fac2*dt*x0 + fac2*dt*F[i]\n        xn = x0 + dt*vn\n        Ef = 0.5*(x0**2 + ((v0 + vn)/2.0)**2)\n        v0 = vn\n        x0 = xn\n        # For Plotting/Debug\n        with torch.no_grad():\n            v[i + 1] = vn\n            x[i + 1] = xn\n            E[i] = Ef\n    \n    Ef = 0.5*(x0**2 + v0**2)\n    with torch.no_grad():\n        E[-1] = Ef\n    \n    ###########################################################################\n    # return solution\n    return ( (x0,v0,Ef) , (x,v,E) )\n\n\n###############################################################################\n# Do the actual numerical integration\nF = np.zeros(N)\n\nx_initial = torch.tensor([1.0], requires_grad = True)\nv_initial = torch.tensor([1.0], requires_grad = True)\n#gamma = torch.tensor([0.05], requires_grad = True)\ngamma = torch.tensor([.05], requires_grad = True)\n((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,gamma) # x0 = 0.0, v0 = 1.0, gamma = 0.0\n\n\n#((),(x2,v2,E2)) = integrate(F,0.0,1.0,0.05) # x0 = 0.0, v0 = 1.0, gamma = 0.01\n#((),(x2,v2,E2)) = integrate(F,0.0,1.0,0.05) # x0 = 0.0, v0 = 1.0, gamma = 0.01\n\n#((),(x3,v3,E3)) = integrate(F,0.0,1.0,0.4) # x0 = 0.0, v0 = 1.0, gamma = 0.5\n\n###############################################################################\ndef plot_solution(x1,E1,gamma):\n    plt.rcParams[\"axes.grid\"] = True\n    plt.rcParams['font.size'] = 14\n    plt.rcParams['axes.labelsize'] = 18\n    plt.figure()\n    plt.subplot(211)\n    plt.plot(t,x1)\n    #plt.plot(t,x2)\n    #plt.plot(t,x3)\n    plt.ylabel(\"x(t)\")\n\n    plt.subplot(212)\n    plt.plot(t,E1,label=fr\"$\\gamma = {float(gamma):.2f}$\")\n    #plt.plot(t,E2,label=r\"$\\gamma = 0.01$\")\n    #plt.plot(t,E3,label=r\"$\\gamma = 0.5$\")\n    plt.ylim(0,1.0)\n    plt.ylabel(\"E(t)\")\n\n    plt.xlabel(\"Time\")\n    plt.legend(loc=\"center right\")\n\n    plt.tight_layout()\n\nplot_solution(x1,E1,gamma)\n\n\n\n\n\n\n\n\n\nprint(Ef)\nEf.backward(retain_graph=True)\n\ntensor([0.6137], grad_fn=&lt;MulBackward0&gt;)\n\n\nNow let’s print the gradient of the system Energy with respect to some of the initial conditions:\n\nprint(gamma.grad)\nprint(v_initial.grad)\nprint(x_initial.grad)\n\ntensor([-5.9562])\ntensor([0.6026])\ntensor([0.6248])\n\n\n\nprint(vf)\nvf.backward()\n\ntensor([-0.2245], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nprint(gamma.grad)\nprint(v_initial.grad)\nprint(x_initial.grad)\n\ntensor([-4.7535])\ntensor([-0.0437])\ntensor([1.0466])\n\n\n\n5.4.1 Optimizing the Damping Coefficient via SGD and AD\nFirst let’s just get a visual intuition for how \\(\\gamma\\) affects the final energy:\n\nnum_gammas = 30\ngamma_plot = np.logspace(-0.5,1.0,num_gammas)\nEfs = np.zeros(num_gammas)\nfor i,g in enumerate(gamma_plot):\n    ((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,g)\n    Efs[i] = Ef\n\n\nplt.figure()\nplt.semilogx(gamma_plot,Efs)\nplt.xlabel(r'$\\gamma$')\nplt.ylabel('E Final')\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that there is a pretty flat plateau from around \\(\\gamma=1\\) until around \\(\\gamma=3\\).\nNow let’s use our backward mode AD to actually optimize \\(\\gamma\\) directly by calling backward on the output of the final energy of the Verlet integration of the ODE:\n\n# This part is just a helper library for plotting\ndef plot_optimization(initial_gamma, num_steps, optimizer, opt_kwargs={}):\n    # Take an initial guess at the optimum:\n    gamma = torch.tensor([initial_gamma], requires_grad=True)\n\n    # Initialize the optimizer\n    optimizer = optimizer([gamma], **opt_kwargs)\n\n    steps = [ ] # Here is where we'll keep track of the steps\n    # Take num_steps of the optimizer\n    for i in range(num_steps):\n        # This function runs an actual optimization step. We wrap it in closure so that optimizers\n        # that take multiple function calls per step can do so -- e.g., LBFGS.\n        def closure():\n            # Get rid of the existing gradients on the tape\n            optimizer.zero_grad()\n            # Run the numerical integration -- this is the forward pass through the solver\n            ((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,gamma) # x0 = 0.0, v0 = 1.0, gamma = 0.0\n            # Compute the backward mode AD pass\n            Ef.backward()\n            return Ef\n        # Now ask the optimizer to take a step\n        optimizer.step(closure)\n        \n        # The below part is just for printing/plotting. We call torch.no_grad() here to signify that\n        # we do not need to track this as part of the gradient operations. That is, these parts will not\n        # be added to the computational graph or used for backward mode AD.\n        with torch.no_grad():\n            #print(gamma)\n            # Run again just to plot the solution for this gamma\n            ((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,gamma)\n            #print(Ef)\n            if num_steps&gt;10 and i%3==0:\n                plot_solution(x1,E1,gamma)\n            # Add it to steps so that we can see/plot it later.\n            steps.append(np.array(gamma.detach().numpy()))\n            \n    steps = np.array(steps)\n    return steps\n\n\n5.4.1.1 ADAM Example\n\nsteps_Adam = plot_optimization(initial_gamma=0.05, \n                               num_steps=20,\n                               optimizer=torch.optim.AdamW,\n                               opt_kwargs={'lr':0.5})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.1.2 SGD Example\n\nsteps_SGD = plot_optimization(initial_gamma=0.05, \n                               num_steps=20,\n                               optimizer=torch.optim.SGD,\n                               opt_kwargs={'lr':0.05,'momentum':0.9})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.1.3 LBFGS Example\n(Warning: Per-run solves of LBFGS take a while, so don’t set num_steps too high here)\n\nsteps_LBFGS = plot_optimization(initial_gamma=0.05,\n                                num_steps=5,\n                                optimizer=torch.optim.LBFGS,\n                                opt_kwargs={'lr':0.3})\n\n\n\n\n5.4.2 Compare the steps taken\n\nplt.figure()\nplt.semilogx(gamma_plot,Efs)\nsteps_Adam = steps_Adam.flatten()\nplt.plot(steps_Adam,[0.0]*len(steps_Adam),'|', color = 'r', label = 'Adam Steps')\nplt.plot(steps_SGD,[0.005]*len(steps_SGD),'|', color = 'g', label = 'SGD Steps')\nplt.plot(steps_LBFGS,[0.01]*len(steps_LBFGS),'|', color = 'k', label = 'LBFGS Steps')\nplt.xlabel(r'$\\gamma$')\nplt.ylabel('E Final')\nplt.title(\"Comparison of Optimizers\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives with Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html",
    "href": "part1/distribution_distance.html",
    "title": "6  Measuring Distribution Distances",
    "section": "",
    "text": "6.1 A Toy Example: Comparing Two Gaussians\nThis notebook explores several ways of quantifying distances between one-dimensional probability distributions. We will contrast their mathematical definitions, visualize how they react to parameter changes, and compare the gradients that each metric produces. The goal is to build intuition for why the choice of divergence matters in optimization problems such as generative modeling or system identification. We will explore the following:\nWe will compare a target distribution \\(p(x)\\) against a model distribution \\(q_\\mu(x)\\) whose mean \\(\\mu\\) we can adjust. Unless stated otherwise, the target will be the standard normal distribution. You can change that later to a uniform or a bimodal mixture to reveal different behaviours.\nOur baseline parameterization keeps the model standard deviation fixed at \\(\\sigma_q = 1\\). Adjusting the mean already highlights how asymmetric divergences produce different gradient signals.\nShow Code\n# Baseline visualization: standard normal target vs. shifted model Gaussian\nTARGET = TARGETS[\"gaussian\"]\nMODEL_STD = torch.tensor(1.0, dtype=DEFAULT_DTYPE, device=device)\nMU_BASE = torch.tensor(1.5, dtype=DEFAULT_DTYPE, device=device)\n\nwith torch.no_grad():\n    target_pdf = TARGET.pdf(GRID_X)\n    model_pdf = gaussian_pdf_torch(GRID_X, MU_BASE, MODEL_STD)\n\nplot_distribution_pair(GRID_X, target_pdf, model_pdf, title=\"Baseline Comparison\", target_label=TARGET.name, model_label=f\"Gaussian(μ={MU_BASE.item():.1f}, σ=1.0)\")",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html#kullbackleibler-divergence",
    "href": "part1/distribution_distance.html#kullbackleibler-divergence",
    "title": "6  Measuring Distribution Distances",
    "section": "6.2 Kullback–Leibler Divergence",
    "text": "6.2 Kullback–Leibler Divergence\nForward KL divergence compares how much mass the target distribution assigns relative to the model: \\[\n\\mathrm{KL}(p\\,\\|\\,q) = \\int p(x) \\log \\frac{p(x)}{q(x)}\\,dx.\n\\] Because it integrates expectations under \\(p\\), it heavily penalizes situations where \\(q(x)\\) is small while \\(p(x)\\) is large, even if the model allocates extra mass elsewhere.\n\n6.2.1 Analytical Gradient for Gaussian vs. Gaussian\nFor \\(p = \\mathcal{N}(\\mu_p, \\sigma_p^2)\\) and \\(q = \\mathcal{N}(\\mu, \\sigma_q^2)\\), forward KL admits a closed form: \\[\n\\mathrm{KL}(p\\,\\|\\,q) = \\log \\frac{\\sigma_q}{\\sigma_p} + \\frac{\\sigma_p^2 + (\\mu_p - \\mu)^2}{2\\sigma_q^2} - \\frac{1}{2}.\n\\] Differentiating w.r.t. the model mean gives \\[\n\\frac{\\partial}{\\partial \\mu} \\mathrm{KL}(p\\,\\|\\,q) = \\frac{\\mu - \\mu_p}{\\sigma_q^2},\n\\] which pushes \\(\\mu\\) directly toward \\(\\mu_p\\). We will evaluate the integral numerically to keep the pipeline consistent when we later switch to mixtures.\n\n\nShow Code\n# Forward and reverse KL implementations using numerical integration\n_target_pdf_cache: Dict[str, torch.Tensor] = {}\n\n\ndef get_target_pdf(spec: DistributionSpec) -&gt; torch.Tensor:\n    if spec.name not in _target_pdf_cache:\n        with torch.no_grad():\n            _target_pdf_cache[spec.name] = spec.pdf(GRID_X).detach()\n    return _target_pdf_cache[spec.name]\n\n\ndef forward_kl(mu: torch.Tensor, target: DistributionSpec, model_std: float = 1.0) -&gt; torch.Tensor:\n    p_pdf = get_target_pdf(target)\n    q_pdf = gaussian_pdf_torch(GRID_X, mu, model_std)\n    integrand = p_pdf * torch.log((p_pdf + EPS) / (q_pdf + EPS))\n    return torch.trapz(integrand, GRID_X)\n\n\ndef reverse_kl(mu: torch.Tensor, target: DistributionSpec, model_std: float = 1.0) -&gt; torch.Tensor:\n    p_pdf = get_target_pdf(target)\n    q_pdf = gaussian_pdf_torch(GRID_X, mu, model_std)\n    integrand = q_pdf * torch.log((q_pdf + EPS) / (p_pdf + EPS))\n    return torch.trapz(integrand, GRID_X)\n\n\nNow let’s look at how the forward KL behaves as the model mean \\(\\mu\\) shifts away from the target mean \\(\\mu_p = 0\\). We will plot the KL divergence, and its gradient, as we shift the model mean from -15 to 15. What do you notice about the behavior of the KL Divergence and it’s gradient?\n\n# Forward KL distance curve for standard normal target\n##### Change the following to see what happens #######\nmu_values = np.linspace(-15.0, 15.0, 121)\nmodel_std = 1.0\n####################################\n\nforward_metrics, forward_grads = evaluate_metric_curve(\n    mu_values,\n    lambda mu: forward_kl(mu, TARGETS[\"gaussian\"], model_std=model_std),\n)\nplot_metric_and_gradient(mu_values, forward_metrics, forward_grads, metric_name=\"Forward KL\", target_desc=TARGETS[\"gaussian\"].name)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Forward KL on Gaussian Targets\n\n\n\n\nWhat happens to both the KL Divergence and its gradient when the model mean is close to the target mean? What about when it is far away? Why is this?\nHow would the curve change if we widened the model variance? Try modifying model_std in the code cell above and re-running it.\n\n\n\n\n\nShow Code\n# Interactive widget for forward KL\nif widgets is None:\n    maybe_display(None)\nelse:\n    target_dropdown = widgets.Dropdown(\n        options=[(spec.name, key) for key, spec in TARGETS.items()],\n        value=\"gaussian\",\n        description=\"Target\",\n        style={\"description_width\": \"initial\"},\n    )\n    mu_slider = widgets.FloatSlider(value=0.0, min=-15.0, max=15.0, step=0.5, description=\"μ\")\n    std_slider = widgets.FloatSlider(value=1.0, min=0.4, max=2.5, step=0.1, description=\"σ_q\")\n    output = widgets.Output()\n\n    def _update_forward_kl(*_):\n        with output:\n            output.clear_output(wait=True)\n            target = TARGETS[target_dropdown.value]\n            mu_val = torch.tensor(mu_slider.value, dtype=DEFAULT_DTYPE, device=device, requires_grad=True)\n            std_val = float(std_slider.value)\n            kl_value = forward_kl(mu_val, target, model_std=std_val)\n            grad = torch.autograd.grad(kl_value, mu_val)[0].detach().cpu().item()\n            target_pdf = get_target_pdf(target)\n            model_pdf = gaussian_pdf_torch(GRID_X, mu_slider.value, std_val)\n\n            mu_values = np.linspace(mu_slider.min, mu_slider.max, 181)\n            metrics_curve, grads_curve = evaluate_metric_curve(\n                mu_values,\n                lambda mu: forward_kl(mu, target, model_std=std_val),\n            )\n\n            grid_np = GRID_X.cpu().numpy()\n            target_np = target_pdf.cpu().numpy()\n            model_np = model_pdf.cpu().numpy()\n\n            fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n\n            axes[0].plot(grid_np, target_np, label=target.name, linewidth=2)\n            axes[0].plot(grid_np, model_np, label=f\"Model μ={mu_slider.value:.1f}, σ={std_val:.1f}\", linestyle=\"--\", linewidth=2)\n            axes[0].set_title(\"Density comparison\")\n            axes[0].set_xlabel(\"x\")\n            axes[0].set_ylabel(\"Density\")\n            axes[0].legend()\n\n            axes[1].plot(mu_values, metrics_curve, color=\"tab:blue\", linewidth=2)\n            axes[1].axvline(mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[1].scatter([mu_slider.value], [kl_value.detach().cpu().item()], color=\"tab:blue\")\n            axes[1].set_title(\"Forward KL vs. μ\")\n            axes[1].set_xlabel(\"Model mean μ\")\n            axes[1].set_ylabel(\"Forward KL\")\n\n            axes[2].plot(mu_values, grads_curve, color=\"tab:red\", linewidth=2)\n            axes[2].axhline(0.0, color=\"black\", linestyle=\":\")\n            axes[2].axvline(mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[2].scatter([mu_slider.value], [grad], color=\"tab:red\")\n            axes[2].set_title(\"Gradient d(KL)/dμ\")\n            axes[2].set_xlabel(\"Model mean μ\")\n            axes[2].set_ylabel(\"Gradient\")\n\n            fig.suptitle(\n                f\"Forward KL = {kl_value.detach().cpu().item():.3f} | Gradient = {grad:.3f}\",\n                fontsize=14,\n            )\n            fig.tight_layout()\n            plt.show()\n\n    for control in (target_dropdown, mu_slider, std_slider):\n        control.observe(_update_forward_kl, names=\"value\")\n\n    _update_forward_kl()\n    maybe_display(widgets.VBox([widgets.HTML(\"&lt;h4&gt;Interactive Forward KL Explorer&lt;/h4&gt;\"), target_dropdown, mu_slider, std_slider, output]))\n\n\n\n\n\n\n\n6.2.2 Forward vs. Reverse KL on a Bimodal Target\nReverse KL, \\(\\mathrm{KL}(q\\,\\|\\,p)\\), prefers to avoid regions where the model would place mass but the target does not. On multimodal targets this often leads to mode seeking: the optimizer chooses one mode to cover while ignoring others.\n\n6.2.2.1 Mathematical Overview\nThe reverse KL is defined as \\[\n\\mathrm{KL}(q\\,\\|\\,p) = \\int q(x) \\log \\frac{q(x)}{p(x)}\\,dx,\n\\] so the expectation is taken under the model distribution \\(q\\). If \\(q(x)\\) places mass where \\(p(x)\\) is negligible, the logarithm term explodes and strongly penalizes those regions. Conversely, areas where \\(p\\) has mass but \\(q\\) does not contribute nothing, which is why the optimizer can collapse onto a single mode when matching multimodal targets.\nLet us compare the two on a mixture of Gaussians.\n\n\nShow Code\n# Forward vs. reverse KL on a bimodal target\nmixture_target = TARGETS[\"gaussian_mixture\"]\ncomponent_means = mixture_target.metadata.get(\"means\", (-2.5, 2.5))\ncomponent_stds = mixture_target.metadata.get(\"stds\", (0.6, 0.6))\ncomponent_weights = mixture_target.metadata.get(\"weights\", (0.5, 0.5))\n\nwith torch.no_grad():\n    mixture_pdf = mixture_target.pdf(GRID_X).cpu().numpy()\n    component_curves = []\n    for mean, std, weight in zip(component_means, component_stds, component_weights):\n        component_pdf = weight * gaussian_pdf_torch(GRID_X, mean, std)\n        component_curves.append((mean, weight, component_pdf.cpu().numpy()))\n\ngrid_np = GRID_X.cpu().numpy()\nfig_density, ax_density = plt.subplots(figsize=(10, 4))\nax_density.plot(grid_np, mixture_pdf, label=\"Target mixture\", linewidth=3, color=\"tab:blue\")\nfor mean, weight, comp_pdf in component_curves:\n    ax_density.plot(\n        grid_np,\n        comp_pdf,\n        linestyle=\"--\",\n        linewidth=1.5,\n        label=f\"Component μ={mean:.1f}, w={weight:.2f}\",\n    )\nax_density.set_title(\"Gaussian Mixture Target and Component Contributions\")\nax_density.set_xlabel(\"x\")\nax_density.set_ylabel(\"Density\")\nax_density.legend()\nfig_density.tight_layout()\nplt.show()\n\nmu_values = np.linspace(-4.0, 4.0, 161)\nforward_metrics_mix, forward_grads_mix = evaluate_metric_curve(\n    mu_values,\n    lambda mu: forward_kl(mu, mixture_target, model_std=1.0),\n)\nreverse_metrics_mix, reverse_grads_mix = evaluate_metric_curve(\n    mu_values,\n    lambda mu: reverse_kl(mu, mixture_target, model_std=1.0),\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True)\naxes[0].plot(mu_values, forward_metrics_mix, label=\"Forward KL\", linewidth=2)\naxes[0].plot(mu_values, reverse_metrics_mix, label=\"Reverse KL\", linewidth=2)\naxes[0].set_title(\"KL Divergences vs. μ (Mixture Target)\")\naxes[0].set_xlabel(\"Model mean μ\")\naxes[0].set_ylabel(\"Divergence\")\naxes[0].legend()\n\naxes[1].plot(mu_values, forward_grads_mix, label=\"Forward KL gradient\", linewidth=2)\naxes[1].plot(mu_values, reverse_grads_mix, label=\"Reverse KL gradient\", linewidth=2)\naxes[1].axhline(0.0, color=\"black\", linestyle=\":\")\naxes[1].set_title(\"Gradient signals\")\naxes[1].set_xlabel(\"Model mean μ\")\naxes[1].set_ylabel(\"Gradient\")\naxes[1].legend()\n\nfig.suptitle(\"Mode-Covering vs. Mode-Seeking Behaviour\")\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipReflection: Mode Covering vs. Mode Seeking\n\n\n\n\nWhy does the forward KL gradient stay non-zero even between the two modes?\n\nIdentify the regions where the reverse KL gradient vanishes. How does that explain a model that only tracks one mode?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html#jensenshannon-divergence",
    "href": "part1/distribution_distance.html#jensenshannon-divergence",
    "title": "6  Measuring Distribution Distances",
    "section": "6.3 Jensen–Shannon Divergence",
    "text": "6.3 Jensen–Shannon Divergence\nThe Jensen–Shannon (JS) divergence symmetrizes KL by averaging the two distributions: \\(m(x) = \\tfrac{1}{2}(p(x)+q(x))\\). It stays finite even when supports do not match and is bounded between 0 and \\(\\log 2\\).\n\n6.3.1 Mathematical Formulation\n\\[\n\\mathrm{JS}(p, q) = \\tfrac{1}{2} \\mathrm{KL}(p\\,\\|\\,m) + \\tfrac{1}{2} \\mathrm{KL}(q\\,\\|\\,m), \\quad m = \\tfrac{1}{2}(p + q).\n\\] We will compute JS numerically on the same grid. Autograd gives us gradients through the logarithms as long as we keep everything in PyTorch tensors.\n\ndef jensen_shannon(mu: torch.Tensor, target: DistributionSpec, model_std: float = 1.0) -&gt; torch.Tensor:\n    p_pdf = get_target_pdf(target)\n    q_pdf = gaussian_pdf_torch(GRID_X, mu, model_std)\n    m_pdf = 0.5 * (p_pdf + q_pdf)\n    term_p = p_pdf * torch.log((p_pdf + EPS) / (m_pdf + EPS))\n    term_q = q_pdf * torch.log((q_pdf + EPS) / (m_pdf + EPS))\n    return 0.5 * torch.trapz(term_p, GRID_X) + 0.5 * torch.trapz(term_q, GRID_X)\n\n\n\nShow Code\n# JS divergence vs. μ for Gaussian target compared to forward KL\nmu_values = np.linspace(-15.0, 15.0, 121)\njs_metrics, js_grads = evaluate_metric_curve(\n    mu_values,\n    lambda mu: jensen_shannon(mu, TARGETS[\"gaussian\"], model_std=1.0),\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True)\naxes[0].plot(mu_values, forward_metrics, label=\"Forward KL\", linewidth=2)\naxes[0].plot(mu_values, js_metrics, label=\"JS\", linewidth=2)\naxes[0].set_title(\"Forward KL vs. JS\")\naxes[0].set_xlabel(\"Model mean μ\")\naxes[0].set_ylabel(\"Divergence\")\naxes[0].legend()\n\naxes[1].plot(mu_values, forward_grads, label=\"Forward KL gradient\", linewidth=2)\naxes[1].plot(mu_values, js_grads, label=\"JS gradient\", linewidth=2)\naxes[1].axhline(0.0, color=\"black\", linestyle=\":\")\naxes[1].set_title(\"Gradient Comparison\")\naxes[1].set_xlabel(\"Model mean μ\")\naxes[1].set_ylabel(\"Gradient\")\naxes[1].legend()\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipReflection: Bounded Divergence, Softer Gradients\n\n\n\n\nJS approaches \\(\\log 2\\) when the two distributions barely overlap. Where in the plot does that happen?\n\nCompare the slope near \\(\\mu=0\\) for JS and forward KL. What do you notice?\nWhat happens when the distributions move far away from each other?\n\n\n\n\n\nShow Code\n# Interactive JS explorer\nif widgets is None:\n    maybe_display(None)\nelse:\n    js_target_dropdown = widgets.Dropdown(\n        options=[(spec.name, key) for key, spec in TARGETS.items()],\n        value=\"gaussian\",\n        description=\"Target\",\n        style={\"description_width\": \"initial\"},\n    )\n    js_mu_slider = widgets.FloatSlider(value=0.0, min=-4.0, max=4.0, step=0.1, description=\"μ\")\n    js_std_slider = widgets.FloatSlider(value=1.0, min=0.5, max=2.5, step=0.1, description=\"σ_q\")\n    js_output = widgets.Output()\n\n    def _update_js(*_):\n        with js_output:\n            js_output.clear_output(wait=True)\n            target = TARGETS[js_target_dropdown.value]\n            mu_val = torch.tensor(js_mu_slider.value, dtype=DEFAULT_DTYPE, device=device, requires_grad=True)\n            std_val = float(js_std_slider.value)\n            js_val = jensen_shannon(mu_val, target, model_std=std_val)\n            grad = torch.autograd.grad(js_val, mu_val)[0].detach().cpu().item()\n            target_pdf = get_target_pdf(target)\n            model_pdf = gaussian_pdf_torch(GRID_X, js_mu_slider.value, std_val)\n\n            mu_values = np.linspace(js_mu_slider.min, js_mu_slider.max, 181)\n            metrics_curve, grads_curve = evaluate_metric_curve(\n                mu_values,\n                lambda mu: jensen_shannon(mu, target, model_std=std_val),\n            )\n\n            grid_np = GRID_X.cpu().numpy()\n            target_np = target_pdf.cpu().numpy()\n            model_np = model_pdf.cpu().numpy()\n\n            fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n\n            axes[0].plot(grid_np, target_np, label=target.name, linewidth=2)\n            axes[0].plot(grid_np, model_np, label=f\"Model μ={js_mu_slider.value:.1f}, σ={std_val:.1f}\", linestyle=\"--\", linewidth=2)\n            axes[0].set_title(\"Density comparison\")\n            axes[0].set_xlabel(\"x\")\n            axes[0].set_ylabel(\"Density\")\n            axes[0].legend()\n\n            axes[1].plot(mu_values, metrics_curve, color=\"tab:blue\", linewidth=2)\n            axes[1].axvline(js_mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[1].scatter([js_mu_slider.value], [js_val.detach().cpu().item()], color=\"tab:blue\")\n            axes[1].set_title(\"JS divergence vs. μ\")\n            axes[1].set_xlabel(\"Model mean μ\")\n            axes[1].set_ylabel(\"JS divergence\")\n\n            axes[2].plot(mu_values, grads_curve, color=\"tab:red\", linewidth=2)\n            axes[2].axhline(0.0, color=\"black\", linestyle=\":\")\n            axes[2].axvline(js_mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[2].scatter([js_mu_slider.value], [grad], color=\"tab:red\")\n            axes[2].set_title(\"Gradient d(JS)/dμ\")\n            axes[2].set_xlabel(\"Model mean μ\")\n            axes[2].set_ylabel(\"Gradient\")\n\n            fig.suptitle(\n                f\"JS = {js_val.detach().cpu().item():.3f} | Gradient = {grad:.3f}\",\n                fontsize=14,\n            )\n            fig.tight_layout()\n            plt.show()\n\n    for control in (js_target_dropdown, js_mu_slider, js_std_slider):\n        control.observe(_update_js, names=\"value\")\n\n    _update_js()\n    maybe_display(widgets.VBox([widgets.HTML(\"&lt;h4&gt;Interactive JS Explorer&lt;/h4&gt;\"), js_target_dropdown, js_mu_slider, js_std_slider, js_output]))",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html#maximum-mean-discrepancy-mmd",
    "href": "part1/distribution_distance.html#maximum-mean-discrepancy-mmd",
    "title": "6  Measuring Distribution Distances",
    "section": "6.4 Maximum Mean Discrepancy (MMD)",
    "text": "6.4 Maximum Mean Discrepancy (MMD)\nMMD measures the distance between mean embeddings of distributions in a reproducing kernel Hilbert space (RKHS). With a kernel \\(k\\), \\[\n\\mathrm{MMD}^2(p, q) = \\mathbb{E}_{x, x' \\sim p}[k(x, x')] - 2\\, \\mathbb{E}_{x \\sim p, y \\sim q}[k(x, y)] + \\mathbb{E}_{y, y' \\sim q}[k(y, y')].\n\\] We will use the radial basis function (RBF) kernel \\[\nk_\\sigma(x, y) = \\exp\\!\\left(-\\tfrac{(x - y)^2}{2\\sigma_k^2}\\right),\n\\] which assigns high similarity when two samples are closer than the bandwidth \\(\\sigma_k\\) and decays smoothly otherwise. A small \\(\\sigma_k\\) emphasises very local discrepancies—only nearby points contribute—so MMD becomes sensitive to fine-grained differences but can miss global shifts. A large \\(\\sigma_k\\) blurs the notion of neighbourhood, making the kernel respond to broader trends while down-weighting local wiggles. Balancing \\(\\sigma_k\\) therefore tunes whether we care more about microstructure or coarse alignment between \\(p\\) and \\(q\\).\n\n# Shared noise tensors for deterministic MMD estimates\nBASE_NOISE_NORMAL = torch.randn(4096, dtype=DEFAULT_DTYPE, device=device)\nBASE_NOISE_UNIFORM = torch.linspace(0.0, 1.0, 4096, dtype=DEFAULT_DTYPE, device=device)\n\n\ndef sample_with_matching_noise(spec: DistributionSpec, sample_count: int) -&gt; torch.Tensor:\n    if \"Uniform\" in spec.name:\n        base = BASE_NOISE_UNIFORM[:sample_count]\n    else:\n        base = BASE_NOISE_NORMAL[:sample_count]\n    return spec.sampler(sample_count, base)\n\n\ndef rbf_kernel(x: torch.Tensor, y: torch.Tensor, bandwidth: float) -&gt; torch.Tensor:\n    bw = torch.as_tensor(bandwidth, dtype=DEFAULT_DTYPE, device=device)\n    diff = x[:, None] - y[None, :]\n    return torch.exp(-0.5 * diff.pow(2) / (bw ** 2 + EPS))\n\n\ndef maximum_mean_discrepancy(\n    mu: torch.Tensor,\n    target: DistributionSpec,\n    model_std: float = 1.0,\n    bandwidth: float = 1.0,\n    sample_count: int = 512,\n) -&gt; torch.Tensor:\n    samples_p = sample_with_matching_noise(target, sample_count)\n    base_gauss = BASE_NOISE_NORMAL[:sample_count]\n    samples_q = mu + model_std * base_gauss\n    xx = rbf_kernel(samples_p, samples_p, bandwidth)\n    yy = rbf_kernel(samples_q, samples_q, bandwidth)\n    xy = rbf_kernel(samples_p, samples_q, bandwidth)\n    xx = xx - torch.diag(torch.diag(xx))\n    yy = yy - torch.diag(torch.diag(yy))\n    n = sample_count\n    mmd2 = xx.sum() / (n * (n - 1) + EPS) - 2.0 * xy.mean() + yy.sum() / (n * (n - 1) + EPS)\n    return torch.sqrt(torch.clamp(mmd2, min=0.0))\n\n\n\nShow Code\n# MMD vs. μ for different kernel bandwidths\nmu_values = np.linspace(-15.0, 15.0, 121)\nbandwidths = [0.3, 1.0, 2.5]\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True)\nfor bw in bandwidths:\n    metrics, grads = evaluate_metric_curve(\n        mu_values,\n        lambda mu, bw=bw: maximum_mean_discrepancy(mu, TARGETS[\"gaussian\"], model_std=1.0, bandwidth=bw),\n    )\n    axes[0].plot(mu_values, metrics, label=f\"σ_k={bw}\")\n    axes[1].plot(mu_values, grads, label=f\"σ_k={bw}\")\n\naxes[0].set_title(\"MMD vs. μ (Gaussian target)\")\naxes[0].set_xlabel(\"Model mean μ\")\naxes[0].set_ylabel(\"MMD\")\naxes[0].legend()\n\naxes[1].set_title(\"MMD gradient vs. μ\")\naxes[1].set_xlabel(\"Model mean μ\")\naxes[1].set_ylabel(\"Gradient\")\naxes[1].legend()\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Effect of MMD Kernel Bandwidth\n\n\n\n\nWhich bandwidth reacts most strongly to small shifts in \\(\\mu\\)?\n\nObserve how the gradient flattens for large \\(\\sigma_k\\). When might that be desirable?\n\nHow would you select \\(\\sigma_k\\) automatically in a learning system?\n\n\n\n\n\nShow Code\n# Interactive MMD explorer\nif widgets is None:\n    maybe_display(None)\nelse:\n    mmd_target_dropdown = widgets.Dropdown(\n        options=[(spec.name, key) for key, spec in TARGETS.items()],\n        value=\"gaussian\",\n        description=\"Target\",\n        style={\"description_width\": \"initial\"},\n    )\n    mmd_mu_slider = widgets.FloatSlider(value=0.0, min=-15.0, max=15.0, step=0.5, description=\"μ\")\n    mmd_std_slider = widgets.FloatSlider(value=1.0, min=0.5, max=2.5, step=0.1, description=\"σ_q\")\n    mmd_bw_slider = widgets.FloatLogSlider(value=1.0, base=10, min=-1, max=1, step=0.05, description=\"σ_k\")\n    mmd_output = widgets.Output()\n\n    def _update_mmd(*_):\n        with mmd_output:\n            mmd_output.clear_output(wait=True)\n            target = TARGETS[mmd_target_dropdown.value]\n            mu_val = torch.tensor(mmd_mu_slider.value, dtype=DEFAULT_DTYPE, device=device, requires_grad=True)\n            std_val = float(mmd_std_slider.value)\n            bw_val = float(mmd_bw_slider.value)\n            mmd_val = maximum_mean_discrepancy(mu_val, target, model_std=std_val, bandwidth=bw_val)\n            grad = torch.autograd.grad(mmd_val, mu_val)[0].detach().cpu().item()\n\n            mu_values = np.linspace(mmd_mu_slider.min, mmd_mu_slider.max, 181)\n            metrics_curve, grads_curve = evaluate_metric_curve(\n                mu_values,\n                lambda mu: maximum_mean_discrepancy(mu, target, model_std=std_val, bandwidth=bw_val),\n            )\n\n            target_pdf = get_target_pdf(target)\n            model_pdf = gaussian_pdf_torch(GRID_X, mmd_mu_slider.value, std_val)\n\n            grid_np = GRID_X.cpu().numpy()\n            target_np = target_pdf.cpu().numpy()\n            model_np = model_pdf.cpu().numpy()\n\n            fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n\n            axes[0].plot(grid_np, target_np, label=target.name, linewidth=2)\n            axes[0].plot(grid_np, model_np, label=f\"Model μ={mmd_mu_slider.value:.1f}, σ={std_val:.1f}\", linestyle=\"--\", linewidth=2)\n            axes[0].set_title(\"Density comparison\")\n            axes[0].set_xlabel(\"x\")\n            axes[0].set_ylabel(\"Density\")\n            axes[0].legend()\n\n            axes[1].plot(mu_values, metrics_curve, color=\"tab:blue\", linewidth=2)\n            axes[1].axvline(mmd_mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[1].scatter([mmd_mu_slider.value], [mmd_val.detach().cpu().item()], color=\"tab:blue\")\n            axes[1].set_title(\"MMD vs. μ\")\n            axes[1].set_xlabel(\"Model mean μ\")\n            axes[1].set_ylabel(\"MMD\")\n\n            axes[2].plot(mu_values, grads_curve, color=\"tab:red\", linewidth=2)\n            axes[2].axhline(0.0, color=\"black\", linestyle=\":\")\n            axes[2].axvline(mmd_mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[2].scatter([mmd_mu_slider.value], [grad], color=\"tab:red\")\n            axes[2].set_title(\"Gradient d(MMD)/dμ\")\n            axes[2].set_xlabel(\"Model mean μ\")\n            axes[2].set_ylabel(\"Gradient\")\n\n            fig.suptitle(\n                f\"MMD = {mmd_val.detach().cpu().item():.3f} | Gradient = {grad:.3f}\",\n                fontsize=14,\n            )\n            fig.tight_layout()\n            plt.show()\n\n    for control in (mmd_target_dropdown, mmd_mu_slider, mmd_std_slider, mmd_bw_slider):\n        control.observe(_update_mmd, names=\"value\")\n\n    _update_mmd()\n    maybe_display(\n        widgets.VBox(\n            [\n                widgets.HTML(\"&lt;h4&gt;Interactive MMD Explorer&lt;/h4&gt;\"),\n                mmd_target_dropdown,\n                mmd_mu_slider,\n                mmd_std_slider,\n                mmd_bw_slider,\n                mmd_output,\n            ]\n        )\n    )",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html#optimal-transport-wasserstein-2-distance",
    "href": "part1/distribution_distance.html#optimal-transport-wasserstein-2-distance",
    "title": "6  Measuring Distribution Distances",
    "section": "6.5 Optimal Transport (Wasserstein-2 Distance)",
    "text": "6.5 Optimal Transport (Wasserstein-2 Distance)\nOptimal Transport (OT) frames distribution comparison as the problem of moving probability mass from a source distribution \\(q\\) to a target distribution \\(p\\) at minimal cost. Given a ground cost \\(c(x, y)\\) measuring how expensive it is to transport mass from \\(x\\) to \\(y\\), the quadratic OT problem seeks a coupling \\(\\pi(x, y)\\) with marginals \\(q\\) and \\(p\\) that minimises the total transport cost: \\[\nW_2^2(p, q) = \\inf_{\\pi \\in \\Pi(p, q)} \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} \\|x - y\\|^2 \\, d\\pi(x, y),\n\\] where \\(\\Pi(p, q)\\) denotes the set of joint distributions whose marginals are \\(p\\) and \\(q\\). This formulation differs from divergence-based distances above (KL, JS, MMD) because it explicitly models where probability mass must move, rather than comparing densities pointwise or via kernel similarities. As a result, OT remains informative even when supports do not overlap: the metric still reflects how far mass must travel to align the distributions.\nIn one dimension, the optimal coupling sorts both distributions and pairs quantiles. The squared Wasserstein-2 distance simplifies to \\[\nW_2^2(p, q) = \\int_0^1 \\left|F_p^{-1}(u) - F_q^{-1}(u)\\right|^2 \\, du,\n\\] with \\(F^{-1}\\) denoting the quantile function. For Gaussians, this has a closed form \\(W_2^2 = (\\mu_p - \\mu)^2 + (\\sigma_p - \\sigma_q)^2\\). We will compute it via quantile functions to keep the numerical pipeline consistent for non-Gaussian targets.\n\n\nShow Code\ndef gaussian_quantile(u: torch.Tensor, mean: float, std: float) -&gt; torch.Tensor:\n    mean_t = torch.as_tensor(mean, dtype=DEFAULT_DTYPE, device=device)\n    std_t = torch.as_tensor(std, dtype=DEFAULT_DTYPE, device=device)\n    return mean_t + std_t * math.sqrt(2.0) * torch.erfinv(2.0 * u - 1.0)\n\n\ndef wasserstein_2(\n    mu: torch.Tensor,\n    target: DistributionSpec,\n    model_std: float = 1.0,\n    sample_count: int = 1024,\n) -&gt; torch.Tensor:\n    if target.name.startswith(\"Gaussian\"):  # use closed form\n        mu_p = target.metadata.get(\"mean\", 0.0)\n        sigma_p = target.metadata.get(\"std\", 1.0)\n        sigma_q = model_std\n        diff_sq = (mu - mu_p) ** 2 + (sigma_q - sigma_p) ** 2\n        return torch.sqrt(torch.clamp(diff_sq, min=0.0))\n\n    # Empirical quantile-based estimate for non-Gaussian targets\n    u = torch.linspace(0.0 + 0.5 / sample_count, 1.0 - 0.5 / sample_count, sample_count, dtype=DEFAULT_DTYPE, device=device)\n    samples_p = torch.sort(sample_with_matching_noise(target, sample_count))[0]\n    samples_q = torch.sort(mu + model_std * BASE_NOISE_NORMAL[:sample_count])[0]\n    return torch.sqrt(torch.mean((samples_p - samples_q) ** 2))\n\n\n\n\nShow Code\n# Wasserstein-2 distance vs. μ for Gaussian target\nmu_values = np.linspace(-15.0, 15.0, 121)\nw2_metrics, w2_grads = evaluate_metric_curve(\n    mu_values,\n    lambda mu: wasserstein_2(mu, TARGETS[\"gaussian\"], model_std=1.0),\n)\nplot_metric_and_gradient(mu_values, w2_metrics, w2_grads, metric_name=\"W2\", target_desc=TARGETS[\"gaussian\"].name)\n\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Visualizing the optimal transport map in 1D\nmu_demo = 2.5\nsample_count = 64\nu = torch.linspace(0.0 + 0.5 / sample_count, 1.0 - 0.5 / sample_count, sample_count, dtype=DEFAULT_DTYPE, device=device)\nsource_samples = torch.sort(mu_demo + BASE_NOISE_NORMAL[:sample_count])[0].cpu().numpy()\ntarget_samples = torch.sort(sample_with_matching_noise(TARGETS[\"gaussian\"], sample_count))[0].cpu().numpy()\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.scatter(source_samples, np.zeros_like(source_samples), label=\"Model\", color=\"tab:orange\", s=40)\nax.scatter(target_samples, np.ones_like(target_samples), label=\"Target\", color=\"tab:blue\", s=40)\nfor xs, xt in zip(source_samples, target_samples):\n    ax.plot([xs, xt], [0.0, 1.0], color=\"gray\", alpha=0.4)\nax.set_title(\"Optimal Transport Matching for μ = 2.5\")\nax.set_yticks([0, 1])\nax.set_yticklabels([\"Model\", \"Target\"])\nax.set_xlabel(\"x\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipReflection: Transport as Matching\n\n\n\n\nHow does the transport map change if you halve the model variance?\n\nCompare the length of the transport lines with the Wasserstein distance you computed earlier.\n\nWhat would happen if the target distribution had gaps (e.g., a mixture)? Sketch the expected matching pattern.\n\n\n\n\n\nShow Code\n# Interactive Wasserstein explorer\nif widgets is None:\n    maybe_display(None)\nelse:\n    w2_target_dropdown = widgets.Dropdown(\n        options=[(spec.name, key) for key, spec in TARGETS.items()],\n        value=\"gaussian\",\n        description=\"Target\",\n        style={\"description_width\": \"initial\"},\n    )\n    w2_mu_slider = widgets.FloatSlider(value=0.0, min=-15.0, max=15.0, step=0.1, description=\"μ\")\n    w2_std_slider = widgets.FloatSlider(value=1.0, min=0.5, max=2.5, step=0.1, description=\"σ_q\")\n    w2_output = widgets.Output()\n\n    def _update_w2(*_):\n        with w2_output:\n            w2_output.clear_output(wait=True)\n            target = TARGETS[w2_target_dropdown.value]\n            mu_val = torch.tensor(w2_mu_slider.value, dtype=DEFAULT_DTYPE, device=device, requires_grad=True)\n            std_val = float(w2_std_slider.value)\n            w2_val = wasserstein_2(mu_val, target, model_std=std_val)\n            grad = torch.autograd.grad(w2_val, mu_val)[0].detach().cpu().item()\n\n            mu_values = np.linspace(w2_mu_slider.min, w2_mu_slider.max, 181)\n            metrics_curve, grads_curve = evaluate_metric_curve(\n                mu_values,\n                lambda mu: wasserstein_2(mu, target, model_std=std_val),\n            )\n\n            target_pdf = get_target_pdf(target)\n            model_pdf = gaussian_pdf_torch(GRID_X, w2_mu_slider.value, std_val)\n\n            grid_np = GRID_X.cpu().numpy()\n            target_np = target_pdf.cpu().numpy()\n            model_np = model_pdf.cpu().numpy()\n\n            fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n\n            axes[0].plot(grid_np, target_np, label=target.name, linewidth=2)\n            axes[0].plot(grid_np, model_np, label=f\"Model μ={w2_mu_slider.value:.1f}, σ={std_val:.1f}\", linestyle=\"--\", linewidth=2)\n            axes[0].set_title(\"Density comparison\")\n            axes[0].set_xlabel(\"x\")\n            axes[0].set_ylabel(\"Density\")\n            axes[0].legend()\n\n            axes[1].plot(mu_values, metrics_curve, color=\"tab:blue\", linewidth=2)\n            axes[1].axvline(w2_mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[1].scatter([w2_mu_slider.value], [w2_val.detach().cpu().item()], color=\"tab:blue\")\n            axes[1].set_title(\"Wasserstein-2 vs. μ\")\n            axes[1].set_xlabel(\"Model mean μ\")\n            axes[1].set_ylabel(\"W2 distance\")\n\n            axes[2].plot(mu_values, grads_curve, color=\"tab:red\", linewidth=2)\n            axes[2].axhline(0.0, color=\"black\", linestyle=\":\")\n            axes[2].axvline(w2_mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[2].scatter([w2_mu_slider.value], [grad], color=\"tab:red\")\n            axes[2].set_title(\"Gradient d(W2)/dμ\")\n            axes[2].set_xlabel(\"Model mean μ\")\n            axes[2].set_ylabel(\"Gradient\")\n\n            fig.suptitle(\n                f\"W2 = {w2_val.detach().cpu().item():.3f} | Gradient = {grad:.3f}\",\n                fontsize=14,\n            )\n            fig.tight_layout()\n            plt.show()\n\n            if not target.name.startswith(\"Gaussian\"):\n                transport_samples = 80\n                src = torch.sort(mu_val.detach() + std_val * BASE_NOISE_NORMAL[:transport_samples])[0].cpu().numpy()\n                tgt = torch.sort(sample_with_matching_noise(target, transport_samples))[0].cpu().numpy()\n                fig_map, ax_map = plt.subplots(figsize=(8, 4))\n                ax_map.scatter(src, np.zeros_like(src), label=\"Model\", color=\"tab:orange\", s=25)\n                ax_map.scatter(tgt, np.ones_like(tgt), label=\"Target\", color=\"tab:blue\", s=25)\n                for xs, xt in zip(src, tgt):\n                    ax_map.plot([xs, xt], [0.0, 1.0], color=\"gray\", alpha=0.3)\n                ax_map.set_yticks([0, 1])\n                ax_map.set_yticklabels([\"Model\", \"Target\"])\n                ax_map.set_title(\"Transport plan snapshot\")\n                ax_map.legend(loc=\"upper left\")\n                fig_map.tight_layout()\n                plt.show()\n\n    for control in (w2_target_dropdown, w2_mu_slider, w2_std_slider):\n        control.observe(_update_w2, names=\"value\")\n\n    _update_w2()\n    maybe_display(widgets.VBox([widgets.HTML(\"&lt;h4&gt;Interactive Wasserstein Explorer&lt;/h4&gt;\"), w2_target_dropdown, w2_mu_slider, w2_std_slider, w2_output]))",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html#information-geometry-vs.-optimal-transport",
    "href": "part1/distribution_distance.html#information-geometry-vs.-optimal-transport",
    "title": "6  Measuring Distribution Distances",
    "section": "6.6 Information Geometry vs. Optimal Transport",
    "text": "6.6 Information Geometry vs. Optimal Transport\nInformation Geometry treats families of probability distributions as curved manifolds equipped with the Fisher information metric. Intuitively, it asks: how sensitive are likelihoods to infinitesimal parameter changes? The geometry that emerges is tailored to statistical inference—geodesics correspond to paths that keep models maximally informative, and inner products reflect the Cramér–Rao notion of efficiency. This is why information-geometric tools appear in natural gradient descent, variational inference, and sensor placement problems where we care about how much information parameters carry about data.\nBecause the Fisher–Rao metric lives on the parameter manifold, it is invariant to reparameterisations and emphasises directions where the distribution changes statistically rather than spatially. In contrast, Optimal Transport metrics such as \\(W_2\\) operate directly on the sample space and quantify how far probability mass must move. OT is therefore ideal when spatial structure matters—think of matching shapes, aligning time-series histograms, or enforcing smooth transport maps in PDE-constrained problems.\nWhen would you pick one over the other? - Use Information Geometry when optimizing probabilistic models with latent variables or exponential-family structure, where natural gradients provide preconditioning aligned with likelihood curvature. - Use OT when discrepancies in physical space are key, such as calibrating simulators to sensor data, training generative models that must respect spatial coherence, or comparing distributions with disjoint supports. It is also useful when the target distributions do not have an analytical form but samples are available.\nLet us compare the two on Gaussian families and visualize the geodesic paths they induce.\n\n\nShow Code\ntry:\n    from geomstats.geometry.normal_distribution import NormalDistributions\n    from geomstats.learning.frechet_mean import FrechetMean\n    GEOMSTATS_AVAILABLE = True\nexcept Exception:\n    GEOMSTATS_AVAILABLE = False\n\n\ndef fisher_rao_distance(mu1: float, sigma1: float, mu2: float, sigma2: float) -&gt; float:\n    term = (sigma1 ** 2 + sigma2 ** 2 + (mu1 - mu2) ** 2) / (2.0 * sigma1 * sigma2)\n    term = max(term, 1.0)\n    return math.sqrt(2.0) * math.acosh(term)\n\n\ndef ot_geodesic(mu1: float, sigma1: float, mu2: float, sigma2: float, num_points: int = 50) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    ts = np.linspace(0.0, 1.0, num_points)\n    mus = (1 - ts) * mu1 + ts * mu2\n    sigmas = (1 - ts) * sigma1 + ts * sigma2\n    return ts, mus, sigmas\n\n\ndef fisher_rao_geodesic(mu1: float, sigma1: float, mu2: float, sigma2: float, num_points: int = 50) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    ts = np.linspace(0.0, 1.0, num_points)\n    if GEOMSTATS_AVAILABLE:\n        manifold = NormalDistributions()\n        path = manifold.geodesic(initial_point=np.array([mu1, sigma1 ** 2]), end_point=np.array([mu2, sigma2 ** 2]))\n        geodesic_points = np.array([path(t) for t in ts])\n        mus = geodesic_points[:, 0]\n        sigmas = np.sqrt(np.abs(geodesic_points[:, 1]))\n        return ts, mus, sigmas\n    # Fallback: interpolate in natural parameters (μ/σ, log σ)\n    theta1 = np.array([mu1 / sigma1, math.log(sigma1)])\n    theta2 = np.array([mu2 / sigma2, math.log(sigma2)])\n    thetas = np.outer(1 - ts, theta1) + np.outer(ts, theta2)\n    sigmas = np.exp(thetas[:, 1])\n    mus = thetas[:, 0] * sigmas\n    return ts, mus, sigmas\n\n\n\n\nShow Code\n# Compare Wasserstein-2 and Fisher-Rao distances/geodesics\nif widgets is None:\n    mu1, sigma1 = 0.0, 1.0\n    mu2, sigma2 = 2.5, 0.5\n\n    w2_val = wasserstein_2(\n        torch.tensor(mu2, dtype=DEFAULT_DTYPE, device=device),\n        make_gaussian(mean=mu1, std=sigma1),\n        model_std=sigma2,\n    ).item()\n    fr_val = fisher_rao_distance(mu1, sigma1, mu2, sigma2)\n    print(f\"W2(p, q) = {w2_val:.3f}\\nFisher–Rao(p, q) = {fr_val:.3f}\")\n\n    ts_ot, mus_ot, sigmas_ot = ot_geodesic(mu1, sigma1, mu2, sigma2, num_points=200)\n    ts_fr, mus_fr, sigmas_fr = fisher_rao_geodesic(mu1, sigma1, mu2, sigma2, num_points=200)\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    axes[0].plot(ts_ot, mus_ot, label=\"OT geodesic\", linewidth=2)\n    axes[0].plot(ts_fr, mus_fr, label=\"Fisher–Rao geodesic\", linewidth=2)\n    axes[0].set_xlabel(\"t\")\n    axes[0].set_ylabel(\"Mean μ(t)\")\n    axes[0].set_title(\"Mean evolution\")\n    axes[0].legend()\n\n    axes[1].plot(ts_ot, sigmas_ot, label=\"OT geodesic\", linewidth=2)\n    axes[1].plot(ts_fr, sigmas_fr, label=\"Fisher–Rao geodesic\", linewidth=2)\n    axes[1].set_xlabel(\"t\")\n    axes[1].set_ylabel(\"Std σ(t)\")\n    axes[1].set_title(\"Scale evolution\")\n    axes[1].legend()\n\n    fig.tight_layout()\n    plt.show()\n\n    sample_ts = np.linspace(0.0, 1.0, 6)\n    colors = plt.cm.viridis(sample_ts)\n    grid_np = GRID_X.cpu().numpy()\n\n    fig_interp, interp_axes = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n    for t_val, color in zip(sample_ts, colors):\n        mu_t_ot = np.interp(t_val, ts_ot, mus_ot)\n        sigma_t_ot = np.interp(t_val, ts_ot, sigmas_ot)\n        pdf_ot = gaussian_pdf_torch(GRID_X, mu_t_ot, sigma_t_ot).cpu().numpy()\n        interp_axes[0].plot(grid_np, pdf_ot, color=color, linewidth=2, label=f\"t={t_val:.2f}\")\n\n    interp_axes[0].set_title(\"OT interpolation (W2 geodesic)\")\n    interp_axes[0].set_xlabel(\"x\")\n    interp_axes[0].set_ylabel(\"Density\")\n    interp_axes[0].legend(loc=\"upper right\", ncol=2)\n\n    for t_val, color in zip(sample_ts, colors):\n        mu_t_fr = np.interp(t_val, ts_fr, mus_fr)\n        sigma_t_fr = np.interp(t_val, ts_fr, sigmas_fr)\n        pdf_fr = gaussian_pdf_torch(GRID_X, mu_t_fr, sigma_t_fr).cpu().numpy()\n        interp_axes[1].plot(grid_np, pdf_fr, color=color, linewidth=2, label=f\"t={t_val:.2f}\")\n\n    interp_axes[1].set_title(\"Information-geometry interpolation (Fisher–Rao)\")\n    interp_axes[1].set_xlabel(\"x\")\n    interp_axes[1].legend(loc=\"upper right\", ncol=2)\n\n    fig_interp.tight_layout()\n    plt.show()\nelse:\n    ig_mu1_slider = widgets.FloatSlider(value=0.0, min=-4.0, max=4.0, step=0.1, description=\"μ₁\")\n    ig_sigma1_slider = widgets.FloatSlider(value=1.0, min=0.2, max=3.0, step=0.05, description=\"σ₁\")\n    ig_mu2_slider = widgets.FloatSlider(value=2.5, min=-4.0, max=4.0, step=0.1, description=\"μ₂\")\n    ig_sigma2_slider = widgets.FloatSlider(value=0.5, min=0.2, max=3.0, step=0.05, description=\"σ₂\")\n    ig_output = widgets.Output()\n\n    def _update_geodesic(*_):\n        with ig_output:\n            ig_output.clear_output(wait=True)\n            mu1 = float(ig_mu1_slider.value)\n            sigma1 = float(ig_sigma1_slider.value)\n            mu2 = float(ig_mu2_slider.value)\n            sigma2 = float(ig_sigma2_slider.value)\n\n            w2_val = wasserstein_2(\n                torch.tensor(mu2, dtype=DEFAULT_DTYPE, device=device),\n                make_gaussian(mean=mu1, std=sigma1),\n                model_std=sigma2,\n            ).item()\n            fr_val = fisher_rao_distance(mu1, sigma1, mu2, sigma2)\n            print(f\"W2(p, q) = {w2_val:.3f}\\nFisher–Rao(p, q) = {fr_val:.3f}\")\n\n            ts_ot, mus_ot, sigmas_ot = ot_geodesic(mu1, sigma1, mu2, sigma2, num_points=200)\n            ts_fr, mus_fr, sigmas_fr = fisher_rao_geodesic(mu1, sigma1, mu2, sigma2, num_points=200)\n\n            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n            axes[0].plot(ts_ot, mus_ot, label=\"OT geodesic\", linewidth=2)\n            axes[0].plot(ts_fr, mus_fr, label=\"Fisher–Rao geodesic\", linewidth=2)\n            axes[0].set_xlabel(\"t\")\n            axes[0].set_ylabel(\"Mean μ(t)\")\n            axes[0].set_title(\"Mean evolution\")\n            axes[0].legend()\n\n            axes[1].plot(ts_ot, sigmas_ot, label=\"OT geodesic\", linewidth=2)\n            axes[1].plot(ts_fr, sigmas_fr, label=\"Fisher–Rao geodesic\", linewidth=2)\n            axes[1].set_xlabel(\"t\")\n            axes[1].set_ylabel(\"Std σ(t)\")\n            axes[1].set_title(\"Scale evolution\")\n            axes[1].legend()\n\n            fig.tight_layout()\n            plt.show()\n\n            sample_ts = np.linspace(0.0, 1.0, 6)\n            colors = plt.cm.viridis(sample_ts)\n            grid_np = GRID_X.cpu().numpy()\n\n            fig_interp, interp_axes = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n            for t_val, color in zip(sample_ts, colors):\n                mu_t_ot = np.interp(t_val, ts_ot, mus_ot)\n                sigma_t_ot = np.interp(t_val, ts_ot, sigmas_ot)\n                pdf_ot = gaussian_pdf_torch(GRID_X, mu_t_ot, sigma_t_ot).cpu().numpy()\n                interp_axes[0].plot(grid_np, pdf_ot, color=color, linewidth=2, label=f\"t={t_val:.2f}\")\n\n            interp_axes[0].set_title(\"OT interpolation (W2 geodesic)\")\n            interp_axes[0].set_xlabel(\"x\")\n            interp_axes[0].set_ylabel(\"Density\")\n            interp_axes[0].legend(loc=\"upper right\", ncol=2)\n\n            for t_val, color in zip(sample_ts, colors):\n                mu_t_fr = np.interp(t_val, ts_fr, mus_fr)\n                sigma_t_fr = np.interp(t_val, ts_fr, sigmas_fr)\n                pdf_fr = gaussian_pdf_torch(GRID_X, mu_t_fr, sigma_t_fr).cpu().numpy()\n                interp_axes[1].plot(grid_np, pdf_fr, color=color, linewidth=2, label=f\"t={t_val:.2f}\")\n\n            interp_axes[1].set_title(\"Information-geometry interpolation (Fisher–Rao)\")\n            interp_axes[1].set_xlabel(\"x\")\n            interp_axes[1].legend(loc=\"upper right\", ncol=2)\n\n            fig_interp.tight_layout()\n            plt.show()\n\n    for control in (ig_mu1_slider, ig_sigma1_slider, ig_mu2_slider, ig_sigma2_slider):\n        control.observe(_update_geodesic, names=\"value\")\n\n    _update_geodesic()\n    controls = widgets.HBox([widgets.VBox([ig_mu1_slider, ig_sigma1_slider]), widgets.VBox([ig_mu2_slider, ig_sigma2_slider])])\n    maybe_display(widgets.VBox([widgets.HTML(\"&lt;h4&gt;IG vs. OT Geodesic Explorer&lt;/h4&gt;\"), controls, ig_output]))\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Comparison of OT versus Fisher–Rao\n\n\n\n\nOT interpolates linearly in the mean; Fisher–Rao bends the path. How might that influence optimization trajectories?\n\nWhich metric is more sensitive to changes in variance vs. mean in this example?\n\nIf you needed to regularize a generative model toward a reference Gaussian, which geometry would you prefer and why?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html#wrap-up",
    "href": "part1/distribution_distance.html#wrap-up",
    "title": "6  Measuring Distribution Distances",
    "section": "6.7 Wrap-Up",
    "text": "6.7 Wrap-Up\nWe inspected five prominent distribution distances, paying attention to the gradients they induce for a simple Gaussian parameter. Key takeaways: - KL divergences emphasize support mismatch and can be asymmetric in how they penalize missing mass. - JS divergence softens gradients and stays bounded, making it attractive for adversarial training. - MMD exposes the role of kernel bandwidth in shaping sensitivity to local vs. global structure. - Wasserstein distance reasons about mass transport and yields interpretable gradient flows. - Fisher–Rao geometry offers a complementary, information-theoretic notion of proximity with different geodesics.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part2/part2.html",
    "href": "part2/part2.html",
    "title": "Model-Specific Approaches",
    "section": "",
    "text": "These chapters look more like a “traditional” textbook, in the sense that they cover individual techniques or concepts that I thought were maximally relevant to Mechanical Engineers at the time that I wrote the book. These chapters will be most immediately useful in terms of getting up to speed with specific types of models, but they are also the most likely to become out of date quickly as newer/better models are invented.\n\nReview of Prior Course Models – CNNs, UNets, RNNs, AEs\nAdvanced Neural Models\n\nThe Attention Mechanism\nIntroduction to Transformers\nRegularization of Neural Networks\nIntroduction to Geometric Deep Learning (Message Passing, GNNs, Working with Meshes)\nFailure Mechanisms in Neural Models\n\nProbabilistic Models and Kernels\n\nIntroduction to Probabilistic Graphical Models\nExact Inference (MLE, MAP, EM)\nApproximate Inference (MCMC, VI)\nIntroduction to Probabilistic Programming\nFailure Mechanisms in Probabilistic Models\n\nEnsembles (Not included in current course for scope reasons)\n\nBagging\nBoosting",
    "crumbs": [
      "Model-Specific Approaches"
    ]
  },
  {
    "objectID": "part2/review_neural_networks.html",
    "href": "part2/review_neural_networks.html",
    "title": "7  Review of Neural Networks",
    "section": "",
    "text": "7.1 Example of PyTorch SGD for Linear Regression and a Simple Feed-Forward Network\nThis notebook reviews some basic notation and concepts related to neural networks. It will skip many details that would have been covered in your earlier Stochastics and Machine Learning course, but will set a basis for us jumping into more advanced topics in a few chapters.\nBefore we get started with PyTorch code, I encourage you first to interactive with the ConvNetJS demo, specifically the interactive 1D Regression where you can toogle on and off the different layers and plotting functions, as well as the 2D Classification demo to see how a neural network is just performing a series of feature transformations that ultimately lead to a linear classification boundary. These demos help build a good intuition for what is going on before we move onto more obtuse PyTorch code.\nShow Code\nn_samples = 30\n\n# True Function we want to estimate\ndef true_func(X): return np.cos(1.5 * np.pi * X)\n\n# Noisy Samples from the true function\nX = np.sort(2*np.random.rand(n_samples)-1)\ny = true_func(X) + np.random.randn(n_samples) * 0.1\n\nplt.figure(figsize=(10,10))\n# Plot the true function:\nX_plot = np.linspace(-1.5, 1.5, 100)\nplt.plot(X_plot, true_func(X_plot), '--',label=\"True function\")\n# Plot the data samples\nplt.scatter(X,y, label=\"Samples\")\nplt.legend(loc=\"best\")\nplt.show()\nShow Code\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.4, random_state=0)\nplt.figure(figsize=(7,7))\n# Plot the data samples\nplt.scatter(X_train,y_train, label=\"Train\", c='Blue', s=20, edgecolors='none')\nplt.scatter(X_test,y_test, label=\"Test\", c='Red', s=50, edgecolors='none')\n#plt.plot(X_plot, true_func(X_plot), 'g--',label=\"True function\")\nplt.legend(loc=\"best\")\nsns.despine()\nplt.show()\nShow Code\n# Convert the data into a shape and data-type that PyTorch likes\nX_train = X_train.reshape(-1,1).astype(np.float32)\ny_train = y_train.reshape(-1,1).astype(np.float32)",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review of Neural Networks</span>"
    ]
  },
  {
    "objectID": "part2/review_neural_networks.html#example-of-pytorch-sgd-for-linear-regression-and-a-simple-feed-forward-network",
    "href": "part2/review_neural_networks.html#example-of-pytorch-sgd-for-linear-regression-and-a-simple-feed-forward-network",
    "title": "7  Review of Neural Networks",
    "section": "",
    "text": "7.1.1 Linear Regression\n\ninput_size  = 1\noutput_size = 1\n# Linear regression model\nmodel = nn.Linear(input_size, output_size)\n\n# Loss and optimizer\ncriterion = nn.MSELoss()\nlearning_rate = 0.1 # alpha\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n\n# Train the model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Convert numpy arrays to torch tensors\n    inputs = torch.from_numpy(X_train)\n    targets = torch.from_numpy(y_train)\n\n    # Forward pass\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch+1) % 20 == 0:\n        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n\nEpoch [20/100], Loss: 0.3992\nEpoch [40/100], Loss: 0.3596\nEpoch [60/100], Loss: 0.3553\nEpoch [80/100], Loss: 0.3548\nEpoch [100/100], Loss: 0.3548\n\n\n\n\nShow Code\n# Plot the graph\nplt.figure()\nplt.plot(X_train, y_train, 'ro', label='Data')\n#predicted = model(torch.from_numpy(X_train)).detach().numpy()\n#plt.plot(X_train, predicted, 'b+',label='Predictions')\npredicted = model(torch.from_numpy(X_plot.reshape(-1,1).astype(np.float32))).detach().numpy()\nplt.plot(X_plot, predicted, 'b', label='Prediction')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n7.1.2 Neural Network Examples\n\n7.1.2.1 Example of Defining a Network via the full Module class\n\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(Net, self).__init__()  \n        # Fully-Connected Layer: 1 (input data) -&gt; 5 (hidden node)\n        self.fc1 = nn.Linear(input_size, hidden_size)  \n        \n        # Non-Linear Layer\n        self.sigmoid = nn.Sigmoid()\n        # You can try other kinds as well\n        # self.relu = nn.ReLU()\n        # self.elu = nn.ELU()\n        \n        \n        # Fully-Connected Layer: 5 (hidden node) -&gt; 1 (output)\n        self.fc2 = nn.Linear(hidden_size, 1) \n    \n    # Forward pass builds the model prediction from the inputs\n    def forward(self, x):                              \n        out = self.fc1(x)\n        out = self.sigmoid(out)\n        out = self.fc2(out)\n        return out\n    \n# Build the network -- is it not trained yet\nmodel = Net(input_size=1, hidden_size=5)\n\n\n\n7.1.2.2 Example of building a model using the Sequential helper function\n\n\nShow Code\ninput_size=1\nhidden_size=4\nmodel = nn.Sequential(\n          nn.Linear(input_size, hidden_size),\n          nn.Sigmoid(),\n          nn.Linear(hidden_size, hidden_size),\n          nn.ReLU(),\n          nn.Linear(hidden_size, 1)\n        )\n\n\n\n\n7.1.2.3 Example using Python list expansions to help build deeper networks\n\n\nShow Code\ninput_size=1\nhidden_size=7\nnum_hidden_layers = 3\nactivation = nn.ReLU\n\ninput_layer = [nn.Linear(input_size, hidden_size),\n                activation()]\nhidden_layers = num_hidden_layers*[nn.Linear(hidden_size, hidden_size), \n                                   activation()]\noutput_layer = [ nn.Linear(hidden_size, 1) ] \n\n# Stack them all together\nlayers = input_layer + hidden_layers + output_layer\n\nprint(layers)\n\n# Use the * operator to \"expand\" or \"unpack\" the list\nmodel = nn.Sequential(*layers)\n\n\n[Linear(in_features=1, out_features=7, bias=True), ReLU(), Linear(in_features=7, out_features=7, bias=True), ReLU(), Linear(in_features=7, out_features=7, bias=True), ReLU(), Linear(in_features=7, out_features=7, bias=True), ReLU(), Linear(in_features=7, out_features=1, bias=True)]\n\n\n\n\n\n7.1.3 Now let’s do the actual training\n\n# What Loss function should we use? MSE!\ncriterion = nn.MSELoss()\n\n# What Optimization procedure should we use?\n\n##### Change these and let's see how it affects the model fit #################\nlearning_rate = 0.05\nweight_decay = 0.0\n##########################\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0)\n\n\n### Train the model\n\n# Convert numpy arrays to torch tensors\ninputs = torch.from_numpy(X_train)\ntargets = torch.from_numpy(y_train)\n\nnum_epochs = 5000\nfor epoch in range(num_epochs):\n\n    ## Do Forward pass\n    # Make predictions\n    outputs = model(inputs)\n    # Compute the loss function\n    loss = criterion(outputs, targets)\n    \n    ## Update the model\n    # Reset the optimizer gradients\n    optimizer.zero_grad()\n    # Compute the gradient of the loss function\n    loss.backward()\n    # Do an optimization step\n    optimizer.step()\n    \n    # Print the loss\n    if (epoch+1) % 200 == 0:\n        print ('Epoch [{:4}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n\nEpoch [ 200/5000], Loss: 0.0064\nEpoch [ 400/5000], Loss: 0.0025\nEpoch [ 600/5000], Loss: 0.0029\nEpoch [ 800/5000], Loss: 0.0023\nEpoch [1000/5000], Loss: 0.0023\nEpoch [1200/5000], Loss: 0.0023\nEpoch [1400/5000], Loss: 0.0023\nEpoch [1600/5000], Loss: 0.0023\nEpoch [1800/5000], Loss: 0.0022\nEpoch [2000/5000], Loss: 0.0023\nEpoch [2200/5000], Loss: 0.0022\nEpoch [2400/5000], Loss: 0.0033\nEpoch [2600/5000], Loss: 0.0023\nEpoch [2800/5000], Loss: 0.0209\nEpoch [3000/5000], Loss: 0.0218\nEpoch [3200/5000], Loss: 0.0023\nEpoch [3400/5000], Loss: 0.0022\nEpoch [3600/5000], Loss: 0.0020\nEpoch [3800/5000], Loss: 0.0070\nEpoch [4000/5000], Loss: 0.0020\nEpoch [4200/5000], Loss: 0.0019\nEpoch [4400/5000], Loss: 0.0023\nEpoch [4600/5000], Loss: 0.0019\nEpoch [4800/5000], Loss: 0.0030\nEpoch [5000/5000], Loss: 0.0118\n\n\nNow let’s plot the prediction:\n\n\nShow Code\n# Plot the graph\nplt.figure()\nplt.plot(X_train, y_train, 'ro', label='Data')\npredicted = model(torch.from_numpy(X_plot.reshape(-1,1).astype(np.float32))).detach().numpy()\nplt.plot(X_plot, predicted, 'b', label='Prediction')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Effect of Regularization on Simple Toy Regression Problem\n\n\n\nRevisit the above regression code and experiment with the weight decay parameter in the Adam optimizer. How does this affect the learned function? Why do you think this is?:\n\n\nYou can also play around with different activation functions, e.g., nn.ReLU(), nn.Sigmoid(), nn.Tanh(), etc., and the below plot pulls out several options from PyTorch for you to visualize the activation functions:\n\n\nShow Code\n# Common activation functions from PyTorch:\nactivations = {\n    'Identity': nn.Identity(),\n    'ReLU': nn.ReLU(),\n    'Sigmoid': nn.Sigmoid(),\n    'Tanh': nn.Tanh(),\n    'LeakyReLU': nn.LeakyReLU(),\n    'ELU': nn.ELU(),\n    'GELU': nn.GELU(),\n    'SiLU': nn.SiLU(),  # also known as Swish\n    'Softplus': nn.Softplus(),\n    'Softsign': nn.Softsign(),\n    'Hardtanh': nn.Hardtanh(),\n    'PReLU': nn.PReLU(),\n    'CELU': nn.CELU(),\n    'SELU': nn.SELU(),\n    'Mish': nn.Mish()\n}\nx = torch.linspace(-3, 3, 100)\nn = len(activations)\nncols = 4\nnrows = int(np.ceil(n/ncols))\nplt.figure(figsize=(15,10))\nfor i, (name, activation) in enumerate(activations.items()):\n    plt.subplot(nrows, ncols, i+1)\n    plt.plot(x.numpy(), activation(x).detach().numpy())\n    plt.ylim([-1.5, 3])\n    plt.title(name)\n    plt.grid()\nplt.tight_layout()\nplt.suptitle(\"Common Neural Network Activation Functions\", y=1.02, fontsize=16)\nplt.show()",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review of Neural Networks</span>"
    ]
  },
  {
    "objectID": "part2/review_neural_networks.html#unsupervised-learning-using-autoencoders",
    "href": "part2/review_neural_networks.html#unsupervised-learning-using-autoencoders",
    "title": "7  Review of Neural Networks",
    "section": "7.2 Unsupervised Learning using Autoencoders",
    "text": "7.2 Unsupervised Learning using Autoencoders\nFor this demonstration, we will construct what is fundamentally a 1D function (t) but then embed it in a higher-dimensional space (3D) using a non-linear transformation. This will allow us to compare what a linear method (PCA) can do versus a non-linear method (Autoencoder), on a simple example.\n\n# First we create a simple line (t)\nt = np.linspace(-1,1,100)\n\n\n# Now let's make it 3D and add some (optional) noise\nnoise_level = 0.01\n# You can try out different functions below by uncommenting/commenting them\n#X = np.vstack([t,1*t**2,-1*t**3, 0.4*t**5,t**2-t**3,-0.4*t**4]).T\n#X = np.vstack([2*np.sin(3*t),1*t**2,-1*t**3, 2*np.sin(6*t+1)-t**3,2*np.cos(3*t)]).T\nX = np.vstack([np.sin(2*t),1*t**2,1*np.cos(5*t)]).T + noise_level*np.random.randn(len(t),3)\n\n\n\nShow Code\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(X[:,0], X[:,1], X[:,2])\nax.set_xlabel('$X_0$')\nax.set_ylabel('$X_1$')\nax.set_zlabel('$X_2$')\nax.elev=45\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that it results in 100 points each of which has three dimensions:\n\nX.shape\n\n(100, 3)\n\n\nWe can attempt to reduce the dimensionality of this data using PCA, but as we can already see from the 3D plot, the data is not linearly embedded in 3D space, so PCA will not be able to find a good low-dimensional representation:\n\npca = PCA(3)\nZ_PCA =pca.fit_transform(X)\nplt.figure()\nplt.scatter(Z_PCA[:,0],Z_PCA[:,1],s=15)\nplt.xlabel('PCA Dim 1')\nplt.ylabel('PCA Dim 2')\nplt.title('PCA Projection on the first two principal components')\nplt.show()\n\n\n\n\n\n\n\n\nAnd we can also see this reflected in the explained variance, which shows that we need all three original dimensions to explain the variance in the data, even though we know that the data fundamentally lies on a 1D manifold:\n\n\nShow Code\nplt.figure()\nplt.plot(pca.explained_variance_)\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance')\nplt.title('PCA Explained Variance')\nplt.xticks([0,1,2])\nplt.ylim(0,1.1*max(pca.explained_variance_))\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see below what happens if we try to truncate PCA to only two dimensions and then reconstruct back to 3D space. The reconstruction is not very good, as expected, and what do you notice about the shape of the reconstructed data?\n\n\nShow Code\n\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\npca_2d = PCA(2)\nZ_PCA = pca_2d.fit_transform(X)\nX_PCA= pca_2d.inverse_transform(Z_PCA)\nax.scatter(X[:,0], X[:,1], X[:,2],alpha=0.5)\nax.scatter(X_PCA[:,0], X_PCA[:,1], X_PCA[:,2])\n\nax.set_xlabel('$X_0$')\nax.set_ylabel('$X_1$')\nax.set_zlabel('$X_2$')\nax.elev=35\nax.azim=10\nplt.title(\"PCA Reconstruction with only two components\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNow let’s see how this works using an autoencoder. We will use a very simple architecture with just one hidden layer in the encoder and one hidden layer in the decoder, and we will use 1D latent space (since we know that the data is fundamentally 1D). You will have the option in the below code to change the number of hidden units in the encoder and decoder, as well as the number of latent dimensions, and see how this affects the results.\n\nclass Encoder(nn.Module):\n    def __init__(self, num_input, num_latent,num_hidden):\n        super().__init__()\n        self.num_input  = num_input\n        self.num_latent = num_latent\n        self.num_hidden = num_hidden\n        \n        # I encourage you to modify the architecture here by adding more layers or changing activation functions, if you wish\n        self.encode = nn.Sequential(\n            nn.Linear(self.num_input, self.num_hidden),\n            nn.ReLU(),\n            #nn.Linear(self.num_hidden, self.num_hidden),\n            #nn.ReLU(),\n            nn.Linear(self.num_hidden, self.num_latent),\n        )\n        \n    def forward(self, X):\n        encoded = self.encode(X)\n        return encoded\n    \nclass Decoder(nn.Module):\n    def __init__(self, num_input, num_latent,num_hidden):\n        super().__init__()\n        self.num_input  = num_input\n        self.num_latent = num_latent\n        self.num_hidden = num_hidden\n        \n        self.decode = nn.Sequential(\n            nn.Linear(self.num_latent, self.num_hidden),\n            nn.ReLU(),\n            #nn.Linear(self.num_hidden, self.num_hidden),\n            #nn.ReLU(),\n            nn.Linear(self.num_hidden, self.num_input)\n        )\n        \n    def forward(self, Z):\n        decoded = self.decode(Z)\n        return decoded\n    \nclass AutoEncoder(nn.Module):\n    def __init__(self, num_input,num_latent,num_hidden):\n        super().__init__()\n        self.num_input  = num_input\n        self.num_latent = num_latent\n        self.num_hidden = num_hidden\n        \n        self.encoder = Encoder(num_input  = self.num_input,\n                               num_latent = self.num_latent,\n                               num_hidden = self.num_hidden)\n        self.decoder = Decoder(num_input  = self.num_input,\n                               num_latent = self.num_latent,\n                               num_hidden = self.num_hidden)\n        \n    def forward(self, X):\n        encoded = self.encoder(X)\n        decoded = self.decoder(encoded)\n        return decoded, encoded  # &lt;- return a tuple of two values\n    \n    def transform(self,X):\n        '''Take X and encode to latent space'''\n        return self.encoder(X)\n    \n    def inverse_transform(self,Z):\n        '''Take Z and decode to X space'''\n        return self.decoder(Z)\n\n\n# Here we can set some parameters for the autoencoder that we are about to train\n# What happens if you change them?\n# e.g., increase/decrease num_latent, num_hidden, learning rate, weight decay\nnum_points, D_orig = X.shape\nnum_latent = 3\nnum_hidden = 5\nmodel = AutoEncoder(D_orig,num_latent,num_hidden)\n# Create the optimizer object:\n# Adam optimizer with learning rate and weight decay\n# I encourage you to try out different learning rates and weight decays and\n# observe their effect on the model\noptimizer = optim.AdamW(model.parameters(), \n                        lr=1e-3, \n                        weight_decay=1e-2)\n# Add a mean-squared error loss\ncriterion = nn.MSELoss()\n\n\nX_torch = torch.from_numpy(X)\nX_torch = X_torch.float()\n\n# Depending on the model architecture you use, you may need to increase or decrease this to get good training\nepochs=10000\nfor epoch in range(epochs):\n    loss = 0\n\n    optimizer.zero_grad()\n\n    # compute reconstructions\n    decoded, encoded = model(X_torch)\n\n    # compute training reconstruction loss\n    train_loss = criterion(decoded, X_torch)\n            \n    # Total Loss\n    loss = train_loss \n    \n    # compute accumulated gradients\n    loss.backward()\n\n    # perform parameter update based on current gradients\n    optimizer.step()\n\n    # display the epoch training loss\n    if epoch%500==0:\n        print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss.item()))\n\nepoch : 1/10000, loss = 0.716544\nepoch : 501/10000, loss = 0.350305\nepoch : 1001/10000, loss = 0.090813\nepoch : 1501/10000, loss = 0.010054\nepoch : 2001/10000, loss = 0.000795\nepoch : 2501/10000, loss = 0.000162\nepoch : 3001/10000, loss = 0.000120\nepoch : 3501/10000, loss = 0.000097\nepoch : 4001/10000, loss = 0.000081\nepoch : 4501/10000, loss = 0.000073\nepoch : 5001/10000, loss = 0.000062\nepoch : 5501/10000, loss = 0.000054\nepoch : 6001/10000, loss = 0.000035\nepoch : 6501/10000, loss = 0.000016\nepoch : 7001/10000, loss = 0.000006\nepoch : 7501/10000, loss = 0.000000\nepoch : 8001/10000, loss = 0.000000\nepoch : 8501/10000, loss = 0.000000\nepoch : 9001/10000, loss = 0.000000\nepoch : 9501/10000, loss = 0.000000\n\n\nNow that the model is trained, we can put the data through the encoder and decoder to pull out both the encoded (i.e., latent) representation of each point, as well as the decoded (i.e., reconstructed) version of each point, and visualize the results.\n\ndecoded, encoded = model(X_torch)\n\nFirst, let’s take a look at the reconstructed data in the original 3D space, compared to the original data:\n\n\nShow Code\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(projection='3d')\nX_P = decoded.detach().numpy()\n\nax.scatter(X[:,0], X[:,1], X[:,2],alpha=0.5)\nax.scatter(X_P[:,0], X_P[:,1], X_P[:,2])\nax.set_xlabel('$X_0$')\nax.set_ylabel('$X_1$')\nax.set_zlabel('$X_2$')\nax.elev=25\nax.azim=10\nplt.legend(['Original Data','Reconstructed Data'])\nplt.title(\"AE Reconstruction\")\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s see how the encoded (i.e., latent) points actually look in Z space:\n\n\nShow Code\nZ = encoded.cpu().detach().numpy()\n# Only works if num_latent &gt;=2\nif num_latent&gt;=2:\n    plt.figure()\n    plt.scatter(Z[:,0],Z[:,1])\n    plt.xlabel('z1')\n    plt.ylabel('z2')\n    plt.title('Latent Space Representation of the autoencoder')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nplt.figure()\nplt.plot(Z[:,0],marker='+')\nplt.xlabel('index')\nplt.ylabel('$z_0$')\nplt.title('Latent Dimension 0 values for each data point')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also look at a pairplot of the points in the latent space to get a sense of how the different dimensions correlate with each other:\n\n\nShow Code\ndf = pd.DataFrame(Z, columns=[f'z{i}' for i in range(Z.shape[1])])\nplt.figure()\nsns.pairplot(df)\nplt.suptitle('Latent Space Pairplot', y=1.02)\nplt.show()\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Effect on Autoencoder Architecture on Reconstruction Accuracy and Latent Space Behavior\n\n\n\nRevisit the above autoencoder code and experiment by changing the number of hidden units, the bottleneck (latent) dimension, and the number of layers in the encoder and decoder. How do these changes affect the reconstruction accuracy and the behavior of the learned latent space? Consider some of the below questions:\n\nWe know that the data is fundamentally 1D, but why does setting the autoencoder latent dimension to 1 not work well? Why might it be useful to have a latent dimension larger than the true dimensionality of the data for this type of model?\nWhat happens if you set the latent dimension to be three or larger? Why do you think this happens? What have we given up by doing this?\nAs you increase or decrease the number of hidden units in Autoencoder, how does this affect the reconstruction accuracy? Why do you think this is? Consider in particular the case where the number of hidden units is one or two.\nUnlike in PCA, when we re-run the autoencoder training, we get different results each time. Why do you think this is?\nUnlike PCA, the autoencoder does not guarantee that the latent dimensions are orthogonal or ordered by importance (e.g., \\(z_0\\) being more important than \\(z_1\\), etc.). Do you see any evidence of this in the learned latent space? Why do you think this is?\nThe Autoencoder used a ReLU activation function in the hidden layers. How does this manifest in the way that the network reconstructs the data? (Note, this may only be visible if you set the latent dimension to 1 and the number of hidden units to a small number, e.g., 2 or 3).\nWe can see that when we set the latent dimension to two or greater, the autoencoder can reconstruct the data well, but it is not capturing the intrinsic dimensionality of the data. One way we might do this is by adding L1 regularization to the latent space coordinates, which is commented out in the code above. Try adding in this regularization. Does it fix the problem?",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review of Neural Networks</span>"
    ]
  },
  {
    "objectID": "part2/review_neural_networks.html#least-volume-regularization",
    "href": "part2/review_neural_networks.html#least-volume-regularization",
    "title": "7  Review of Neural Networks",
    "section": "7.3 Least Volume Regularization",
    "text": "7.3 Least Volume Regularization\nOne of the problems that we saw in Autoencoders is that the latent space does not have any particular structure, and in particular, it is not guaranteed to be ordered, in the same sense as PCA. It was also difficult for us to determine the exact “size” of the latent dimension, since, as we saw, setting the latent dimension to 1 did not work well, even though we knew that the data was fundamentally 1D, due to training variability and the fact that the autoencoder could get trapped in local minima. One possible solution to this is to add a regularization term that encourages the latent space to be small in some sense. One such regularization is called Least Volume Regularization, which encourages the latent space to have a small volume by penalizing the volume of the encoded points in the latent space.\n\n\nShow Code\nclass _Combo(nn.Module):\n    def forward(self, input):\n        return self.model(input)\n\nclass LinearCombo(_Combo):\n    def __init__(self, in_features, out_features, activation=nn.LeakyReLU(0.2)):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(in_features, out_features),\n            activation\n        )\n\nclass MLP(nn.Module):\n    \"\"\"Regular fully connected network generating features.\n\n    Args:\n        in_features: The number of input features.\n        out_feature: The number of output features.\n        layer_width: The widths of the hidden layers.\n        combo: The layer combination to be stacked up.\n\n    Shape:\n        - Input: `(N, H_in)` where H_in = in_features.\n        - Output: `(N, H_out)` where H_out = out_features.\n    \"\"\"\n    def __init__(\n        self, in_features: int, out_features:int, layer_width: list,\n        combo = LinearCombo\n        ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.layer_width = list(layer_width)\n        self.model = self._build_model(combo)\n\n    def forward(self, input):\n        return self.model(input)\n\n    def _build_model(self, combo):\n        model = nn.Sequential()\n        idx = -1\n        for idx, (in_ftr, out_ftr) in enumerate(self.layer_sizes[:-1]):\n            model.add_module(str(idx), combo(in_ftr, out_ftr))\n        model.add_module(str(idx+1), nn.Linear(*self.layer_sizes[-1])) # type:ignore\n        return model\n\n    @property\n    def layer_sizes(self):\n        return list(zip([self.in_features] + self.layer_width,\n        self.layer_width + [self.out_features]))\n\n\n\nambient_dim = X.shape[1]\n\n# Change the below latent dimension to see what happens to the embedded points\nlatent_dim = 3\n\nwidth = ambient_dim * 16\nencoder = MLP(ambient_dim, latent_dim, [width] * 4)\ndecoder = MLP(latent_dim, ambient_dim, [width] * 4)\n\nopt = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)\n\n\n# Set X to a torch tensor:\nX_torch = torch.from_numpy(X).float()\n\nfor i in range(5000):\n    opt.zero_grad()\n    z = encoder(X_torch)\n    rec_loss = F.mse_loss(decoder(z), X_torch)\n    loss = rec_loss\n    # If you want, you can even add an L1 penalty on the latent space\n    # to try to encourage sparsity by uncommenting the line below:\n    #loss += 1e-3 * torch.mean(torch.abs(z))\n\n    loss.backward()\n    opt.step()\n    if (i+1) % 100 == 0:\n        print(f'Epoch {i:4}: rec = {rec_loss:.5g}')\n\nEpoch   99: rec = 0.39166\nEpoch  199: rec = 0.31308\nEpoch  299: rec = 0.048501\nEpoch  399: rec = 0.023062\nEpoch  499: rec = 0.015194\nEpoch  599: rec = 0.010049\nEpoch  699: rec = 0.0068286\nEpoch  799: rec = 0.0048156\nEpoch  899: rec = 0.0035384\nEpoch  999: rec = 0.0026096\nEpoch 1099: rec = 0.0018915\nEpoch 1199: rec = 0.001313\nEpoch 1299: rec = 0.00084967\nEpoch 1399: rec = 0.00053819\nEpoch 1499: rec = 0.00027124\nEpoch 1599: rec = 0.00014737\nEpoch 1699: rec = 0.00010135\nEpoch 1799: rec = 8.2536e-05\nEpoch 1899: rec = 7.4167e-05\nEpoch 1999: rec = 6.9067e-05\nEpoch 2099: rec = 6.5697e-05\nEpoch 2199: rec = 6.2941e-05\nEpoch 2299: rec = 6.0584e-05\nEpoch 2399: rec = 5.8694e-05\nEpoch 2499: rec = 5.6992e-05\nEpoch 2599: rec = 5.5272e-05\nEpoch 2699: rec = 5.3765e-05\nEpoch 2799: rec = 5.2397e-05\nEpoch 2899: rec = 5.1138e-05\nEpoch 2999: rec = 5.0006e-05\nEpoch 3099: rec = 4.8935e-05\nEpoch 3199: rec = 4.7868e-05\nEpoch 3299: rec = 4.6667e-05\nEpoch 3399: rec = 4.5643e-05\nEpoch 3499: rec = 4.4691e-05\nEpoch 3599: rec = 4.3831e-05\nEpoch 3699: rec = 4.3016e-05\nEpoch 3799: rec = 4.2142e-05\nEpoch 3899: rec = 4.1268e-05\nEpoch 3999: rec = 4.0287e-05\nEpoch 4099: rec = 3.9385e-05\nEpoch 4199: rec = 3.8568e-05\nEpoch 4299: rec = 3.7773e-05\nEpoch 4399: rec = 3.7031e-05\nEpoch 4499: rec = 3.6334e-05\nEpoch 4599: rec = 3.5686e-05\nEpoch 4699: rec = 3.5056e-05\nEpoch 4799: rec = 3.4465e-05\nEpoch 4899: rec = 3.388e-05\nEpoch 4999: rec = 3.332e-05\n\n\nNow that the autoencoder has been trained, let’s take a look at the standard deviation of the embedded points (i.e., in \\(z\\)). We can sort the latent dimensions according to which dimensions have the highest standard deviation in Z.\n\n\nShow Code\nencoder.eval()\ndecoder.eval()\n\n# Embed the data into Z using the trained encoder\nwith torch.no_grad():\n    z = encoder(X_torch)\n# Now let's sort the latent codes by which ones have the\n# largest standard deviation in Z:\nidx = z.std(0).argsort(descending=True)\n\nplt.figure()\nplt.bar(np.arange(z.std(0).size(-1)), z.std(0)[idx])\nplt.title('latent STDs (autoencoder)')\nplt.show()\n\n\n\n\n\n\n\n\n\nAs with before, we can plot some of the data points in Z space:\n\n\nShow Code\nplt.figure()\nplt.scatter(z[:, idx[0]].cpu().detach().numpy(), z[:, idx[1]].cpu().detach().numpy(), s=10)\nplt.gca().set_aspect('equal')\nplt.xlabel('$z_0$')\nplt.ylabel('$z_1$')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also plot the covariance among the latent codes of the embedded data:\n\n\nShow Code\ncov = z.T[idx].cov().detach().cpu().numpy()\nplt.figure()\nplt.matshow(cov, cmap='Reds')\nfor (i, j), var in np.ndenumerate(cov):\n    plt.gca().text(j, i, '{:.3e}'.format(var), ha='center', va='center')\nplt.title('Latent Covariance Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also get a general idea about how well we are reconstructing the original data by comparing the ground truth values versus predicted (i.e., encoded then decoded) data points – this is often called a parity plot:\n\n\nShow Code\nX_ = decoder(z).detach()\n\nfor i in range(X_torch.size(-1)):\n    plt.figure(figsize=(3,3))\n    plt.scatter(X[:, i], X_[:, i], s=3)\n    plt.gca().set_aspect('equal')\n    plt.xlabel('groundtruth')\n    plt.ylabel('reconstruction')\n    plt.title('$x_{}$'.format(i))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLastly, we can visualize a pairplot of the latent space to see how the different dimensions correlate with each other:\n\n\nShow Code\ndf = pd.DataFrame(z.cpu().detach().numpy(), columns=[f'z{i}' for i in range(z.shape[1])])\nplt.figure()\nsns.pairplot(df)\nplt.suptitle('Latent Space Pairplot', y=1.02)\nplt.show()\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review of Neural Networks</span>"
    ]
  },
  {
    "objectID": "part2/review_neural_networks.html#how-might-we-select-the-right-order-in-an-autoencoder-building-the-case-for-least-volume-analysis-lva",
    "href": "part2/review_neural_networks.html#how-might-we-select-the-right-order-in-an-autoencoder-building-the-case-for-least-volume-analysis-lva",
    "title": "7  Review of Neural Networks",
    "section": "7.4 How might we select the right order in an Autoencoder? Building the case for Least Volume Analysis (LVA)",
    "text": "7.4 How might we select the right order in an Autoencoder? Building the case for Least Volume Analysis (LVA)\nOur explorations above exposed both the advantages and disadvantages of using non-linear maps to attempt to embed and capture the underlying distribution and topology of the data.\nBelow we describe the basic principle of “Least Volume” regularization in Autoencoders and demonstrate how it can be useful in providing automated order selection in over-parameterized Autoencoders. It can allow us to capture relevant topological structure, but with minimal dimension.\n\nImage above from: Qiuyi Chen and Mark Fuge, “Least Volume Analysis”\nIn general, the idea behind least volume is that we want to encourage the latent space to take up as little volume as possible, while still being able to reconstruct the data well. This can be achieved by adding a regularization term to the loss function that penalizes the volume of the latent space. A simple way to do this is to penalize the geometric mean of the standard deviation of the latent dimensions, which encourages the latent space to be small in all dimensions. Specifically, we can minimize the product of all elements of the latent code’s standard deviation vector \\(\\prod \\sigma\\), which is equivalent to minimizing the exponential of the mean of the log of the standard deviation vector:\nvol_loss = torch.exp(torch.log(z.std(0) + η).mean())\nWe add a small constant η to avoid numerical issues when any one of the standard deviation’s in any dimension approaches zero – that is when the autoencoder eliminates a dimension, and thus \\(\\prod \\sigma\\) would have a zero in the product. This loss term can be added to the reconstruction loss, weighted by a hyperparameter λ, to form the total loss.\nIn principle, while this loss can encourage the latent space to reduce its volume, there is one catch: the autoencoder could simply scale up the weights in the encoder and decoder to make the latent space arbitrarily small, while still being able to reconstruct the data well. To prevent this, we have to prevent the decoder from being able to arbitrarily increase its weights, and one easy way to enforce this is through spectral normalization on the weights of the decoder, which constrains the Lipschitz constant of the decoder to be at most 1. By preventing the decoder from scaling up its weights too much, and the encoder cannot easily defeat the volume penalty by isotropically shrinking the weights, and thus the only way to achieve a good volume penalty is to actually reduce dimensions.\nFun Fact: It turns out that in the case of an Autoencoder that only uses Linear layers, and with no activation functions, the least volume penalty is equivalent to PCA. That is, PCA can be seen as a special case of Least Volume Autoencoder. For more details on the mathematical proof, see Proposition 15 in the original paper.\nBelow code implements the spectral normalized decoder:\n\nfrom torch.nn.utils.parametrizations import spectral_norm\n\nclass SNLinearCombo(_Combo):\n    def __init__(self, in_features, out_features, activation=nn.LeakyReLU(0.2)):\n        super().__init__()\n        self.model = nn.Sequential(\n            spectral_norm(nn.Linear(in_features, out_features)),\n            activation\n        )\n\nclass SNMLP(MLP):\n    def __init__(\n        self, in_features: int, out_features: int, layer_width: list,\n        combo=SNLinearCombo):\n        super().__init__(in_features, out_features, layer_width, combo)\n\n    def _build_model(self, combo):\n        model = nn.Sequential()\n        idx = -1\n        for idx, (in_ftr, out_ftr) in enumerate(self.layer_sizes[:-1]):\n            model.add_module(str(idx), combo(in_ftr, out_ftr))\n        # Note here is the main difference: the last layer also has spectral normalization\n        # This was not the case in the previous MLP definition\n        model.add_module(str(idx+1), spectral_norm(nn.Linear(*self.layer_sizes[-1])))\n        return model\n\n\nwidth = ambient_dim * 16\n# Note in particular the lack of the bottleneck choice below\n# That is, we don't need to actually pick a bottleneck dimension -- LVA automatically determines this, like PCA\nencoder = MLP(ambient_dim, ambient_dim, [width] * 4)\n# Note also the change in the decoder to have spectral normalization\ndecoder = SNMLP(ambient_dim, ambient_dim, [width] * 4)\n\nopt = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n\n\nη, λ = 0.01, 0.01\n\nfor i in range(20000):\n    opt.zero_grad()\n    z = encoder(X_torch)\n    rec_loss = F.mse_loss(decoder(z), X_torch)\n    # Note below the least volume loss\n    vol_loss = torch.exp(torch.log(z.std(0) + η).mean())\n    loss = rec_loss + λ * vol_loss\n    loss.backward()\n    opt.step()\n\n\n    if (i+1) % 1000 == 0:\n        # Print floats with 5 significant digits and fill epoch with leading spaces if under 4 digits\n        print(f'Epoch {i:4}: rec = {rec_loss:.5g}, vol = {vol_loss:.5g}')\n\nEpoch  999: rec = 0.015768, vol = 0.20888\nEpoch 1999: rec = 0.0093937, vol = 0.2092\nEpoch 2999: rec = 0.0075273, vol = 0.21866\nEpoch 3999: rec = 0.0061228, vol = 0.26539\nEpoch 4999: rec = 0.0049839, vol = 0.23421\nEpoch 5999: rec = 0.0042767, vol = 0.1931\nEpoch 6999: rec = 0.0035521, vol = 0.17556\nEpoch 7999: rec = 0.0034988, vol = 0.22726\nEpoch 8999: rec = 0.0025694, vol = 0.1861\nEpoch 9999: rec = 0.0028511, vol = 0.202\nEpoch 10999: rec = 0.0022147, vol = 0.17631\nEpoch 11999: rec = 0.0021955, vol = 0.17034\nEpoch 12999: rec = 0.0020662, vol = 0.19222\nEpoch 13999: rec = 0.0019016, vol = 0.20411\nEpoch 14999: rec = 0.0015431, vol = 0.15869\nEpoch 15999: rec = 0.0014341, vol = 0.18272\nEpoch 16999: rec = 0.0015622, vol = 0.18977\nEpoch 17999: rec = 0.0013786, vol = 0.17417\nEpoch 18999: rec = 0.0017483, vol = 0.15937\nEpoch 19999: rec = 0.0014537, vol = 0.20069\n\n\n\n\nShow Code\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(projection='3d')\nX_P = decoder(encoder(X_torch)).detach().numpy()\n#X_P = decoded.detach().numpy()\n\nax.scatter(X[:,0], X[:,1], X[:,2],alpha=0.5)\nax.scatter(X_P[:,0], X_P[:,1], X_P[:,2])\nax.set_xlabel('$X_0$')\nax.set_ylabel('$X_1$')\nax.set_zlabel('$X_2$')\nax.elev=25\nax.azim=10\nplt.legend(['Original Data','Reconstructed Data'])\nplt.title(\"AE Reconstruction\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nencoder.eval()\ndecoder.eval()\n\nwith torch.no_grad():\n    z = encoder(X_torch)\nidx = z.std(0).argsort(descending=True)\n\nplt.scatter(z[:, idx[0]].cpu().detach().numpy(), z[:, idx[1]].cpu().detach().numpy(), s=10)\nplt.gca().set_aspect('equal')\nplt.xlabel('$z_0$')\nplt.ylabel('$z_1$')\nplt.show()\n\nplt.scatter(z[:, idx[0]].cpu().detach().numpy(), z[:, idx[2]].cpu().detach().numpy(),s=10)\nplt.gca().set_aspect('equal')\nplt.xlabel('$z_0$')\nplt.ylabel('$z_2$')\nplt.show()\n\n# Plot the latent STDs by magnitude in the sorted order:\nplt.figure()\nplt.bar(np.arange(z.std(0).size(-1)), z.std(0)[idx])\nplt.title('latent STDs (autoencoder)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting the latent code covariances:\n\n\nShow Code\ncov = z.T[idx].cov().detach().cpu().numpy()\nplt.matshow(cov, cmap='cool')\nfor (i, j), var in np.ndenumerate(cov):\n    plt.gca().text(j, i, '{:.2e}'.format(var), ha='center', va='center')\nplt.title('Latent Covariance Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\ndf = pd.DataFrame(z.cpu().detach().numpy(), columns=[f'z{i}' for i in range(z.shape[1])])\nplt.figure()\nsns.pairplot(df)\nplt.suptitle('Latent Space Pairplot', y=1.02)\nplt.show()\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nLet’s check again the reconstruction errors:\n\n\nShow Code\nX_ = decoder(z).detach()\n\nfor i in range(X_torch.size(-1)):\n    plt.figure(figsize=(3,3))\n    plt.scatter(X[:, i], X_[:, i], s=3)\n    plt.gca().set_aspect('equal')\n    plt.xlabel('groundtruth')\n    plt.ylabel('reconstruction')\n    plt.title('$x_{}$'.format(i))\n    plt.show()",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review of Neural Networks</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html",
    "href": "part2/gen_models/intro_to_GANS.html",
    "title": "8  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "",
    "text": "8.1 What are Generative Models really doing?\nIn this chapter, we will build geometric intuition for push-forward generative models, which are essentially trying to transport probability distributions from one space to another. We will start with simple linear maps to gain a sense of how they modify a simple 2D data space, then scale up to a small MLP GAN on a ring-of-Gaussians toy dataset. We will also talk about common metrics to evaluate generative models. Even though GANs may be comparatively simple compared to more advanced generative models that we will explore later, they will nevertheless be useful in building intuition for how push-forward generative models work.\nFundamentally, generative models are really just functions that transform a probability distribution from one space to another – you can think of them as “distribution transformers” or, move intuitively, as moving probability mass around in space. As an analogy, consider that you are sitting in a sandbox with a smooth mound of sand in front of you – the 2D Gaussian that we will use below is not too far from this, actually. A generative model is kind of like your hands, which you can use to push the sand around, creating hills and valleys, and moving the sand from one place to another. In this way, we are essentially moving the probability mass (sand, in this analogy) from a simple distribution (the smooth mound of sand) to a more complex distribution (the hills and valleys that you create with your hands). Different models that we will explore later (e.g., VAEs, normalizing flows, diffusion models) have different ways of doing this, but the core goal is the same, and they essentially all try to do one or more of three operations:\nAs we go forward, we will see how different types of models are better or worse at these different operations, and, in some cases, how they can be combined to create more powerful models.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#what-are-generative-models-really-doing",
    "href": "part2/gen_models/intro_to_GANS.html#what-are-generative-models-really-doing",
    "title": "8  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "",
    "text": "\\(f(z) \\rightarrow x\\): Mapping points from one space (typically called the latent space) to another (typically, the data space).\n\\(f^{-1}(x) \\rightarrow z\\): Mapping points from data space back to latent space.\n\\(p(x) \\Leftrightarrow  p(z)\\): Mapping probability densities from data space to latent space (and vice versa).",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#basic-deterministic-push-forward-models-from-latent-space-to-data-space",
    "href": "part2/gen_models/intro_to_GANS.html#basic-deterministic-push-forward-models-from-latent-space-to-data-space",
    "title": "8  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "8.2 Basic Deterministic Push-Forward Models: From Latent Space to Data Space",
    "text": "8.2 Basic Deterministic Push-Forward Models: From Latent Space to Data Space\nA push-forward generative model defines a mapping \\(x = f(z)\\), where \\(z\\) is sampled from a simple latent distribution (often a standard Gaussian, though it need not be) and \\(f\\) is a deterministic function (e.g., linear map or neural network).\nWe’ll start with a 2D latent \\(z \\sim N(0, I)\\) (i.e., a standard Gaussian) and inspect how different choices of \\(f\\) reshape the distribution in data space. In this case, the latent space is 2D and the data space is also 2D, so we can visualize both spaces directly, however, in general, the latent space is often lower-dimensional than the data space.\n\n\nShow Code\ndef plot_pushforward(z: np.ndarray, x: np.ndarray, title: str = \"Linear push-forward\"):\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    axes[0].scatter(z[:, 0], z[:, 1], s=8, alpha=0.35, color='tab:gray')\n    axes[0].set_title('Latent samples z ~ N(0, I)')\n    axes[0].set_xlabel('z1')\n    axes[0].set_ylabel('z2')\n    axes[0].axis('equal')\n    axes[0].grid(True, alpha=0.3)\n\n    axes[1].scatter(x[:, 0], x[:, 1], s=8, alpha=0.5, color='tab:blue')\n    axes[1].set_title(title)\n    axes[1].set_xlabel('x1')\n    axes[1].set_ylabel('x2')\n    axes[1].axis('equal')\n    axes[1].grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n\n\n# Linear push-forward demo with interactive sliders\ndef linear_pushforward_demo(A: np.ndarray, b: np.ndarray, n: int = 1000, seed: int = RNG_SEED):\n    rng = np.random.default_rng(seed)\n    z = rng.standard_normal(size=(n, 2))\n    x = (z @ A.T) + b\n    return z, x\n\n# Default transform\nA0 = np.array([[1.2, 0.4],[0.0, 0.8]], dtype=float)\nb0 = np.array([0.0, 0.0], dtype=float)\nz_lin, x_lin = linear_pushforward_demo(A0, b0, n=2000)\nplot_pushforward(z_lin, x_lin, title='x = A z + b')\n\nWe can see from above that the simple linear map has slightly “moved” or shifted the location of the probability mass, just like if you were sculpting sand in your sandbox. You can play below with some of the sliders to manipulate the simple 2x2 weight matrix and 2x1 bias vector to see how this works interactively:\n\n\nShow Code\nif interact is not None:\n    def _interactive_pushforward(a11=1.2, a12=0.4, a21=0.0, a22=0.8, bx=0.0, by=0.0):\n        A = np.array([[a11, a12],[a21, a22]], dtype=float)\n        b = np.array([bx, by], dtype=float)\n        z, x = linear_pushforward_demo(A, b, n=2000)\n        plot_pushforward(z, x, title=f'x = A z + b')\n    interact(\n        _interactive_pushforward,\n        a11=FloatSlider(min=-2.0, max=2.0, step=0.05, value=1.2, description='a11'),\n        a12=FloatSlider(min=-2.0, max=2.0, step=0.05, value=0.4, description='a12'),\n        a21=FloatSlider(min=-2.0, max=2.0, step=0.05, value=0.0, description='a21'),\n        a22=FloatSlider(min=-2.0, max=2.0, step=0.05, value=0.8, description='a22'),\n        bx=FloatSlider(min=-2.0, max=2.0, step=0.05, value=0.0, description='b1'),\n        by=FloatSlider(min=-2.0, max=2.0, step=0.05, value=0.0, description='b2'),\n    )",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#toy-dataset-ring-of-gaussians",
    "href": "part2/gen_models/intro_to_GANS.html#toy-dataset-ring-of-gaussians",
    "title": "8  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "8.3 Toy Dataset: Ring of Gaussians",
    "text": "8.3 Toy Dataset: Ring of Gaussians\nOK, now that we have some intuition for how simple linear maps can move probability mass around in space, let’s try a more complex example where it is not so clear that a simple linear map will be sufficient. Below, we will create a common toy dataset consisting of a “ring of Gaussians” – that is, several Gaussian blobs arranged in a circle. You can see from above that no combination of weights in a simple linear map will be able to move our original probability mass from a standard Gaussian to this ring of Gaussians, so we will need something more powerful. A simple starting point for this is something called a Generative Adversarial Network (GAN), which we will explore next.\n\n\nShow Code\nX_ring, y_ring = create_ring_gaussians()\nplt.figure(figsize=(5.5,5.5))\nsc = plt.scatter(X_ring[:,0], X_ring[:,1], c=y_ring, cmap='tab10', s=10, alpha=0.6)\nplt.colorbar(sc, label='Mode index')\nplt.title('Toy Dataset: Colored Gaussian Ring')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.axis('equal')\nplt.grid(True, alpha=0.2)\nplt.tight_layout()\nplt.show()\n\nring_latent_limits = ((-3.5, 3.5), (-3.5, 3.5))\nring_data_limits = _compute_axis_limits(X_ring)",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#what-is-a-generative-adversarial-network-gan",
    "href": "part2/gen_models/intro_to_GANS.html#what-is-a-generative-adversarial-network-gan",
    "title": "8  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "8.4 What is a Generative Adversarial Network (GAN)?",
    "text": "8.4 What is a Generative Adversarial Network (GAN)?\nA Generative Adversarial Network (GAN) is a framework for training generative models using two competing neural networks: a generator and a discriminator. The generator, \\(G\\), learns to map random noise from a latent space (e.g., \\(z \\sim p_z(z)\\) or \\(f(z)=x\\), where \\(G\\) functions as \\(f\\) here) to data space, aiming to produce samples that resemble the real data. It is tempting to want to train just the Generator by minimizing Mean Squared Error between \\(z\\) and \\(f(z)=x\\), except we have one big problem for right now – we do not know apriori which samples of \\(x\\) correspond to which samples of \\(z\\), and so it is not straightforward to compute this MSE.1 Instead, what we will do is train a separate network, the Discriminator, that can help us push the samples produced by the Generator closer to the real data distribution.\n1 Indeed, we will return to this idea in later notebooks once we have introduced the concept of Optimal Transport, which solves this mapping problem, but for now, let’s assume that we don’t know how to do this.Specifically, the discriminator, \\(D\\), tries to distinguish between real data samples and those produced by the generator and our loss function will encourage the discriminator to become better and better at this task. In turn, the generator will be trained to produce samples that the discriminator classifies as real – that is to try to fool the disciminator.\nThe two networks are trained simultaneously in something called a minimax game:\n\nThe generator tries to “fool” the discriminator by generating realistic samples.\nThe discriminator tries to correctly classify real vs. generated samples.\n\n\n8.4.1 Mathematical Formulation\nThe standard GAN objective, as introduced by Goodfellow et al. (2014), is:\n\\[\n\\min_G \\max_D \\; V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\left[ \\log D(x) \\right] + \\mathbb{E}_{z \\sim p_z(z)} \\left[ \\log (1 - D(G(z))) \\right]\n\\]\n\n\\(p_{\\text{data}}(x)\\): Distribution of real data.\n\\(p_z(z)\\): Prior distribution over latent variables (often standard normal).\n\\(G(z)\\): Generator’s output given latent input \\(z\\).\n\\(D(x)\\): Discriminator’s estimate of the probability that \\(x\\) is real.\n\nThe generator and discriminator are typically neural networks trained with stochastic gradient descent. The generator improves by producing samples that the discriminator cannot distinguish from real data (the second term on the right-hand side of the equation above), while the discriminator improves by getting better at distinguishing real from fake (the first term on the right-hand side of the equation above).",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#a-simple-generative-adversarial-network-gan",
    "href": "part2/gen_models/intro_to_GANS.html#a-simple-generative-adversarial-network-gan",
    "title": "8  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "8.5 A Simple Generative Adversarial Network (GAN)",
    "text": "8.5 A Simple Generative Adversarial Network (GAN)\nNow we will define a small GAN and see how it moves probability mass around after passing samples through the Generator. You can feel free to modify the architecture below if you like and see how that impacts the below results, but for now we will use a single hidden layer MLP with LeakyReLU activation for both the Generator and Discriminator.\n\nclass MLPGenerator(nn.Module):\n    def __init__(self, noise_dim: int = 2, hidden_dim: int = 256, out_dim: int = 2):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Linear(noise_dim, hidden_dim), nn.LeakyReLU(),\n            nn.Linear(hidden_dim, hidden_dim), nn.LeakyReLU(),\n            nn.Linear(hidden_dim, out_dim),\n)\n    def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n        return self.main(z)\n\nclass MLPDiscriminator(nn.Module):\n    def __init__(self, input_dim: int = 2, hidden_dim: int = 256):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim), nn.LeakyReLU(0.2),\n            nn.Linear(hidden_dim, hidden_dim), nn.LeakyReLU(0.2),\n            nn.Linear(hidden_dim, 1),\n)\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.main(x).squeeze(-1)\n\ndef build_generator(noise_dim=2, hidden_dim=256):\n    return MLPGenerator(noise_dim=noise_dim, hidden_dim=hidden_dim).to(device)\n\ndef build_discriminator(hidden_dim=256):\n    return MLPDiscriminator(hidden_dim=hidden_dim).to(device)",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#basic-training-loop",
    "href": "part2/gen_models/intro_to_GANS.html#basic-training-loop",
    "title": "8  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "8.6 Basic Training Loop",
    "text": "8.6 Basic Training Loop\nWith the models defined, we will now set up the training loop to optimize both the generator and discriminator.\n\ndef train_vanilla_gan(\n    data: np.ndarray, *, noise_dim: int = 2, batch_size: int = 256, epochs: int = 120,\n    lr_g: float = 2e-4, lr_d: float = 2e-4, hidden_dim: int = 256, print_every: int = 40) -&gt; tuple[nn.Module, nn.Module, GanHistory]:\n    # Load the data into a DataLoader for batching and make PyTorch happy\n    loader = make_loader(data, batch_size)\n\n    # Set up the basic networks\n    G = build_generator(noise_dim=noise_dim, hidden_dim=hidden_dim)\n    D = build_discriminator(hidden_dim=hidden_dim)\n    # Instantiate the optimizers for each model\n    opt_g = optim.Adam(G.parameters(), lr=lr_g, betas=(0.5, 0.999))\n    opt_d = optim.Adam(D.parameters(), lr=lr_d, betas=(0.5, 0.999))\n    bce = nn.BCEWithLogitsLoss()\n    # Record the loss history for plotting later\n    hist = GanHistory([], [], [], [], [])\n\n    # Now we do the training loop for # epochs defined in `epochs`\n    for ep in range(epochs):\n        d_losses=[]\n        g_losses=[]\n        real_scores=[]\n        fake_scores=[]\n        for (xb,) in loader:\n            # Send the data to the GPU, if using.\n            xb = xb.to(device)\n\n            # Take a Discriminator step\n            z = torch.randn(xb.size(0), noise_dim, device=device)\n            with torch.no_grad():\n                x_fake = G(z)\n            opt_d.zero_grad()\n            d_real = D(xb)\n            d_fake = D(x_fake)\n            loss_d = bce(d_real, torch.ones_like(d_real)) + bce(d_fake, torch.zeros_like(d_fake))\n            loss_d.backward()\n            opt_d.step()\n            d_losses.append(float(loss_d.detach().cpu().item()))\n            real_scores.append(d_real.mean().item())\n            fake_scores.append(d_fake.mean().item())\n\n            # Take a Generator step\n            z = torch.randn(xb.size(0), noise_dim, device=device)\n            opt_g.zero_grad()\n            xg = G(z)\n            dg = D(xg)\n            loss_g = bce(dg, torch.ones_like(dg))\n            loss_g.backward()\n            opt_g.step()\n            g_losses.append(float(loss_g.detach().cpu().item()))\n        \n        # We'll record some epoch metrics\n        with torch.no_grad():\n            z_eval = torch.randn(2048, noise_dim, device=device)\n            samples = G(z_eval)\n            div = compute_diversity_metric(samples)\n\n        # Now we'll record the metrics for plotting later and reporting\n        hist.d_loss.append(float(np.mean(d_losses)))\n        hist.g_loss.append(float(np.mean(g_losses)))\n        hist.diversity.append(div)\n        hist.real_scores.append(float(np.mean(real_scores)))\n        hist.fake_scores.append(float(np.mean(fake_scores)))\n        if (ep+1) % max(1, print_every) == 0 or ep==0:\n            print(f\"Epoch {ep+1:03d}/{epochs} | D {hist.d_loss[-1]:.3f} | G {hist.g_loss[-1]:.3f} | Div {div:.3f}\")\n    return G, D, hist\n\nYou can feel free to modify some of the training elements here, such as the epoch length or learning rates of the Generator or Discriminator, respectively.\n\nG, D, H = train_vanilla_gan(\n    X_ring, epochs=120, batch_size=256, \n    lr_g=2e-4, lr_d=2e-4, \n    hidden_dim=256, noise_dim=2, \n    print_every=40)\n\nOK, now the model is trained, so let’s look at some basic visualizations of how it did. In the below plots, which we will re-use for other models later on, we will show four things:\n\nUpper Left: Any training losses for the model, as a function of training epoch. In the case of a GAN, this will plot both the Generator and Discriminator losses. This plot type allows us to assess something about the convergence and stability of training.\nUpper Right: The Sample Diversity, as a function of Epoch. This is computing and plotting the variance of \\(\\mathbf{x}\\) as a function of epoch, where \\(\\mathbf{x}\\) are samples drawn from the Generator at each epoch. This plot type allows us one way to assess whether the model is suffering from mode collapse (i.e., low diversity in the generated samples) or not.\nLower Right: A plot of the real data samples (in light grey) and samples drawn from the Generator (in reddish-orange). This plot type allows us to visually assess how well the model is capturing the data distribution. This will be easy to compare in this simple 2D case, but will be harder in higher dimensions, and in those cases you might have to resort to just comparing selected samples or overall distribution summary statistics.\nLower Right: This plot will vary depending on the specific model we are studying, but in this case it will show the specific scores of the Discriminator (often called the “Critic”) with respect to its classification accuracy on real vs. fake samples. In the context of a GAN model, this allows us to assess how well a Generator is fooling the discriminator, since a well-trained Generator should produce samples that the Discriminator classifies as real, and thus both the “Real Score” and “Fake Score” should have around 50% accuracy. (i.e., the Discriminator is effectively guessing randomly).\n\n\n\nShow Code\nplot_model_diagnostics(H, X_ring, G, noise_dim=2, title_suffix=' (Vanilla GAN)')\n\n\nIn addition to the above training diagnostics, it can also be useful to plot interpolations in latent space to see how smoothly the Generator can move between different modes of the data distribution. In the case of a well-trained GAN, we would expect that interpolating between two points in latent space should produce a smooth transition in data space, moving through intermediate samples that also look realistic. In this case, this means hopping smoothly between the different Gaussian blobs in our ring-of-Gaussians dataset, and not jumping back-and-forth randomly between them.\n\n\nShow Code\nplot_latent_interpolation(G, noise_dim=2, title_suffix=' (Vanilla GAN)')\n\n\nTo give you some interactive control on visualizing the forward mapping \\(f(z) \\rightarrow x\\), use the sliders below to move a single latent vector (left-side plot) and observe the corresponding generated point in data space (right-side plot). By moving around in \\(z\\) you can try to align the generated point with a particular mode on the ring.\n\n\nShow Code\nif interact is not None:\n    def _move_latent(z1: float = 0.0, z2: float = 0.0):\n        \"\"\"\n        Visualize the effect of moving a latent vector z = [z1, z2] through the generator.\n        Left: latent space (z1, z2) with current point highlighted.\n        Right: generated data point in data space, overlaid on the real data.\n        \"\"\"\n        z = torch.tensor([[z1, z2]], dtype=torch.float32, device=device)\n        with torch.no_grad():\n            x = G(z).cpu().numpy()[0]\n\n        fig, axes = plt.subplots(1, 2, figsize=(11, 5))\n\n        # Left: latent space\n        axes[0].scatter(0, 0, s=40, color='gray', alpha=0.2, label='Origin')\n        axes[0].scatter(z1, z2, s=120, color='crimson', edgecolors='k', linewidths=0.5, label='Current z')\n        axes[0].set_title('Latent Space (z)')\n        axes[0].set_xlabel('z1')\n        axes[0].set_ylabel('z2')\n        axes[0].set_xlim(ring_latent_limits[0])\n        axes[0].set_ylim(ring_latent_limits[1])\n        axes[0].grid(True, alpha=0.3)\n        axes[0].legend(loc='upper left', frameon=False)\n\n        # Right: data space\n        axes[1].scatter(X_ring[:,0], X_ring[:,1], s=10, alpha=0.15, color='gray', label='Real')\n        axes[1].scatter([x[0]], [x[1]], s=120, color='crimson', edgecolors='k', linewidths=0.5, label='Generated')\n        axes[1].set_title('Generated Data Point')\n        axes[1].set_xlabel('x1')\n        axes[1].set_ylabel('x2')\n        axes[1].set_xlim(ring_data_limits[0])\n        axes[1].set_ylim(ring_data_limits[1])\n        axes[1].grid(True, alpha=0.3)\n        axes[1].legend(loc='upper left', frameon=False)\n\n        plt.tight_layout()\n        plt.show()\n    interact(\n        _move_latent,\n        z1=FloatSlider(min=-3.0, max=3.0, step=0.05, value=0.0, description='z1'),\n        z2=FloatSlider(min=-3.0, max=3.0, step=0.05, value=0.0, description='z2'),\n    )\nelse:\n    print('ipywidgets not available; skipping latent sliders.')",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#summary-and-next-steps",
    "href": "part2/gen_models/intro_to_GANS.html#summary-and-next-steps",
    "title": "8  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "8.7 Summary and Next Steps:",
    "text": "8.7 Summary and Next Steps:\nOK, so we have seen how a basic GAN is set up and trained, and covered some diagnostic plots that give us insight into how the training went. This gave us the following sets of useful tools:\n\nWe could learn a forward mapping function \\(f(z) \\rightarrow x\\) that could push forward a given sample in \\(z\\) to a corresponding sample in \\(x\\). This could learn distributions that were far more complex than a simple linear map could achieve above.\nThis \\(f\\) is a deterministic function – i.e., the same \\(z\\) will always produce the same \\(x\\).\nWe could produce latent interpolations in \\(z\\) that produced smooth transitions in \\(x\\) and seemed to capture the overall clustering behavior of certain distributions.\n\nUnfortunately, GANs did not give us any of the following properties, which will turn out to be useful later and motivate other types of generative models:\n\nWe did not learn an inverse mapping \\(f^{-1}(x) \\rightarrow z\\). This means that we cannot easily encode real data samples into the latent space, which would be useful for many applications. For example, if I wanted to slightly modify \\(x\\) – e.g., \\(x^\\prime = x+\\delta x\\) – and compute the corresponding latent coordinate \\(z^\\prime = z+\\delta z\\), GANS do not provide a built-it mechanism to do this.2 We will see how both Variational Autoencoders (VAEs) and Normalizing Flows (NFs) address this problem.\nWe also cannot yet easily talk about or compute probability mappings between \\(p(x)\\) and \\(p(z)\\), which would be useful for computing likelihoods of data samples or for evaluating how well the model fits the data distribution, both of which are quite useful. Again, we will see later how both VAEs and NFs address this problem.\n\n2 Of course, I could try to get at this another way, for example, by applying automatic differentiation to the Generator to compute \\(\\frac{\\partial f}{\\partial z}\\) and then use that to estimate \\(\\delta z \\approx (\\frac{\\partial f}{\\partial z})^{-1} \\delta x\\), but this requires extra work on our part and this is not always straightforward, especially if the Jacobian is ill-conditioned.Before we move beyond GANs, however, we will first explore some of the complex training dynamics and other subtle issues that arise when applying them in practice. This will help you build up your intuition and experience in how to assess problems in generative models and will serve as a strong basis upon which to launch into more complex models later. It will also give us an opportunity to introduce the concept of Optimal Transport, which will be useful in its own right applications later on, and will also help us understand some of the limitations of GANs should you encounter their use in the future.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/GAN_pitfalls.html",
    "href": "part2/gen_models/GAN_pitfalls.html",
    "title": "9  GAN Training Pitfalls",
    "section": "",
    "text": "9.1 Experiment 1: When the Discriminator Overpowers the Generator\nThis notebook will highlight some of the original problems with GAN models, some of the techniques you can use to diagnose model errors, and also introduce common rememdies for simple feed-forward GAN models. The next notebook will introduce the concept of Optimal Transport, and show how this mitigates some of the problems caused by the original GAN formulation as a minimax game.\nAs with the last notebook, we will use the simple Ring of Gaussian Toy example to illustrate this for pedagogical purposes. In later notebooks, we will see how these problems can manifest in more complex settings.\nLet’s start by loading the dataset again:\nWe will now run a series of experiments to show you common problems and how they manifest in the plotting visualizations.\nIn this experiment, we will see what happens with the Discriminator part of the GAN significantly overpowers the Generator. This can occur because of several reasons:\nWe can induce this below by just significantly increasing the learning rate of the Discriminator relative to the Generator. Consider the following questions and experiment:\nShow Code\nclass MLPGenerator(nn.Module):\n    def __init__(self, noise_dim: int = 2, hidden_dim: int = 256, out_dim: int = 2, spectral_normalization: bool = False):\n        super().__init__()\n        # Create linear layers and optionally apply spectral normalization\n        lin1 = nn.Linear(noise_dim, hidden_dim)\n        lin2 = nn.Linear(hidden_dim, hidden_dim)\n        lin3 = nn.Linear(hidden_dim, out_dim)\n        if spectral_normalization:\n            from torch.nn.utils import spectral_norm\n            lin1 = spectral_norm(lin1)\n            lin2 = spectral_norm(lin2)\n            lin3 = spectral_norm(lin3)\n        self.main = nn.Sequential(\n            lin1, nn.LeakyReLU(),\n            lin2, nn.LeakyReLU(),\n            lin3,\n        )\n\n    def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n        return self.main(z)\n\nclass MLPDiscriminator(nn.Module):\n    def __init__(self, input_dim: int = 2, hidden_dim: int = 256, spectral_normalization: bool = False):\n        super().__init__()\n        # Optionally apply spectral normalization to linear layers to stabilize training\n        layers = []\n        lin1 = nn.Linear(input_dim, hidden_dim)\n        lin2 = nn.Linear(hidden_dim, hidden_dim)\n        lin3 = nn.Linear(hidden_dim, 1)\n        if spectral_normalization:\n            from torch.nn.utils import spectral_norm\n            lin1 = spectral_norm(lin1)\n            lin2 = spectral_norm(lin2)\n            lin3 = spectral_norm(lin3)\n        layers.extend([lin1, nn.LeakyReLU(0.2), lin2, nn.LeakyReLU(0.2), lin3])\n        self.main = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.main(x).squeeze(-1)\n\n\ndef build_generator(noise_dim=2, hidden_dim=256, spectral_normalization: bool = False):\n    return MLPGenerator(noise_dim=noise_dim, hidden_dim=hidden_dim, spectral_normalization=spectral_normalization).to(device)\n\n\ndef build_discriminator(hidden_dim=256, spectral_normalization: bool = False):\n    return MLPDiscriminator(hidden_dim=hidden_dim, spectral_normalization=spectral_normalization).to(device)\nShow Code\ndef train_vanilla_gan(\n    data: np.ndarray, *, noise_dim: int = 2, batch_size: int = 256, epochs: int = 120,\n    lr_g: float = 2e-4, lr_d: float = 2e-4, hidden_dim: int = 256, print_every: int = 40,\n    spectral_normalization: bool = False, checkpoint_interval: int = 0) -&gt; tuple[nn.Module, nn.Module, GanHistory]:\n    # Load the data into a DataLoader for batching and make PyTorch happy\n    loader = make_loader(data, batch_size)\n\n    # Set up the basic networks\n    G = build_generator(noise_dim=noise_dim, hidden_dim=hidden_dim)\n    D = build_discriminator(hidden_dim=hidden_dim, spectral_normalization=spectral_normalization)\n    # Instantiate the optimizers for each model\n    opt_g = optim.Adam(G.parameters(), lr=lr_g, betas=(0.5, 0.999))\n    opt_d = optim.Adam(D.parameters(), lr=lr_d, betas=(0.5, 0.999))\n    bce = nn.BCEWithLogitsLoss()\n    # Record the loss history for plotting later\n    hist = GanHistory([], [], [], [], [])\n\n    # Helper to capture a snapshot\n    def _capture_snapshot(ep_idx: int):\n        with torch.no_grad():\n            z_eval = torch.randn(2048, noise_dim, device=device)\n            samples = G(z_eval).cpu().numpy()\n        # Save a shallow copy of generator state and a small sample for visualization\n        state = {k: v.cpu().clone() for k, v in G.state_dict().items()}\n        small_samples = samples[np.random.choice(samples.shape[0], size=min(512, samples.shape[0]), replace=False)]\n        hist.snapshots.append({\"epoch\": ep_idx, \"state_dict\": state, \"samples\": small_samples})\n\n    # Optionally capture an initial snapshot\n    if checkpoint_interval and checkpoint_interval &gt; 0:\n        _capture_snapshot(0)\n\n    # Now we do the training loop for # epochs defined in `epochs`\n    for ep in range(epochs):\n        d_losses=[]\n        g_losses=[]\n        real_scores=[]\n        fake_scores=[]\n        for (xb,) in loader:\n            # Send the data to the GPU, if using.\n            xb = xb.to(device)\n\n            # Take a Discriminator step\n            z = torch.randn(xb.size(0), noise_dim, device=device)\n            with torch.no_grad():\n                x_fake = G(z)\n            opt_d.zero_grad()\n            d_real = D(xb)\n            d_fake = D(x_fake)\n            loss_d = bce(d_real, torch.ones_like(d_real)) + bce(d_fake, torch.zeros_like(d_fake))\n            loss_d.backward()\n            opt_d.step()\n            d_losses.append(float(loss_d.detach().cpu().item()))\n            real_scores.append(d_real.mean().item())\n            fake_scores.append(d_fake.mean().item())\n\n            # Take a Generator step\n            z = torch.randn(xb.size(0), noise_dim, device=device)\n            opt_g.zero_grad()\n            xg = G(z)\n            dg = D(xg)\n            loss_g = bce(dg, torch.ones_like(dg))\n            loss_g.backward()\n            opt_g.step()\n            g_losses.append(float(loss_g.detach().cpu().item()))\n        \n        # We'll record some epoch metrics\n        with torch.no_grad():\n            z_eval = torch.randn(2048, noise_dim, device=device)\n            samples = G(z_eval)\n            div = compute_diversity_metric(samples)\n\n        # Now we'll record the metrics for plotting later and reporting\n        hist.d_loss.append(float(np.mean(d_losses)))\n        hist.g_loss.append(float(np.mean(g_losses)))\n        hist.diversity.append(div)\n        hist.real_scores.append(float(np.mean(real_scores)))\n        hist.fake_scores.append(float(np.mean(fake_scores)))\n\n        # Optionally capture a checkpoint snapshot\n        if checkpoint_interval and checkpoint_interval &gt; 0 and ((ep + 1) % checkpoint_interval == 0):\n            _capture_snapshot(ep + 1)\n\n        if (ep+1) % max(1, print_every) == 0 or ep==0:\n            print(f\"Epoch {ep+1:03d}/{epochs} | D {hist.d_loss[-1]:.3f} | G {hist.g_loss[-1]:.3f} | Div {div:.3f}\")\n    return G, D, hist\nG, D, H = train_vanilla_gan(\n    X_ring, epochs=120, batch_size=256,\n    lr_g=1e-5,\n    lr_d=1e-3,\n    hidden_dim=256, noise_dim=2, \n    checkpoint_interval=10,\n    print_every=40)\n\nEpoch 001/120 | D 0.537 | G 1.542 | Div 0.009\nEpoch 040/120 | D 0.331 | G 3.036 | Div 0.307\nEpoch 040/120 | D 0.331 | G 3.036 | Div 0.307\nEpoch 080/120 | D 0.388 | G 2.256 | Div 0.062\nEpoch 080/120 | D 0.388 | G 2.256 | Div 0.062\nEpoch 120/120 | D 0.394 | G 2.226 | Div 0.062\nEpoch 120/120 | D 0.394 | G 2.226 | Div 0.062\nShow Code\nplot_model_diagnostics(H, X_ring, G, noise_dim=2, title_suffix=' (GAN w/ D &gt;&gt; G)')\nplot_latent_interpolation(G, noise_dim=2, title_suffix=' (GAN w/ D &gt;&gt; G)')\nWe can also take a look at this interactively, by viewing various snapshots during training:\nShow Code\n# Interactive checkpoint browser (requires ipywidgets)\nif interact is None:\n    print(\"ipywidgets not available. Install ipywidgets to use the interactive checkpoint browser.\")\nelse:\n    from ipywidgets import IntSlider, Output, VBox, Label\n\n    out = Output()\n\n    def show_checkpoint(idx: int):\n        if not hasattr(H, \"snapshots\") or len(H.snapshots) == 0:\n            with out:\n                print(\"No checkpoints available. Rerun training with checkpoint_interval&gt;0 to populate snapshots.\")\n            return\n        cp = H.snapshots[idx]\n        # Load generator state into a temporary generator instance\n        G_temp = build_generator(noise_dim=2, hidden_dim=256)\n        # load state dict (ensure tensors are moved to device)\n        state = {k: v.to(device) for k, v in cp[\"state_dict\"].items()}\n        G_temp.load_state_dict(state)\n\n        with out:\n            out.clear_output(wait=True)\n            # Plot diagnostics and latent interpolation for this checkpoint\n            print(f\"Checkpoint: index={idx} | epoch={cp['epoch']}\")\n            plot_model_diagnostics(H, X_ring, G_temp, noise_dim=2, title_suffix=f\" (Checkpoint ep {cp['epoch']})\")\n            plot_latent_interpolation(G_temp, noise_dim=2, title_suffix=f\" (Checkpoint ep {cp['epoch']})\", real_samples=X_ring, latent_limits=ring_latent_limits, data_limits=ring_data_limits)\n\n    def make_slider():\n        if not hasattr(H, \"snapshots\") or len(H.snapshots) == 0:\n            print(\"No checkpoints available. Rerun training with checkpoint_interval&gt;0 to populate snapshots.\")\n            return\n        max_idx = len(H.snapshots) - 1\n        slider = IntSlider(value=0, min=0, max=max_idx, step=1, description='Checkpoint')\n        def on_change(change):\n            if change['name'] == 'value':\n                show_checkpoint(change['new'])\n        slider.observe(on_change)\n        display(VBox([Label('Checkpoint browser'), slider, out]))\n        # show initial\n        show_checkpoint(0)\n\n    make_slider()",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GAN Training Pitfalls</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/GAN_pitfalls.html#experiment-1-when-the-discriminator-overpowers-the-generator",
    "href": "part2/gen_models/GAN_pitfalls.html#experiment-1-when-the-discriminator-overpowers-the-generator",
    "title": "9  GAN Training Pitfalls",
    "section": "",
    "text": "The Discriminator is a much larger (or higher capacity) network than the Generator, and thus is able to move its classification decision boundary in more complex ways to distinguish real from fake samples, and the Generator doesn’t have sufficient capacity to compensate for this.\nThe Discriminator trains much faster than the Generator, and thus can move its decision boundary faster than the Generator can keep up.\nThe Discriminator is initialized (either on purpose or by random chance) such that it has an initial significant advantage over the Generator. In this case, even if the Generator has high capacity and can train quickly, it may not be able to recover from this initial disadvantage, as the discriminator make lock it into a portion of the sample space from which local gradient descent on the Generator cannot recover.\n\n\n\n\n\n\n\n\nTipExperiment: What happens when the Discriminator overpowers the Generator?\n\n\n\nBelow you can change the relative learning rates of the Discriminator versus the Generator:\n\nWhat happens when the Discriminator is able to train significantly faster than the Generator? What about the other way around?\nHow can we tell from the various loss plots that the Discriminator is overpowering the Generator? How does this behavior manifest itself in the generated samples?\nWhy is it that the generator is unable to recover from this situation, even though it has the capacity to represent the data distribution? What about the training procedure and dynamics of the minimax game prevents the Generator from catching up?\nIn the latent interpolation plots, as the GAN spans its latent set of coordinates, what happens to the generated samples?",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GAN Training Pitfalls</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/GAN_pitfalls.html#experiment-2-when-the-generator-overpowers-the-discriminator",
    "href": "part2/gen_models/GAN_pitfalls.html#experiment-2-when-the-generator-overpowers-the-discriminator",
    "title": "9  GAN Training Pitfalls",
    "section": "9.2 Experiment 2: When the Generator Overpowers the Discriminator",
    "text": "9.2 Experiment 2: When the Generator Overpowers the Discriminator\nAbove we saw what happens when the Discriminator overpowers the Generator. But what about the other way around? The next experiment will help illuminate what can go wrong.\n\n\n\n\n\n\nTipExperiment: What happens when the Generator overpowers the Discriminator?\n\n\n\nBelow you can change the relative learning rates of the Discriminator versus the Generator to give the Generator a significant advantage:\n\nWhat happens when we turn the tables on the Discriminator, and give the Generator comparatively more power?\nHow are the loss plots this time significantly different than what you saw in the first experiment?\nAs you look through the training curves as well as snapshots during training, what do you notice about the generated samples? Why would an underpowered Discriminator lead to this behavior? (Hint: Play out conceptually in your head how the generator would respond in cases where the Discriminator is very weak or slow.)\n\n\n\n\nG, D, H = train_vanilla_gan(\n    X_ring, epochs=120, batch_size=256,\n    lr_g=1e-4,\n    lr_d=1e-5,\n    hidden_dim=256, noise_dim=2, \n    checkpoint_interval=10,\n    print_every=40)\n\nEpoch 001/120 | D 1.347 | G 0.704 | Div 0.030\nEpoch 040/120 | D 1.346 | G 0.751 | Div 0.015\nEpoch 040/120 | D 1.346 | G 0.751 | Div 0.015\nEpoch 080/120 | D 1.368 | G 0.716 | Div 0.650\nEpoch 080/120 | D 1.368 | G 0.716 | Div 0.650\nEpoch 120/120 | D 1.370 | G 0.731 | Div 0.343\nEpoch 120/120 | D 1.370 | G 0.731 | Div 0.343\n\n\n\n\nShow Code\nplot_model_diagnostics(H, X_ring, G, noise_dim=2, title_suffix=' (GAN w/ G &gt;&gt; D)')\nplot_latent_interpolation(G, noise_dim=2, title_suffix=' (GAN w/ G &gt;&gt; D)')\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nYou can also view this interactively at different training snapshots below, which might help illuminate some behavior:\n\n\nShow Code\n# Interactive checkpoint browser (requires ipywidgets)\nif interact is None:\n    print(\"ipywidgets not available. Install ipywidgets to use the interactive checkpoint browser.\")\nelse:\n    from ipywidgets import IntSlider, Output, VBox, Label\n\n    out = Output()\n\n    def show_checkpoint(idx: int):\n        if not hasattr(H, \"snapshots\") or len(H.snapshots) == 0:\n            with out:\n                print(\"No checkpoints available. Rerun training with checkpoint_interval&gt;0 to populate snapshots.\")\n            return\n        cp = H.snapshots[idx]\n        # Load generator state into a temporary generator instance\n        G_temp = build_generator(noise_dim=2, hidden_dim=256)\n        # load state dict (ensure tensors are moved to device)\n        state = {k: v.to(device) for k, v in cp[\"state_dict\"].items()}\n        G_temp.load_state_dict(state)\n\n        with out:\n            out.clear_output(wait=True)\n            # Plot diagnostics and latent interpolation for this checkpoint\n            print(f\"Checkpoint: index={idx} | epoch={cp['epoch']}\")\n            plot_model_diagnostics(H, X_ring, G_temp, noise_dim=2, title_suffix=f\" (Checkpoint ep {cp['epoch']})\")\n            plot_latent_interpolation(G_temp, noise_dim=2, title_suffix=f\" (Checkpoint ep {cp['epoch']})\", real_samples=X_ring, latent_limits=ring_latent_limits, data_limits=ring_data_limits)\n\n    def make_slider():\n        if not hasattr(H, \"snapshots\") or len(H.snapshots) == 0:\n            print(\"No checkpoints available. Rerun training with checkpoint_interval&gt;0 to populate snapshots.\")\n            return\n        max_idx = len(H.snapshots) - 1\n        slider = IntSlider(value=0, min=0, max=max_idx, step=1, description='Checkpoint')\n        def on_change(change):\n            if change['name'] == 'value':\n                show_checkpoint(change['new'])\n        slider.observe(on_change)\n        display(VBox([Label('Checkpoint browser'), slider, out]))\n        # show initial\n        show_checkpoint(0)\n\n    make_slider()",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GAN Training Pitfalls</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/GAN_pitfalls.html#experiment-3-effect-of-discriminator-spectral-normalization",
    "href": "part2/gen_models/GAN_pitfalls.html#experiment-3-effect-of-discriminator-spectral-normalization",
    "title": "9  GAN Training Pitfalls",
    "section": "9.3 Experiment 3: Effect of Discriminator Spectral Normalization",
    "text": "9.3 Experiment 3: Effect of Discriminator Spectral Normalization\nPart of the issue in Experiment 1 (where the Discriminator overpowers the Generator) is that the Discriminator can make very sudden and extreme changes to its decision boundary, which the Generator cannot keep up with. One way to mitigate this is to use Spectral Normalization on the Discriminator weights, which constrains the Lipschitz constant of the Discriminator function, and thus prevents it from making extreme changes to its decision boundary.\n\n\n\n\n\n\nTipExperiment: How does Spectral Normalization affect the Discriminator?\n\n\n\nBelow we will activate spectral_normalization=True in the Discriminator, and see how this affects the training dynamics when the Discriminator is given a significant learning rate advantage over the Generator.\n\nKeeping the learning rates from Experiment 1, what do you now notice about the Discriminator’s training losses?\nDo different learning rates modulate this effect? You can repeat a version of Experiment 2 here if you are curious.\nIn what ways has applying this spectral normalization improved the model performance? In what ways has it made it worse? Why do you think this is? (Hint: Think about what Spectral Normalization controls within a network, and what effect that would have if applied to the Discriminator in a mini-max game against the Generator.)\nLooking only at the various training curves, do you notice the effect of the Spectral normalization? Where do the effect of the Spectral Normalization show itself within the diagnostics?\n\n\n\n\n\nShow Code\nG, D, H = train_vanilla_gan(\n    X_ring, epochs=240, batch_size=256,\n    lr_g=1e-4,\n    lr_d=5e-4,\n    spectral_normalization=True,\n    hidden_dim=256, noise_dim=2, \n    print_every=40)\n\n\nEpoch 001/240 | D 1.299 | G 0.731 | Div 0.143\nEpoch 040/240 | D 1.371 | G 0.703 | Div 4.572\nEpoch 040/240 | D 1.371 | G 0.703 | Div 4.572\nEpoch 080/240 | D 1.374 | G 0.700 | Div 4.610\nEpoch 080/240 | D 1.374 | G 0.700 | Div 4.610\nEpoch 120/240 | D 1.376 | G 0.700 | Div 4.528\nEpoch 160/240 | D 1.378 | G 0.697 | Div 4.569\nEpoch 200/240 | D 1.381 | G 0.697 | Div 4.568\nEpoch 240/240 | D 1.383 | G 0.699 | Div 4.630\n\n\n\n\nShow Code\nplot_model_diagnostics(H, X_ring, G, noise_dim=2, title_suffix=' (GAN w/ D_spec_norm &gt;&gt; G)')\nplot_latent_interpolation(G, noise_dim=2, title_suffix=' (GAN w/ D_spec_norm &gt;&gt; G)')\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GAN Training Pitfalls</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/GAN_pitfalls.html#summary-and-next-steps",
    "href": "part2/gen_models/GAN_pitfalls.html#summary-and-next-steps",
    "title": "9  GAN Training Pitfalls",
    "section": "9.4 Summary and Next Steps",
    "text": "9.4 Summary and Next Steps\nWe have seen above how the mini-max formulation of a Generative Adversarial Network leads to unwanted training dynamics that need to be carefully tuned if the GAN is to perform well. In large part, we can see that this is the result of the two networks fighting each other. But is this fight really necessary? After all, the Generator is trying to learn the data distribution, and the Discriminator is just a tool to help it do so, because we didn’t have a clear way to map the generated samples to the real samples in a way that directly allowed us to directly optimize something like an MSE.\nIn the next notebook, we will see how we can reformulate this problem in terms of Optimal Transport, which will allow us to sidestep some of these issues and remove some of the problems cased by the mini-max game.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GAN Training Pitfalls</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/OT.html",
    "href": "part2/gen_models/OT.html",
    "title": "10  Optimal Transport for Generative Models",
    "section": "",
    "text": "10.1 What is Optimal Transport?\nIn the previous notebooks, we explored how Generative Adversarial Networks (GANs) use a minimax game between a Generator and Discriminator to learn data distributions. However, we saw that this adversarial training can lead to instability, mode collapse, and other training difficulties. In this notebook, we’ll explore an alternative approach based on Optimal Transport (OT), which provides a more direct way to measure the distance between probability distributions.\nThe key insight is this: instead of training a discriminator to distinguish real from fake samples, we can directly minimize the distance between the generated distribution and the real data distribution using optimal transport metrics. This often leads to more stable training and better coverage of the data distribution.\nLet’s go back to our earlier analogy where we were imagining probability distributions as two piles of sand, and our goal is to transform one pile into the other. Optimal transport addresses the question: What is the most efficient way to move the sand from one configuration to another?\nMore formally, given two probability distributions \\(z\\) and \\(x\\), optimal transport finds a transport plan \\(\\pi\\) that moves mass from \\(z\\) to \\(x\\) while minimizing the total transport cost. There are many possible definitions of cost here, and it is common to think of a distance as a form of cost, with the Wasserstein distance (also called the Earth Mover’s Distance) as a common one with the technical form: \\[\nW_p(z, x) = \\left( \\inf_{\\pi \\in \\Pi(z, x)} \\int \\|x - y\\|^p \\, d\\pi(x, y) \\right)^{1/p}\n\\]\nwhere \\(\\Pi(z, x)\\) is the set of all joint distributions with marginals \\(z\\) and \\(x\\).\nOK, so far so good in principle – I just need to find the transport plan that minimizes some p-norm over all joint distributions of \\(z\\) and \\(x\\). However, in practice, this is not so straightforward, since finding a minimum over all possible joint distributions of \\(z\\) and \\(x\\) is not so computationally tractable.1\nInstead, of computing the Wasserstein distance directly, we will compute an approximate version of it that regularizes the transport plan, and is called computing the Sinkhorn Divergence. Going over the specific implementation details of the Sinkhorn Divergence (which rely on Sinkhorn Iteration and knowledge of doubly stochastic matrices) are beyond the scope of what I want to cover in these notebooks, but interested students can check out Computational Optimal Transport by Gabriel Peyré and Marco Cuturi for further details.\nThe important thing to know in the context of a course at this level is that the Sinkhorn Divergence can only approximate the true Wasserstein distance, and that it does so via what is often called a “blur” parameter. This parameter is essentially a smoothing term that determines how much we penalize the complexity of the transport map. Some small amount of blur will help us compute gradients and use the Sinkhorn Divergence in ML model training, but too much of this will prevent a model from capturing fine details in the data distribution. You will see an interactive example of this next before we move on to using OT for GAN training.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Optimal Transport for Generative Models</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/OT.html#what-is-optimal-transport",
    "href": "part2/gen_models/OT.html#what-is-optimal-transport",
    "title": "10  Optimal Transport for Generative Models",
    "section": "",
    "text": "1 At a high level, this is due to a combinatoric assignment problem in something called the coupling matrix, where you are trying to match generated and real datapoints to one another and optimize for the lowest distance. Because this assignment matrix is ultimately a binary matrix, this makes it not easily differentiable.\n\n\n10.1.1 Simple Optimal Transport Example\nLet’s start with a familiar and concrete example that we have been using in the prior GAN notebooks. We’ll take a simple 2D Gaussian and compute its optimal transport to our ring of Gaussians dataset. We’ll visualize the transport map by computing the transport vectors (i.e., in what direction we move the probability mass) and also demonstrate how moving in those directions shifts our 2D Gaussian towards the ring.\n\n\nShow Code\n# Load the ring dataset\nX_ring, y_ring = create_ring_gaussians(n_samples=2000)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nsc = ax.scatter(X_ring[:, 0], X_ring[:, 1], c=y_ring, cmap='tab10', s=15, alpha=0.6)\nplt.colorbar(sc, label='Mode index')\nax.set_title('Target: Ring of Gaussians')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.axis('equal')\nax.grid(True, alpha=0.2)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNow what we will do below is place a simple 2D Gaussian distribution centered at the origin, and then compute the Sinkhorn Divergence (loss) between each point in the real dataset and each point in the simple 2D Gaussian. Using Automatic Differentiation, we can then compute how to move each point in the generated distribution to minimize this loss. We will plot a sample of these gradient vectors so that you can see what the transport map looks like and also take a (very large) step in that direction for each point, so you can see the visual effect of the transport.\nIn this case, we are only taking a single, giant step along the transport map for pedagogical purposes, but in reality (and as we will do later), you would move slowly along the transport map over many iterations to gradually morph the generated distribution into the real data distribution.\n\n# Create a simple 2D Gaussian source distribution\nn_source = 2000\nsource_samples = np.random.randn(n_source, 2).astype(np.float32) * 0.5\n\n# Convert to torch tensors\nsource_torch = torch.from_numpy(source_samples).to(device)\ntarget_torch = torch.from_numpy(X_ring).to(device)\n\n# Compute optimal transport using Sinkhorn algorithm\n# The blur parameter controls entropic regularization (larger = more regularization)\nsinkhorn_loss = SamplesLoss(\"sinkhorn\", p=2, blur=0.01, scaling=0.9)\n\n# Compute transport plan by taking gradient\nsource_torch.requires_grad_(True)\nloss = sinkhorn_loss(source_torch, target_torch)\nloss.backward()\n\n\n# This is effectively a giant step size so the we can visualize the gradients\n# and see a meaningful change in the distribution.\n# In reality, we would take much smaller steps than this and do it over iterations\nmagnitude_scaling = 2000\n\n# The gradient points in the direction of optimal transport\ntransport_direction = source_torch.grad.detach().cpu().numpy()\ntransported_points = source_samples + transport_direction*magnitude_scaling\n\n\n\nShow Code\n# Visualize the transport\nfig, axes = plt.subplots(3, 1, figsize=(10,16))\n\n# Source distribution\naxes[0].scatter(source_samples[:, 0], source_samples[:, 1], \n                s=15, alpha=0.5, c='tab:blue')\naxes[0].scatter(X_ring[:, 0], X_ring[:, 1], \n                s=8, alpha=0.2, c='lightgray', label='Target')\naxes[0].set_title('Source: 2D Gaussian')\naxes[0].set_xlabel('$x_1$')\naxes[0].set_ylabel('$x_2$')\naxes[0].axis('equal')\naxes[0].grid(True, alpha=0.2)\n\n# Transport vectors\n# Sample a subset for visualization clarity\nidx_subset = np.random.choice(n_source, size=25, replace=False)\naxes[1].scatter(X_ring[:, 0], X_ring[:, 1], \n                s=8, alpha=0.2, c='lightgray', label='Target')\naxes[1].quiver(source_samples[idx_subset, 0], source_samples[idx_subset, 1],\n                magnitude_scaling/1.5*transport_direction[idx_subset, 0], \n                magnitude_scaling/1.5*transport_direction[idx_subset, 1],\n                angles='xy', scale_units='xy', scale=0.5, width=0.005, \n                color='tab:orange', alpha=0.5)\naxes[1].set_title('Optimal Transport Vectors')\naxes[1].set_xlabel('$x_1$')\naxes[1].set_ylabel('$x_2$')\naxes[1].axis('equal')\naxes[1].grid(True, alpha=0.2)\naxes[1].legend()\n\n# Transported distribution\naxes[2].scatter(X_ring[:, 0], X_ring[:, 1], \n                s=8, alpha=0.2, c='lightgray', label='Target')\naxes[2].scatter(transported_points[:, 0], transported_points[:, 1], \n                s=15, alpha=0.5, c='tab:red', label='Transported')\naxes[2].set_title('After Transport')\naxes[2].set_xlabel('$x_1$')\naxes[2].set_ylabel('$x_2$')\naxes[2].axis('equal')\naxes[2].grid(True, alpha=0.2)\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Sinkhorn divergence: {loss.item():.4f}\")\n\n\n\n\n\n\n\n\n\nSinkhorn divergence: 2.7932\n\n\nIn the visualization above, you can see how optimal transport naturally moves mass from the source Gaussian to the target ring distribution. The transport vectors (middle plot) show the direction and magnitude of how each point should move to minimize the total transport cost.\n\n\n\n\n\n\nTipExperiment: Effect of Sinkhorn Divergence Parameters\n\n\n\nThe Sinkhorn divergence has several key parameters that affect the transport:\n\nblur (\\(\\epsilon\\)): Controls the amount of entropic regularization. Larger values make the transport “smoother” but less accurate.\np: The p-norm used for measuring distances. This is typically p=1 for Manhattan distance (sum of absolute differences) or p=2 for Euclidean distance (standard L2 norm)).\n\nUse the slider below to gain intuition about changing the effects of these three parameters: - What effect does moving from a small blur to a large blur have? - How does the transport pattern differ between p=1 and p=2?\n\n\n\n\nShow Code\nif geomloss_available and widgets_available:\n    def explore_pnorm(p: int = 2, blur: float = 0.05):\n        # Create source distribution\n        source_samples = np.random.randn(1000, 2).astype(np.float32) * 0.5\n        source_torch = torch.from_numpy(source_samples).to(device)\n        target_torch = torch.from_numpy(X_ring[:1000]).to(device)\n        \n        # Compute transport\n        sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=p, blur=blur, scaling=0.9)\n        source_torch.requires_grad_(True)\n        loss = sinkhorn_loss(source_torch, target_torch)\n        loss.backward()\n        \n        magnitude_scaling = 1500\n        \n        transport_direction = source_torch.grad.detach().cpu().numpy()\n        transported_points = source_samples + transport_direction * magnitude_scaling\n        \n        # Visualize\n        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n        \n        # Transport vectors\n        idx_subset = np.random.choice(1000, size=150, replace=False)\n        axes[0].scatter(X_ring[:, 0], X_ring[:, 1], \n                       s=8, alpha=0.15, c='lightgray', label='Target')\n        axes[0].quiver(source_samples[idx_subset, 0], source_samples[idx_subset, 1],\n                      magnitude_scaling/1.5*transport_direction[idx_subset, 0], \n                      magnitude_scaling/1.5*transport_direction[idx_subset, 1],\n                      angles='xy', scale_units='xy', scale=1, width=0.003, \n                      color='tab:orange', alpha=0.7)\n        axes[0].set_title(f'Transport Vectors (p={p}, blur={blur:.3f})')\n        axes[0].set_xlabel('$x_1$')\n        axes[0].set_ylabel('$x_2$')\n        axes[0].axis('equal')\n        axes[0].grid(True, alpha=0.2)\n        axes[0].legend()\n        \n        # Result\n        axes[1].scatter(X_ring[:, 0], X_ring[:, 1], \n                       s=8, alpha=0.2, c='lightgray', label='Target')\n        axes[1].scatter(transported_points[:, 0], transported_points[:, 1], \n                       s=15, alpha=0.5, c='tab:red', label='Transported')\n        axes[1].set_title(f'After Transport (loss={loss.item():.4f})')\n        axes[1].set_xlabel('$x_1$')\n        axes[1].set_ylabel('$x_2$')\n        axes[1].axis('equal')\n        axes[1].grid(True, alpha=0.2)\n        axes[1].legend()\n        \n        plt.tight_layout()\n        plt.show()\n    \n    interact(explore_pnorm, \n             p=IntSlider(min=1, max=2, step=1, value=2, description='p-norm'),\n             blur=FloatSlider(min=0.01, max=0.7, step=0.01, value=0.05, description='Blur (ε)'))\nelif not geomloss_available:\n    print(\"GeomLoss not available. Please install it to run this experiment.\")\nelse:\n    print(\"ipywidgets not available. Please install it for interactive controls.\")",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Optimal Transport for Generative Models</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/OT.html#building-an-entropic-ot-gan",
    "href": "part2/gen_models/OT.html#building-an-entropic-ot-gan",
    "title": "10  Optimal Transport for Generative Models",
    "section": "10.2 Building an Entropic OT GAN",
    "text": "10.2 Building an Entropic OT GAN\nNow that we are beginning to understand optimal transport and how it might be useful, let’s try to use it to train a generative model. Instead of using a discriminator (as in standard GANs), we’ll directly minimize the Sinkhorn divergence between generated samples and real data. We can do this because the Sinkhorn divergence is now substituting for the original role of the Discriminator (i.e., to move the generator closer to real-world data), and so the Discriminator is no longer necessary.2\n2 One could argue that we are no longer really doing a “GAN” here because we do not have an “Adversary” now that the Discriminator is gone, so it is perhaps misleading to call it this, but since earlier papers refer to this style of Generative Model as an Entropic GAN, I will be consistent with them, even if it isn’t the best name in my view.This approach has several advantages:\n\nNo discriminator needed: We only need the network for the Generator now, which is simpler and we fewer total parameters to train. We don’t need to worry about two learning rates or different capacities in each network, like we saw in the GAN Pitfalls notebook.\nMore stable to train: Since we no longer have a minimax game to balance, we do not have to contend with oscillatory behavior of the optimizer and potentially getting trapped in a loop. This can also equate to faster training times as a result, if the learning rate is suitably tuned.\nBetter coverage: OT naturally encourages covering all modes of the data distribution, rather than hoping that a Discriminator pushes the Generator to cover all modes.\nMeaningful gradients: Because OT computes a pairwise distance among all data points, so long as the gradient of our distance/cost measure remains finite and non-zero at far away distances, we can still perform useful gradient descent steps in the generative model. This is not the case for some other cost functions (such as the KL Divergence) where gradients might vanish if the two distributions are not close enough.\n\n\nclass GeneratorMLP(nn.Module):\n    \"\"\"Simple MLP generator for 2D data.\"\"\"\n    def __init__(self, noise_dim: int = 2, hidden_dim: int = 256, out_dim: int = 2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(noise_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, out_dim)\n        )\n    \n    def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n        return self.net(z)\n\n\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass SinkhornHistory:\n    \"\"\"Training history for Sinkhorn-based generator.\"\"\"\n    loss: List[float]\n    diversity: List[float]\n\ndef train_sinkhorn_generator(\n    data: np.ndarray,\n    noise_dim: int = 2,\n    batch_size: int = 256,\n    epochs: int = 200,\n    lr: float = 1e-3,\n    hidden_dim: int = 256,\n    blur: float = 0.05,\n    p: int = 2,\n    print_every: int = 50\n) -&gt; tuple:\n    \"\"\"\n    Train a generator using Sinkhorn divergence.\n    \n    Args:\n        data: Real data samples (numpy array)\n        noise_dim: Dimension of latent noise\n        batch_size: Batch size for training\n        epochs: Number of training epochs\n        lr: Learning rate\n        hidden_dim: Hidden dimension for generator\n        blur: Entropic regularization parameter\n        p: p-norm for distance metric\n        print_every: Print progress every N epochs\n    \n    Returns:\n        Tuple of (trained generator, training history)\n    \"\"\"\n    if not geomloss_available:\n        raise ImportError(\"GeomLoss is required for Sinkhorn training. Install with: pip install geomloss\")\n    \n    # Setup data loader\n    loader = make_loader(data, batch_size)\n    \n    # Initialize generator and optimizer\n    G = GeneratorMLP(noise_dim=noise_dim, hidden_dim=hidden_dim, out_dim=2).to(device)\n    optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n    \n    # Setup Sinkhorn loss\n    sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=p, blur=blur, scaling=0.9, debias=True)\n    \n    # Training history\n    history = SinkhornHistory(loss=[], diversity=[])\n    \n    print(f\"Training Sinkhorn OT Generator for {epochs} epochs...\")\n    print(f\"Parameters: blur={blur}, p={p}, lr={lr}, hidden_dim={hidden_dim}\")\n    \n    for epoch in range(epochs):\n        epoch_losses = []\n        \n        for (real_batch,) in loader:\n            real_batch = real_batch.to(device)\n            \n            # Generate fake samples\n            z = torch.randn(batch_size, noise_dim, device=device)\n            fake_batch = G(z)\n            \n            # Compute Sinkhorn divergence\n            loss = sinkhorn_loss(fake_batch, real_batch)\n            \n            # Update generator\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            epoch_losses.append(float(loss.detach().cpu().item()))\n        \n        # Record metrics\n        mean_loss = float(np.mean(epoch_losses))\n        \n        # Compute diversity\n        with torch.no_grad():\n            z_eval = torch.randn(2048, noise_dim, device=device)\n            samples = G(z_eval)\n            diversity = compute_diversity_metric(samples)\n        \n        history.loss.append(mean_loss)\n        history.diversity.append(diversity)\n        \n        if (epoch + 1) % print_every == 0 or epoch == 0:\n            print(f\"Epoch {epoch+1:03d}/{epochs} | Loss: {mean_loss:.4f} | Diversity: {diversity:.4f}\")\n    return G, history\n\nLet’s train the OT generator on our ring dataset:\n\n\nShow Code\nif geomloss_available:\n    # Train the generator\n    G_ot, H_ot = train_sinkhorn_generator(\n        X_ring,\n        epochs=200,\n        batch_size=256,\n        noise_dim=2,\n        hidden_dim=256,\n        blur=0.02,\n        p=2,\n        lr=1e-3,\n        print_every=50\n    )\n    \n    # Generate and visualize samples\n    with torch.no_grad():\n        z_test = torch.randn(2000, 2, device=device)\n        generated_samples = G_ot(z_test).cpu().numpy()\n    \n    fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n    ax.scatter(X_ring[:, 0], X_ring[:, 1], \n              s=10, alpha=0.2, c='lightgray', label='Real')\n    ax.scatter(generated_samples[:, 0], generated_samples[:, 1], \n              s=12, alpha=0.5, c='tab:orange', label='Generated (OT)')\n    ax.set_xlabel('$x_1$')\n    ax.set_ylabel('$x_2$')\n    ax.set_title('Entropic OT GAN: Real vs Generated Samples')\n    ax.legend()\n    ax.axis('equal')\n    ax.grid(True, alpha=0.2)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"GeomLoss not available. Please install it to train the OT generator.\")\n\n\nTraining Sinkhorn OT Generator for 200 epochs...\nParameters: blur=0.02, p=2, lr=0.001, hidden_dim=256\nEpoch 001/200 | Loss: 2.8801 | Diversity: 1.4708\nEpoch 001/200 | Loss: 2.8801 | Diversity: 1.4708\nEpoch 050/200 | Loss: 0.1830 | Diversity: 4.3724\nEpoch 050/200 | Loss: 0.1830 | Diversity: 4.3724\nEpoch 100/200 | Loss: 0.1967 | Diversity: 4.4298\nEpoch 100/200 | Loss: 0.1967 | Diversity: 4.4298\nEpoch 150/200 | Loss: 0.1828 | Diversity: 4.2631\nEpoch 150/200 | Loss: 0.1828 | Diversity: 4.2631\nEpoch 200/200 | Loss: 0.1656 | Diversity: 4.4816\nEpoch 200/200 | Loss: 0.1656 | Diversity: 4.4816\n\n\n\n\n\n\n\n\n\nLet’s use our standard diagnostic plots from the GAN notebooks to evaluate the OT generator’s performance:\n\n\nShow Code\nplot_model_diagnostics(\n    H_ot, \n    X_ring, \n    G_ot, \n    noise_dim=2, \n    title_suffix=' (Sinkhorn OT)'\n)\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nplot_latent_interpolation(\n    G_ot, \n    noise_dim=2, \n    title_suffix=' (Sinkhorn OT)',\n    real_samples=X_ring\n)\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Optimal Transport for Generative Models</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/OT.html#summary-and-next-steps",
    "href": "part2/gen_models/OT.html#summary-and-next-steps",
    "title": "10  Optimal Transport for Generative Models",
    "section": "10.3 Summary and Next Steps",
    "text": "10.3 Summary and Next Steps\nIn this notebook, we explored how Optimal Transport provides an alternative to adversarial training for generative models:\n\nOptimal Transport Intuition: OT finds the most efficient way to transform one distribution into another, providing meaningful gradients even when distributions don’t overlap.\nSinkhorn Divergence: By adding entropic regularization, we make OT computationally tractable while preserving most benefits. The blur parameter controls the trade-off between precision and smoothness.\nEntropic OT GANs: We can train generative models by directly minimizing Sinkhorn divergence, eliminating the need for a discriminator and the associated minimax game.\nAdvantages over Traditional GANs:\n\nSimpler architecture (no discriminator)\nMore stable training (no adversarial dynamics)\nBetter mode coverage (OT naturally spreads mass)\nMeaningful loss values (directly related to distribution distance)\n\nTrade-offs: OT-based training requires careful tuning of the blur parameter and can be computationally expensive for very large datasets. In particular, OT-based distances are very good when the source and target distributions are already well aligned, but less so when the transport distance is high (since then the optimal map computed by Sinkhorn Iteration may not be as discriminative). As a consequence, many real-world applications of OT in a generative model context use OT as a fine-tuning step after doing an alignment or registration step (e.g., when matching a mesh of an organ to data from a CT scan).\n\nSo, at this point we have addressed one of the key weaknesses of GANs – the instability of adversarial training – by removing the discriminator entirely and replacing it with a direct measure of distribution distance. However, there is still one big missing piece to be desired by Entropic GAN-style models. While we have a better forward map now of \\(f(z) \\rightarrow x\\) via the generator, we still do not have an inverse map \\(f^{-1}(x) \\rightarrow z\\) that would allow us to encode real data points back into the latent space. This is something that Variational Autoencoders (VAEs) and Normalizing Flows provide, and is where we turn our attention next. We will see that the general concept of OT will raise its head again after Normalizing Flows in the context of something called “Flow Matching”, and we will return to this in a few notebooks.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Optimal Transport for Generative Models</span>"
    ]
  },
  {
    "objectID": "problems/problems.html",
    "href": "problems/problems.html",
    "title": "Problems",
    "section": "",
    "text": "Here are the problem sets",
    "crumbs": [
      "Problems"
    ]
  },
  {
    "objectID": "problems/ps1.html",
    "href": "problems/ps1.html",
    "title": "11  Problem Set 1",
    "section": "",
    "text": "11.1 PS1 Part 1: Linear Models and Validation",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Problem Set 1</span>"
    ]
  },
  {
    "objectID": "problems/ps1.html#ps1-part-1-linear-models-and-validation",
    "href": "problems/ps1.html#ps1-part-1-linear-models-and-validation",
    "title": "11  Problem Set 1",
    "section": "",
    "text": "11.1.1 Preamble\nWe’ll be loading some CO2 concentration data that is a commonly used dataset for model building of time series prediction. You will build a few baseline linear models and assess them using some of the tools we discussed in class. Which model is best? Let’s find out.\nFirst let’s just load the data and take a look at it:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_openml\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\nsns.set_context('notebook')\n\n# Fetch the data\nmauna_lao = fetch_openml('mauna-loa-atmospheric-co2', as_frame = False)\nprint(mauna_lao.DESCR)\ndata = mauna_lao.data\n# Assemble the day/time from the data columns so we can plot it\nd1958 = datetime(year=1958,month=1,day=1)\ntime = [datetime(int(d[0]),int(d[1]),int(d[2])) for d in data]\nX = np.array([1958+(t-d1958)/timedelta(days=365.2425) for t in time]).T\nX = X.reshape(-1,1)  # Make it a column to make scikit happy\ny = np.array(mauna_lao.target)\n\n**Weekly carbon-dioxide concentration averages derived from continuous air samples for the Mauna Loa Observatory, Hawaii, U.S.A.**&lt;br&gt;&lt;br&gt;\nThese weekly averages are ultimately based on measurements of 4 air samples per hour taken atop intake lines on several towers during steady periods of CO2 concentration of not less than 6 hours per day; if no such periods are available on a given day, then no data are used for that day. The _Weight_ column gives the number of days used in each weekly average. _Flag_ codes are explained in the NDP writeup, available electronically from the [home page](http://cdiac.ess-dive.lbl.gov/ftp/trends/co2/sio-keel-flask/maunaloa_c.dat) of this data set. CO2 concentrations are in terms of the 1999 calibration scale (Keeling et al., 2002) available electronically from the references in the NDP writeup which can be accessed from the home page of this data set.\n&lt;br&gt;&lt;br&gt;\n### Feature Descriptions\n_co2_: average co2 concentration in ppvm &lt;br&gt;\n_year_: year of concentration measurement &lt;br&gt;\n_month_: month of concentration measurement &lt;br&gt;\n_day_: day of month of concentration measurement &lt;br&gt;\n_weight_: number of days used in each weekly average &lt;br&gt;\n_flag_: flag code &lt;br&gt;\n_station_: station code &lt;br&gt;\n&lt;br&gt;\n**Author**: Carbon Dioxide Research Group, Scripps Institution of Oceanography, University of California-San Diego, La Jolla, California, USA 92023-0444 &lt;br&gt;\n**Source**: [original](http://cdiac.ess-dive.lbl.gov/ftp/trends/co2/sio-keel-flask/maunaloa_c.dat) - September 2004\n\nDownloaded from openml.org.\n\n\n\n# Plot the data\nplt.figure(figsize=(10,5))    # Initialize empty figure\nplt.scatter(X, y, c='k',s=1) # Scatterplot of data\nplt.xlabel(\"Year\")\nplt.ylabel(r\"CO$_2$ in ppm\")\nplt.title(r\"Atmospheric CO$_2$ concentration at Mauna Loa\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ny[:100]\n\narray([316.1, 317.3, 317.6, 317.5, 316.4, 316.9, 317.5, 317.9, 315.8,\n       315.8, 315.4, 315.5, 315.6, 315.1, 315. , 314.1, 313.5, 313. ,\n       313.2, 313.5, 314. , 314.5, 314.4, 314.7, 315.2, 315.2, 315.5,\n       315.6, 315.8, 315.4, 316.9, 316.6, 316.6, 316.8, 316.7, 316.7,\n       317.7, 317.1, 317.6, 318.3, 318.2, 318.7, 318. , 318.4, 318.5,\n       318.1, 317.8, 317.7, 316.8, 316.8, 316.4, 316.1, 315.6, 314.9,\n       315. , 314.1, 314.4, 313.9, 313.5, 313.5, 313. , 313.1, 313.4,\n       313.4, 314.1, 314.4, 314.8, 315.2, 315.1, 315. , 315.6, 315.8,\n       315.7, 315.7, 316.4, 316.7, 316.5, 316.6, 316.6, 316.9, 317.4,\n       317. , 316.9, 317.7, 318. , 317.7, 318.6, 319.3, 319. , 319. ,\n       319.7, 319.9, 319.8, 320. , 320. , 319.4, 320. , 319.4, 319. ,\n       318.1])\n\n\n\n\n11.1.2 Linear Models\nConstruct the following linear models: 1. Model 1: “Vanilla” Linear Regression, that is, where \\(CO_2 = a+b \\cdot time\\) 2. Model 2: Quadratic Regression, where \\(CO_2 = a+b \\cdot t + c\\cdot t^2\\) 3. Model 3: A more complex “linear” model with the following additive terms \\(CO_2=a+b\\cdot t+c\\cdot sin(\\omega\\cdot t)\\): * a linear (in time) term * a sinusoidal additive term with period such that the peak-to-peak of the sinsusoid is roughly ~1 year and phase shift of zero (set \\(\\omega\\) as appropriate to match the peaks) 4. Model 4: A “linear” model with the following additive terms (\\(CO_2=a+b\\cdot t+c\\cdot t^2+d\\cdot sin(\\omega\\cdot t)\\): * a quadratic (in time) polynomial * a sinusoidal additive term with period such that the peak-to-peak of the sinsusoid is roughly ~1 year and phase shift of zero (set \\(\\omega\\) as appropriate to match the peaks)\nEvauate these models using the appropriate kind of Cross Validation for each of the following amounts of Training data: 1. N=50 Training Data Points 2. N=100 3. N=200 4. N=500 5. N=1000 6. N=2000\nQuestion: Before you even construct the models or do any coding below, what is your initial guess or intuition behind how each of those four models will perform? Note: there is no right or wrong answer to this part of the assignment and this question will only be graded on completeness, not accuracy. It’s intent is to get you to think about and write down your preliminary intuition regarding what you think will happen before you actually implement anything, based on your approximate understanding of how functions of the above complexity should perform as N increases.\nStudent Response: [Insert your response here]\nQuestion: What is the appropriate kind of Cross Validation to perform in this case if we want a correct Out of Sample estimate of our Test MSE?\nStudent Response: [Insert your response here]\nNow, for each of the above models and training data sizes: * Plot the predicted CO2 as a function of time, including the actual data, for each of the N=X training data examples. This should correspond to six plots (one for each amount of training data) if you plot all models on the same plot, or 6x4 = 24 plots if you plot each model and training data plot separately. * Create a Learning Curve plot for the model which plots its Training and Test MSE as a function of training data. That is, plot how Training and Testing MSE change as you increase the training data for each model. This could be a single plot for all four models (8 lines on the plot) or four different plots corresponding to the learning curve of each model separately.\n\nimport numpy as np\n\nX_train_100 = X[:100]\ny_train_100 = y[:100]\nX_test = X[100:200]\nprint(\"Shape of X_train_100: %s\" % str(X_train_100.shape))\nprint(\"Beginning of X_train_100: %s\" % str(X_train_100[0:5]))\nprint(\"Shape of y_train_100: %s\" % str(y_train_100.shape))\nprint(\"Beginning of y_train_100: %s\" % str(y_train_100[0:5]))\n\nprint('Shape of X_test: %s' % str(X_test.shape))\nprint(\"Beginning of X_test: %s\" % str(X_test[0:5]))\n\n### Modify the below code. You can leave the code above as is. ###\n\n\nShape of X_train_100: (100, 1)\nBeginning of X_train_100: [[1958.23819791]\n [1958.25736326]\n [1958.27652861]\n [1958.29569396]\n [1958.31485931]]\nShape of y_train_100: (100,)\nBeginning of y_train_100: [316.1 317.3 317.6 317.5 316.4]\nShape of X_test: (100, 1)\nBeginning of X_test: [[1960.51887445]\n [1960.5380398 ]\n [1960.55720514]\n [1960.57637049]\n [1960.59553584]]\n\n\n\n# Insert Modeling Building or Plotting code here\n# Note, you may implement these however you see fit\n# Ex: using an existing library, solving the Normal Eqns\n#     implementing your own SGD solver for them. Your Choice.\nfrom sklearn.linear_model import SGDRegressor, LinearRegression\nsgd = SGDRegressor()\nlr = LinearRegression()\n\n\nsgd.fit(X,y)\nlr.fit(X,y)\n\nLinearRegression()\n\n\n\nsgd.predict(X)\n\narray([-1.78959753e+15, -1.78961505e+15, -1.78963256e+15, ...,\n       -1.82954889e+15, -1.82956640e+15, -1.82958392e+15])\n\n\n\nlr.predict(X)\n\narray([310.2080183 , 310.23375578, 310.25949326, ..., 368.9152125 ,\n       368.94094999, 368.96668747])\n\n\nQuestion: Which Model appears to perform best in the N=50 or N=100 Condition? Why is this?\nStudent Response: [Insert your response here]\nQuestion: Which Model appears to perform best as the N=200 to 500? Why is this?\nStudent Response: [Insert your response here]\nQuestion: Which Model appears to perform best as N = 2000? Why is this?\nStudent Response: [Insert your response here]",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Problem Set 1</span>"
    ]
  },
  {
    "objectID": "problems/ps1.html#ps1-part-2-unsupervised-linear-models",
    "href": "problems/ps1.html#ps1-part-2-unsupervised-linear-models",
    "title": "11  Problem Set 1",
    "section": "11.2 PS1 Part 2: Unsupervised Linear Models",
    "text": "11.2 PS1 Part 2: Unsupervised Linear Models\n\n11.2.1 Toy Dataset\nFor this problem, you will use the data file hb.csv. The input is 2,280 data points, each of which is 7 dimensional (i.e., input csv is 2280 rows by 7 columns). Use Principal Component Analysis (either an existing library, or through your own implementation by taking the SVD of the Covariance Matrix) for the follow tasks.\n\n%matplotlib inline\nimport pandas\nurl = \"https://raw.githubusercontent.com/IDEALLab/ML4ME_Textbook/main/problems/hb.csv\"\ndata = pandas.read_csv(url,header=None)\n#data.head()\n\n\n\n11.2.2 Task 1\nAssuming that the 7-dimensional space is excessive, you would like to reduce the dimension of the space. However, what dimensionality of space should we reduce it to? To determine this we need to compute its intrinsic dimensionality. Plot the relative value of the information content of each of the principal components and compare them.\nNote: this information content is called the “explained variance” of each component, but you can also get this from the magnitude of the singular values. This plot is sometimes called a “Scree Plot”.\n\n# Code Here\n\nQuestion: Approximately how many components dominate the space?, and what does this tell us about the intrinsic dimensionality of the space?\nResponse:\n\n11.2.2.1 Task 2\nNow use PCA to project the 7-dimensional points on the K-dimensional space (where K is your answer from above) and plot the points. (For K=1,2, or 3, use a 1, 2, or 3D plot, respectively. For 4+ dimensions, use a grid of pairwise 2D Plots, like the Scatter Matrix we used in class).\n\n# Code Here\n\nQuestion: What do you notice?\nResponse:\n\n\n\n11.2.3 Topology Optimization Dataset\nFor this problem, you will be using unsupervised linear models to help understand and interpret the results of a mechanical optimization problem. Specifically, to understand the solution space generated by a topology optimization code; that is, the results of finding the optimal geometries for minimizing the compliance of various bridge structures with different loading conditions. The input consists of 1,000 images of optimized material distribution for a beam as described in Figure 1. A symmetrical boundary condition, left side, is used to reduce the analysis to only half. Also, a rolling support is included at the lower right corner. Notice that the rolling support is the only support in the vertical direction.\n \n\n\n\n\nFigure 1: Left: Nx-by-Ny design domain for topology optimization problem. Right: Example loading configuration and resulting optimal topology. Two external forces, Fi, were applied to the beam at random nodes represented by (xi, yi) coordinates.1\n\n \nUse Principal Component Analysis (either an existing library, or through your own implementation by taking the SVD of the Covariance Matrix) for the follow tasks.\n1. This problems data is based on the problem setup seen in the following paper: Ulu, E., Zhang, R., & Kara, L. B. (2016). A data-driven investigation and estimation of optimal topologies under variable loading configurations. Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization, 4(2), 61-72.\n\n# To help you get started, the below code will load the images from the associated image folder:\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom PIL import Image\nimport requests, zipfile, io\n\n#im_dir = './topo_opt_runs/'\nurl = \"https://raw.githubusercontent.com/IDEALLab/ML4ME_Textbook/main/problems/topo_opt_runs.zip\"\nresp = requests.get(url)\nresp.raise_for_status()\nzf = zipfile.ZipFile(io.BytesIO(resp.content))\n\nimages = []\nfor name in sorted(zf.namelist()):\n    with zf.open(name) as f:\n        img = Image.open(f).convert('L')\n        images.append(np.asarray(img))\n\nheight,width = images[0].shape\nprint('The images are {:d} pixels high and {:d} pixels wide'.format(height,width))\n\n# Print matrix corresponding to the image:\nprint(images[-1])\n# And show example image, so you can see how matrix correponds:\nimg\n\nThe images are 217 pixels high and 434 pixels wide\n[[  0   0   0 ... 255 255 255]\n [  0   0   0 ... 255 255 255]\n [  0   0   0 ... 255 255 255]\n ...\n [  0   0   0 ...   0   0   0]\n [  0   0   0 ...   0   0   0]\n [  0   0   0 ...   0   0   0]]\n\n\n\n\n\n\n\n\n\n\n\n11.2.4 Task 1: Scree/Singular Value Plot\nAs with the toy example, assume that the 94,178-dimensional space is excessive. You would like to reduce the dimension of the image space. First compute its intrinsic dimensionality. For this application, “good enough” means capturing 95% of the variance in the dataset. How many dimensions are needed to capture at least 95% of the variance in the provided dataset? Store your answer in numDimsNeeded. (Hint: A Scree plot may be helpful, though visual inspection of a graph may not be precise enough.)\nQuestion: Approximately how many components dominate the space? What does this tell us about the intrinsic dimensionality of the space?\nResponse:\n\n\n11.2.5 Task 2: Principal Components\nNow plot the first 5 principal components. Hint: looking at each of these top 5 principal components; do they make sense physically, in terms of what it means for where material in the bridge is placed? Compare, for example, the differences between the 1st and 2nd principal component?",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Problem Set 1</span>"
    ]
  },
  {
    "objectID": "problems/ps1.html#ps1-part-3-bi-linear-models-and-sgd",
    "href": "problems/ps1.html#ps1-part-3-bi-linear-models-and-sgd",
    "title": "11  Problem Set 1",
    "section": "11.3 PS1 Part 3: Bi-Linear Models and SGD",
    "text": "11.3 PS1 Part 3: Bi-Linear Models and SGD\n\n11.3.1 Bilinear Models for Recommendation\nFor this problem, you will derive a very simple recommendation system that uses a combination of unsupervised and supervised approachs and demonstrates use of Stochastic Gradient Descent.\nSpecifically, in class we discussed recommender models of the form: \\[\nf(user,movie) = \\langle v_u,v_m \\rangle + b_u + b_m + \\mu\n\\]\nwhere \\(v\\) is a vector that represents a user’s or movie’s location in an N-Dimensional space, \\(b\\) is a vector that represents a specific “bias” term fo r each movie and user, and \\(\\mu\\) is a scalar that represents a kind of global anchor or base score (i.e., a sort of average movie rating). This means that each user has two vectors (e.g., \\(v_{\\mathrm{jack~smith}}\\) and \\(b_{\\mathrm{jack~smith}}\\)), and each movie has two vectors (e.g., \\(v_{\\mathrm{Avengers}}\\) and \\(b_{\\mathrm{Avengers}}\\)), with each of those vectors being N-Dimensional (in class we used two dimensions). For this, we constructed a loss function as follows: \\[\nCost = Loss + Penalty\n\\] where \\[\nLoss = \\Sigma_{(u,m)\\in \\mathrm{Ratings}} \\frac{1}{2}\\left( \\langle v_u,v_m \\rangle + b_u + b_m + \\mu - y_{u,m}\\right)^2\n\\] and \\[\nPenalty = \\frac{\\lambda}{2}\\left(\\Sigma_u \\left[\\| v_u\\|^2_2 + b_u^2\\right] + \\Sigma_m \\left[\\|v_m\\|^2_2 + b_m^2\\right]\\right)\n\\]\n\n\n11.3.2 Task 1: Analytical Gradients\nTo use stochastic gradient descent, we first need to write down the gradients. Using the above cost function (including both the loss and penalty), compute the following partial derivatives:\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial v_u } =\n\\]\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial v_m } =\n\\]\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial b_u} =\n\\]\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial b_m} =\n\\]\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial \\mu} =\n\\]\nYou can either do this directly in the notebook using LaTeX notation, or via a scanned image. Please remember to show your work in how you computed the derivatives, not just the final result. Note: Recall that the partial derivative of e.g. Nicholas’s rating on Titanic with respect to the user Mark would be zero. When computing your SGD updates, consider how this might impact individual terms for users and movies in the loss function.\n\n\n11.3.3 Task 2: Stochastic Gradient Descent\nNow you are actually going to implement SGD on this type of model and optimize it until convergence on a toy dataset. To simplify the implementation, we’ll actually make the model a little simpler than the one you derived updates for in task 1. Specifically, we’ll just use:\n\\[\nCost = \\Sigma_{(u,m)\\in \\mathrm{Ratings}} \\frac{1}{2}\\left( \\langle v_u,v_m \\rangle + \\mu - y_{u,m}\\right)^2 + \\frac{\\lambda}{2}\\left(\\| v_u\\|^2_2 + \\|v_m\\|^2_2\\right)\n\\]\nThis way all you have to estimate is two vectors — \\(v_u\\) for each user and \\(v_m\\) for each movie — and \\(\\mu\\) — a scalar value similar to an average rating. For simplicity, we’ll assume here that the size of the latent space (K) is 2 (i.e., the length of each \\(v_u\\) & \\(v_m\\)).\nUsing your above gradients, write down the update equations for each vector using stochastic gradient descent. Once you have done this, implement those update equations in code like we did in the in-class notebook. For simplicity, you can just use a constant step size \\(\\alpha\\) if you wish, though you may change this if you want. Note: depending on exactly how you implement your model and what batch size you use, i.e., one point at a time, or some subset of data points, values of \\(\\alpha\\) anywhere between around 0.7 and 0.01 should be sufficient to converge the model in under 1000 epochs, i.e., passes through the dataset. If you implement more advanced tricks covered in some optional readings this can converge much faster, but that is not necessary for this assignment, and it does not matter to me how quickly your model coverges, so long as it does so.\nUse the below small sample dataset of movie ratings for five users and six movies to perform stochastic gradient descent to update those vectors until your model converges. To initialize your SGD, you can use the initial weights/terms we provide below, or you can initialize the model any other way you wish – the exact initialization should not make a big difference here.\n\n# Your Code below!\n\n\nimport numpy as np\nimport pandas as pd\nmissing_ratings = pd.read_csv('missing.csv')\nratings = pd.read_csv('ratings.csv')\nratings\n\n\n\n\n\n\n\n\nmovie\nuser\nratings\n\n\n\n\n0\nThe Avengers\nAlex\n3.0\n\n\n1\nThe Avengers\nPriya\n3.5\n\n\n2\nThe Avengers\nYichen\n3.5\n\n\n3\nWhen Harry Met Sally\nAlex\n3.0\n\n\n4\nWhen Harry Met Sally\nSally\n4.5\n\n\n5\nWhen Harry Met Sally\nPriya\n3.0\n\n\n6\nWhen Harry Met Sally\nYichen\n3.0\n\n\n7\nSilence of the Lambs\nAlex\n3.0\n\n\n8\nSilence of the Lambs\nSally\n4.0\n\n\n9\nSilence of the Lambs\nJuan\n3.5\n\n\n10\nSilence of the Lambs\nPriya\n3.0\n\n\n11\nSilence of the Lambs\nYichen\n2.5\n\n\n12\nShawshank Redemption\nJuan\n2.5\n\n\n13\nShawshank Redemption\nPriya\n4.0\n\n\n14\nShawshank Redemption\nYichen\n4.0\n\n\n15\nThe Hangover\nAlex\n3.0\n\n\n16\nThe Hangover\nSally\n3.5\n\n\n17\nThe Hangover\nPriya\n3.0\n\n\n18\nThe Hangover\nYichen\n2.5\n\n\n19\nThe Godfather\nAlex\n3.0\n\n\n20\nThe Godfather\nPriya\n3.5\n\n\n\n\n\n\n\n\n# Alternatively, if you prefer, you can convert it into numpy first:\nratings_numpy = ratings.to_numpy()\nratings_numpy\n\narray([['The Avengers', 'Alex', 3.0],\n       ['The Avengers', 'Priya', 3.5],\n       ['The Avengers', 'Yichen', 3.5],\n       ['When Harry Met Sally', 'Alex', 3.0],\n       ['When Harry Met Sally', 'Sally', 4.5],\n       ['When Harry Met Sally', 'Priya', 3.0],\n       ['When Harry Met Sally', 'Yichen', 3.0],\n       ['Silence of the Lambs', 'Alex', 3.0],\n       ['Silence of the Lambs', 'Sally', 4.0],\n       ['Silence of the Lambs', 'Juan', 3.5],\n       ['Silence of the Lambs', 'Priya', 3.0],\n       ['Silence of the Lambs', 'Yichen', 2.5],\n       ['Shawshank Redemption', 'Juan', 2.5],\n       ['Shawshank Redemption', 'Priya', 4.0],\n       ['Shawshank Redemption', 'Yichen', 4.0],\n       ['The Hangover', 'Alex', 3.0],\n       ['The Hangover', 'Sally', 3.5],\n       ['The Hangover', 'Priya', 3.0],\n       ['The Hangover', 'Yichen', 2.5],\n       ['The Godfather', 'Alex', 3.0],\n       ['The Godfather', 'Priya', 3.5]], dtype=object)\n\n\nLet’s initialize the vectors to some random numbers, and \\(\\mu\\) to 2.5\n\nK=2\nuser_names = ratings['user'].unique()\nmovie_names = ratings['movie'].unique()\nmu= 2.5\n# Setting the seed of the random generator to a value so that everyone sees the same initialization\n# should should be able to comment out the below with no ill-effects on whatever model you implement\n# this may just help us in office hours if folks have difficulty implementing things\nnp.random.seed(0)\nV = pd.DataFrame(np.random.random((len(user_names)+len(movie_names),K)),index=np.hstack([user_names,movie_names]))\nprint(V)\n\n                             0         1\nAlex                  0.548814  0.715189\nPriya                 0.602763  0.544883\nYichen                0.423655  0.645894\nSally                 0.437587  0.891773\nJuan                  0.963663  0.383442\nThe Avengers          0.791725  0.528895\nWhen Harry Met Sally  0.568045  0.925597\nSilence of the Lambs  0.071036  0.087129\nShawshank Redemption  0.020218  0.832620\nThe Hangover          0.778157  0.870012\nThe Godfather         0.978618  0.799159\n\n\n\n# Here is one example of how to go through rows of a ratings matrix\nfor index, rating in ratings.iterrows():\n    user  = rating['user']\n    movie = rating['movie']\n    score = rating['ratings']\n    print(f\"{user} gave {movie} a score of {score}\")\n\nAlex gave The Avengers a score of 3.0\nPriya gave The Avengers a score of 3.5\nYichen gave The Avengers a score of 3.5\nAlex gave When Harry Met Sally a score of 3.0\nSally gave When Harry Met Sally a score of 4.5\nPriya gave When Harry Met Sally a score of 3.0\nYichen gave When Harry Met Sally a score of 3.0\nAlex gave Silence of the Lambs a score of 3.0\nSally gave Silence of the Lambs a score of 4.0\nJuan gave Silence of the Lambs a score of 3.5\nPriya gave Silence of the Lambs a score of 3.0\nYichen gave Silence of the Lambs a score of 2.5\nJuan gave Shawshank Redemption a score of 2.5\nPriya gave Shawshank Redemption a score of 4.0\nYichen gave Shawshank Redemption a score of 4.0\nAlex gave The Hangover a score of 3.0\nSally gave The Hangover a score of 3.5\nPriya gave The Hangover a score of 3.0\nYichen gave The Hangover a score of 2.5\nAlex gave The Godfather a score of 3.0\nPriya gave The Godfather a score of 3.5\n\n\n\n# Here is an example of one way to access rows of V\nfor index, rating in ratings.iterrows():\n    user  = rating['user']\n    movie = rating['movie']\n    print(f\"{user}'s location in V is {V.loc[user].to_numpy()}.\")\n    print(f\"{movie}'s location in V is {V.loc[movie].to_numpy()}.\")\n    print()\n\n# You could also do it in Numpy directly, which will likely lead to much faster SGD updates,\n# but that shouldn't be necessary for problems of this size. Up to you!\n\nAlex's location in V is [0.5488135  0.71518937].\nThe Avengers's location in V is [0.79172504 0.52889492].\n\nPriya's location in V is [0.60276338 0.54488318].\nThe Avengers's location in V is [0.79172504 0.52889492].\n\nYichen's location in V is [0.4236548  0.64589411].\nThe Avengers's location in V is [0.79172504 0.52889492].\n\nAlex's location in V is [0.5488135  0.71518937].\nWhen Harry Met Sally's location in V is [0.56804456 0.92559664].\n\nSally's location in V is [0.43758721 0.891773  ].\nWhen Harry Met Sally's location in V is [0.56804456 0.92559664].\n\nPriya's location in V is [0.60276338 0.54488318].\nWhen Harry Met Sally's location in V is [0.56804456 0.92559664].\n\nYichen's location in V is [0.4236548  0.64589411].\nWhen Harry Met Sally's location in V is [0.56804456 0.92559664].\n\nAlex's location in V is [0.5488135  0.71518937].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nSally's location in V is [0.43758721 0.891773  ].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nJuan's location in V is [0.96366276 0.38344152].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nPriya's location in V is [0.60276338 0.54488318].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nYichen's location in V is [0.4236548  0.64589411].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nJuan's location in V is [0.96366276 0.38344152].\nShawshank Redemption's location in V is [0.0202184  0.83261985].\n\nPriya's location in V is [0.60276338 0.54488318].\nShawshank Redemption's location in V is [0.0202184  0.83261985].\n\nYichen's location in V is [0.4236548  0.64589411].\nShawshank Redemption's location in V is [0.0202184  0.83261985].\n\nAlex's location in V is [0.5488135  0.71518937].\nThe Hangover's location in V is [0.77815675 0.87001215].\n\nSally's location in V is [0.43758721 0.891773  ].\nThe Hangover's location in V is [0.77815675 0.87001215].\n\nPriya's location in V is [0.60276338 0.54488318].\nThe Hangover's location in V is [0.77815675 0.87001215].\n\nYichen's location in V is [0.4236548  0.64589411].\nThe Hangover's location in V is [0.77815675 0.87001215].\n\nAlex's location in V is [0.5488135  0.71518937].\nThe Godfather's location in V is [0.97861834 0.79915856].\n\nPriya's location in V is [0.60276338 0.54488318].\nThe Godfather's location in V is [0.97861834 0.79915856].\n\n\n\n\n\n11.3.4 Train your Bilinear Model using SGD\n\n# Your Model building and training code here!\n\n\n\n11.3.5 Assessing your accuracy\nLet’s predict the ratings for the missing entries using our (randomly initialized) model.\n\nfor index, rating in missing_ratings.iterrows():\n    user  = rating['user']\n    movie = rating['movie']\n    prediction = np.dot(V.loc[user],V.loc[movie])+mu\n    print(f\"Prediction: {user} will rate {movie}: {prediction:.2f}\")\n\nPrediction: Sally will rate The Avengers: 3.32\nPrediction: Juan will rate The Avengers: 3.47\nPrediction: Juan will rate When Harry Met Sally: 3.40\nPrediction: Alex will rate Shawshank Redemption: 3.11\nPrediction: Sally will rate Shawshank Redemption: 3.25\nPrediction: Juan will rate The Hangover: 3.58\nPrediction: Sally will rate The Godfather: 3.64\nPrediction: Juan will rate The Godfather: 3.75\nPrediction: Yichen will rate The Godfather: 3.43",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Problem Set 1</span>"
    ]
  },
  {
    "objectID": "notebooks/notebooks.html",
    "href": "notebooks/notebooks.html",
    "title": "In-Class Notebooks",
    "section": "",
    "text": "This is a list of some of the in-class notebooks I have created for my course, elements of which are already part of the book. I am listing the individual notebooks here to have a direct link to download individual notebooks, since we often use these in class and it is helpful to have them in a centralized place with download links:\n\nReviewing Data Visualization: California Housing Dataset\nIntroduction to Linear Regression and Cross-Validation\nIntroduction to Gradient Descent\nLoss Functions for Linear Models",
    "crumbs": [
      "In-Class Notebooks"
    ]
  },
  {
    "objectID": "notebooks/california_housing_visualization.html",
    "href": "notebooks/california_housing_visualization.html",
    "title": "12  Housing Price Data Visualization In-Class Exercise",
    "section": "",
    "text": "12.1 Exercise 1: Visualize the relationship between features and price\nIn this review notebook, we will review the basics of data visualization and the importance of this for being able to build performant ML models. It is also a good starting point for people to get used to Python and the use of Jupyter Notebooks. It is designed as an in-class (or on your own) exercise to get your feet wet in working with data. Specifically, in this notebook we will:\nThe below code is a useful starting point that should get you started with exploring the California Housing data.\nSome people might find it useful to use a Pandas dataframe to manipulate the data, but that is not really required for basic plotting and visualization:\nFor the below tasks, you might want to try out some basic Python plotting libraries. I recommend:\nIf you aren’t familiar with any of the above libraries, I would suggest starting with Seaborn, since it hides many of the complex features you might not need right away (check out their tutorial). Bokeh also has a nice Quick Start guide if you like having the ability to pan/zoom the data.\nVisualize the 2D relationship between housing prices and the provided features of the data. You can choose how you want to do this.\n[Enter your code into the empty cell below to create the necessary visualizations. You can create multiple cells for code or Markdown code if that helps you.]\nfeatures = ['MedInc','HouseAge','AveRooms','AveBedrms','Population','AveOccup']\nfor i,feature in enumerate(features):\n    plt.figure()\n    sns.jointplot(x=df[feature],y=df['price'],alpha=0.1)\n    plt.show()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\nQuestion: Do any of the features appear linearly correlated with price?",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Housing Price Data Visualization In-Class Exercise</span>"
    ]
  },
  {
    "objectID": "notebooks/california_housing_visualization.html#exercise-2-visualize-the-relationships-between-features",
    "href": "notebooks/california_housing_visualization.html#exercise-2-visualize-the-relationships-between-features",
    "title": "12  Housing Price Data Visualization In-Class Exercise",
    "section": "12.2 Exercise 2: Visualize the relationships between features",
    "text": "12.2 Exercise 2: Visualize the relationships between features\nVisualize the 1D and 2D relationships between the features in the dataset. For example, how are house ages distributed? What is the relationship between house age and the number of bedrooms? Feel free to explore different 1D and 2D options.\n\n\nfor feature in features:\n    plt.figure()\n    sns.displot(df[feature])\n    plt.show()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# Plot the pairwise distributions between each of the features:\nfor i,feature in enumerate(features):\n    for j,feature2 in enumerate(features):\n        if j&gt;i:\n            plt.figure()\n            sns.jointplot(x=df[feature],y=df[feature2],alpha=1.0)\n            plt.show()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nQuestion: Are there any anomalies that look strange in the data, and which visualization helped you identify them (hint: there should be several)?",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Housing Price Data Visualization In-Class Exercise</span>"
    ]
  },
  {
    "objectID": "notebooks/california_housing_visualization.html#exercise-3-visualize-relationships-with-the-anomalies-removed",
    "href": "notebooks/california_housing_visualization.html#exercise-3-visualize-relationships-with-the-anomalies-removed",
    "title": "12  Housing Price Data Visualization In-Class Exercise",
    "section": "12.3 Exercise 3: Visualize relationships with the Anomalies Removed",
    "text": "12.3 Exercise 3: Visualize relationships with the Anomalies Removed\nUsing your knowledge of the anomalies you found above, remove those anomalies using appropriate code below (either by removing the entire data record, or just the specific values that were anomalous, if you prefer to be more surgical) and replot the Task 1 and Task 2 plots you produced above.\n\n# Code goes here for dealing with outliers\n# You can copy/paste some of your plotting code from above, if that is helpful.\n\nQuestion: Does this change your answer to the original Task 1 or Task 2 questions?",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Housing Price Data Visualization In-Class Exercise</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html",
    "href": "appendices/helpful_tooling.html",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "",
    "text": "A.1 TL;DR Checklist\nMachine learning projects are notoriously brittle: minor implementation details can cause major differences in outcomes. Good practices and tools make your implementation reproducible (and thus debuggable), and portable across different hardware environments, such as a teammate’s laptop or a high-performance computing (HPC) system.\nUnlike traditional programming, debugging ML models by simply “running and fixing in a loop” is rarely effective. Instead, a structured set of practices and tools is needed to understand model behavior and reproduce results reliably.\nHere is what we encourage doing, sorted by impact over effort.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#tldr-checklist",
    "href": "appendices/helpful_tooling.html#tldr-checklist",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "",
    "text": "Environment & Dependencies: Use a proper development environment, virtual environments and pin package versions. Document GPU/CUDA.\n\nLinting: Use tools like ruff to help you write clean code from the start.\nRandom Seeds: Seed all libraries (torch, numpy, random) and enable deterministic operations.\n\nLogging & Checkpoints: Log hyperparameters, save models, track experiments (e.g., wandb).\n\nVisualization: Plot data and learning curves.\nStart Small Then Scale: Debug on small datasets first.\n\nVersion Control: Track code with Git; use branches for experiments.\n\nModular Code: Split code into functions/classes and separate files.\n\nTesting & Type Hints: Write pytest tests and use mypy for type checking.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#reproducible-runs",
    "href": "appendices/helpful_tooling.html#reproducible-runs",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "A.2 Reproducible Runs",
    "text": "A.2 Reproducible Runs\nThe first requirement for a robust ML project is to make the codebase deterministic. Because ML is inherently probabilistic, running the same code twice without precautions may yield different results. Determinism is therefore a prerequisite for debugging.\n\nA.2.1 Python Project Setup\nA good practice is to use a virtual environment (e.g., venv) to isolate dependencies. This prevents conflicts between projects (e.g., my linear regression project uses sklearn 1.0.2, and my generative design project uses sklearn 2.5.3) and avoids interfering with the base operating system.\nDependencies should be pinned to exact versions in a file such as pyproject.toml or requirements.txt. This enables others (including future you) to reproduce the same environment.\nNote that not all dependencies are automatically captured in Python configuration files—for instance, the CUDA version used for GPU processing must be documented separately (typically in a README.md).\n\n\nA.2.2 Seeded Runs\nMost ML techniques involve randomness (e.g., parameter initialization, sampling, data shuffling). To ensure reproducibility, it is necessary to set a random seed so that random number generators produce a deterministic sequence.\nIn practice, several libraries must be seeded and it will look similar to:\nimport torch as th\nimport numpy as np\nimport random\n\nmy_seed = 42\n\nth.manual_seed(my_seed)  # PyTorch\nth.backends.cudnn.deterministic = True\ntorch.cuda.benchmark = False\nrng = np.random.default_rng(my_seed)  # NumPy\nrandom.seed(my_seed)  # Python's built-in random\n\n\nA.2.3 Hyperparameters\nHyperparameter values can influence results as strongly as changing the algorithm itself. It is essential to record which hyperparameters were used for each experiment.\nExperiment-tracking platforms automate this process. For example, Weights and Biases (wandb), or Trackio can log hyperparameters, Python version, and hardware details, as well as visualize results such as learning curves. See, for example:\n\nRun overview with hyperparameter “Config.”\nLearning curves and sampled designs\n\n\n\n\n\n\n\nTipCheckpoint\n\n\n\nOnce versions, seeds and hyperparameters are fixed, running the model multiple times should yield identical results across runs (look at the wandb curves). Inconsistent results usually indicate a missing seed or an unpinned dependency. Without determinism, debugging will be much more time-consuming. We strongly advise to pay attention to this.\n\n\n\n\n\n\n\n\nNoteOther Sources of Non-Determinism\n\n\n\n\n\nThis is unlikely to happen within the course but it is still worth mentioning.\nEven if you set seeds and pin library versions, some sources of non-determinism may persist due to the environment:\n\nMultithreading or parallelism: Operations may be executed in different orders on CPU threads.\n\nGPU operations: Certain GPU kernels are non-deterministic by design, even with fixed seeds.\n\nLibrary versions or BLAS/CUDA backends: Different versions of underlying math libraries may produce slightly different results.\n\nTo mitigate these issues:\n\nEnable deterministic operations where possible (e.g., torch.backends.cudnn.deterministic = True for PyTorch).\n\nBe aware that some operations may never be fully deterministic on GPU—document this for reproducibility.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#code-management",
    "href": "appendices/helpful_tooling.html#code-management",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "A.3 Code Management",
    "text": "A.3 Code Management\nML projects should be approached as software engineering projects. Code quality and management are especially critical: poorly organized or fragile code increases the likelihood of errors and makes debugging more difficult. In addition, Python’s permissiveness can hide subtle mistakes. For example, automatic broadcasting of scalars to vectors may not raise an exception when performing operations on vectors or matrices of mismatched sizes, yet it can still produce incorrect results. Such silent errors are often harder to detect than explicit crashes.\n\nA.3.1 Code Organization\nNotebooks are valuable for exploration and prototyping, but they are less suited for building robust and reproducible experiments. Relying on a single notebook or script often leads to unmanageable code as the project grows. By contrast, a modular codebase is easier to test, extend, and maintain. Organizing code into smaller, modular components simplifies both debugging and collaboration.\n\nDivide the project into functions, each with a single, well-defined purpose. A useful rule of thumb is: if you cannot clearly explain what a function does in one sentence, it should probably be split.\n\nUse classes when it is natural to group related data and behavior together.\n\nSplit large projects across multiple files to make navigation easier. Avoid single files with 1000+ lines, as they are hard to read, debug, and extend.\n\n\n\nA.3.2 Version Control\nVersion control ensures that specific states of a project can be identified and restored. We strongly recommend using Git (with GitHub or similar platforms) for ML projects. When working in teams, branches help manage changes and prevent conflicts.\n\n\nA.3.3 Formatting and Linting\nCode formatting conventions (e.g., number of spaces per indentation, placement of comments, naming conventions) do not affect program behavior but improve readability. There are formatters that can automatically fix the visual style of the code—things like indentation, line breaks, spacing around operators, and alignment.\nLinting, goes beyond formatting: linters analyze your code for potential errors or risky patterns, such as unused variables, variables that may be undefined, or suspicious comparisons.\nruff integrates formatting, linting, and error detection in a single tool. It improves code quality, reduces stylistic disagreements, and allows developers to focus on the intent rather than the syntax.\n\n\nA.3.4 Type Hints\nIn addition to formatting and linting, static type checking helps catch errors before running your code. Python is dynamically typed, which means you can easily pass the wrong type of object to a function without immediate errors. Tools like mypy analyze your code using type hints and report mismatches.\nFor example, in:\ndef add_numbers(a: int, b: int) -&gt; int:\n    return a + b\n\nadd_numbers(2, \"3\") # note the String here\nRunning mypy will output:\nmain.py:4: error: Argument 2 to \"add_numbers\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nFound 1 error in 1 file (checked 1 source file)\nUsing type hints (a: int, -&gt; int) together with mypy lets you detect bugs early, improves code readability, and helps IDEs provide better autocompletion and refactoring support.\n\n\nA.3.5 Testing\nTesting individual components—functions, classes, or modules—is an important way to ensure reliability. Well-written tests allow developers to:\n\nIsolate potential error sources: When a bug occurs, thoroughly tested components can be excluded from investigation, saving time.\n\nDetect unintended side effects: Tests help ensure that changes in one part of the codebase do not break other parts.\n\nTesting and good code organization go hand in hand: modular code is naturally easier to test, and writing tests often encourages cleaner, more maintainable designs.\nThe most common tool for this is pytest. For example:\nIf you define a function in your project:\n# my_project/utils.py\ndef add_numbers(a, b):\n    return a + b\nYou can define tests with:\n# tests/test_math.py -- this is your test file\nfrom my_project.utils import add_numbers\n\ndef test_add_numbers():\n    assert add_numbers(2, 3) == 5\n    assert add_numbers(-1, 1) == 0\nRunning pytest will automatically discover these tests and report any failures.\n\n\nA.3.6 Integrated Development Environments (IDEs)\nWhile you can write Python code in any text editor, using an IDE significantly improves productivity. Visual Studio Code (VS Code) is the most popular choice for Python and ML development. It supports:\n\nExtensions: Add functionality and friendly interface for linting ruff, type checking mypy, testing pytest, and git integration.\n\nHandling of virtual environments: VSCode can create and handle virtual environments for you.\nDebugger: Set breakpoints and inspect the current state of variables, run instruction by instruction. This is much easier than putting prints everywhere.\nNotebooks inside VS Code: You can run Jupyter notebooks directly within your IDE.\nLLMs integration: Students have access to GitHub education (and Copilot), VSCode has a direct LLM integration for code completion and agent.\n\n\n\nA.3.7 Large Language Models (LLMs) for Coding\nTools like ChatGPT or GitHub Copilot can generate code quickly. While this can accelerate boilerplate writing, it does not replace understanding.\nMachine learning code is particularly sensitive to details: a small mistake in data preprocessing, tensor dimensions, or random seeding can completely change results. Using LLMs without knowing what the code does may:\n\nHide important assumptions.\n\nLead to silent bugs that are hard to detect.\n\nPrevent you from learning how ML algorithms really work.\n\nGuideline: LLMs are great for generating snippets (e.g., “write a function to convert my CSV data to JSON”), but always read, run, and understand the code before using it in experiments. For ML, correctness and reproducibility are more important than speed.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#debugging",
    "href": "appendices/helpful_tooling.html#debugging",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "A.4 Debugging",
    "text": "A.4 Debugging\nIf the codebase is well-structured and reproducible but issues persist, the problem is likely related to the maths, hyperparameters, or data.\n\nA.4.1 Visualizing\nVisualization is one of the most effective debugging tools in ML, particularly in engineering contexts where results can often be represented graphically.\n\nA.4.1.1 Algorithm-level Visualizations\n\nLoss curves: Simple plots can reveal overfitting, underfitting, or learning failures.\nPredictions: Comparing model outputs with reference data at various training stages provides direct insight into progress.\n\nFor these, we often report metrics and outputs in wandb, see for instance these lines.\n\n\nA.4.1.2 Data-level Visualizations\n\nInspect dataset distributions: Check whether features are on compatible scales, whether rescaling or normalization is needed, and whether outliers are present. Tools like matplotlib or seaborn can help.\n\nAssess assumptions: Determine whether the data distribution aligns with the model’s underlying assumptions, e.g., can the data distribution be captured by a Gaussian distribution.\n\n\n\n\nA.4.2 Split Your Pipeline\nIt is good practice to split your training pipeline into distinct stages:\n\nData analysis: Visualize your data. Look at the distributions, detect outliers, and gain insights into what preprocessing might be needed and which models may perform well.\nData pre-processing: Massage your data before feeding it to the model. Visualize to ensure transformations are correct and consistent.\nTraining: Train your model on the preprocessed data. Save trained models to disk after each run (torch.save, pickle, or similar). This allows you to avoid retraining from scratch every time you tweak evaluation code.\n\nEvaluation: Load the saved model and run your evaluation routines on validation or test datasets.\n\nBy separating these stages, you can debug each part independently, and validate progress.\n\n\nA.4.3 Start Small, Then Scale\nWhen debugging, it is inefficient to run large-scale experiments immediately. Instead:\n\nBegin with small, fast experiments (e.g., a reduced dataset or a lightweight simulator).\n\nValidate that the model can learn on trivial cases.\n\nAttempt to reproduce established results or baseline performance.\n\nScaling to larger, more complex runs should only occur once smaller experiments confirm that the model behaves as expected.\n\n\nA.4.4 Performance Profiling with timeit\nSometimes the bug is actually that the code is too slow. When this happens, the first step is often to measure where the time goes. You can do that with Python’s built-in timeit.\ntimeit runs a snippet of code multiple times and reports the average execution time, helping you compare different implementations or detect bottlenecks.\nHere is an example for normalizing data:\nimport numpy as np\nimport timeit\n\nsetup = \"\"\"\nimport numpy as np\ndata = np.random.rand(10000, 100)  # 10k samples, 100 features\n\"\"\"\n\n# Option 1: Pure Python loops\nstmt1 = \"\"\"\nnormalized = []\nfor row in data:\n    mean = np.mean(row)\n    std = np.std(row)\n    normalized.append((row - mean) / std)\nnormalized = np.array(normalized)\n\"\"\"\n\n# Option 2: NumPy vectorization\nstmt2 = \"\"\"\nmeans = np.mean(data, axis=1, keepdims=True)\nstds = np.std(data, axis=1, keepdims=True)\nnormalized = (data - means) / stds\n\"\"\"\n\nprint(\"Python loops:\", timeit.timeit(stmt1, setup=setup, number=10))\nprint(\"NumPy vectorization:\", timeit.timeit(stmt2, setup=setup, number=10))\nResults:\nPython loops: 0.8857301659882069\nNumPy vectorization: 0.04489224997814745\nUsing NumPy vectorization is ~20x faster than Python loops.\nIn notebooks, you don’t even need imports:\n%timeit sum(range(1000))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#a-practical-example",
    "href": "appendices/helpful_tooling.html#a-practical-example",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "A.5 A Practical Example",
    "text": "A.5 A Practical Example\nThis section shows a practical example using the techniques explained above on an actual code.\n\nA.5.1 Step 0: The Ugly Script\nWe start with some messy code that Ruff would flag:\n# train.py\nimport numpy as np, torch, torch.nn as nn, torch.optim as optim, matplotlib.pyplot as plt, random\nfrom sklearn.model_selection import train_test_split\n\nX_all =np.linspace(0, 100, 500).reshape(-1,1)\ny_all = 5* np.sin(0.1 * X_all)+np.random.randn(500,1)\nX_train,X_test,y_train,y_test =train_test_split(X_all,y_all,test_size=0.2,random_state=42)\n\nmodel =nn.Linear(1,1)\noptimizer =optim.SGD(model.parameters(),lr=0.01)\nloss_fn= nn.MSELoss()\n\nfor epoch in range(500):\n    pred =model(torch.tensor(X_train))\n    loss= loss_fn(pred, torch.tensor(y_train))\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(f\"Epoch {epoch} - Loss: {loss.item()}\")\nAt first glance, it looks okay but it won’t run. Try executing python train.py to see the errors.\n\n\nA.5.2 Step 1: From Ugly to Bad\nRun ruff to format and check. Fix the errors (or call ruff check --fix train.py).\nNow the code is already cleaner and easier to debug. Still, running the file throws errors.\n\n\nA.5.3 Step 2: Debugging\nRunning python train.py gives a cryptic type error at the loss computation: RuntimeError: mat1 and mat2 must have the same dtype, but got Double and Float.\nThe problem is that the model outputs float32 predictions while y is float64. This causes a mismatch in the loss computation. The fix is to change the lines to:\n- pred = model(torch.tensor(X_train))\n- loss = loss_fn(pred, torch.tensor(y_train))\n+ pred = model(torch.tensor(X_train, dtype=torch.float32))\n+ loss = loss_fn(pred, torch.tensor(y_train, dtype=torch.float32))\n\n\nA.5.4 Step 3: Make your Script as Deterministic as possible\nIt is important to remove sources of non determinism when debugging ML models, see seeding.\n# right after the imports\nrng = np.random.default_rng(42)  # seed NumPy random\ntorch.manual_seed(42)  # seed PyTorch\ntorch.cuda.manual_seed(42)  # see PyTorch CUDA (for NVIDIA GPUs)\ntorch.backends.cudnn.deterministic = True  # tell PyTorch to use deterministic kernels\ntorch.backends.cudnn.benchmark = False # removes internal optimizations that can cause non-determinism \nAnd when defining your outputs:\n# replace np.random by the seeded RNG\n- y = 5 * np.sin(0.1 * X_all) + np.random.randn(100, 1)\n+ y = 5 * np.sin(0.1 * X_all) + rng.standard_normal(size=(100, 1))\n\n\nA.5.5 Step 4: Visualizing\nIt is extremely important to visualize your data. For this, we can add this to the script:\n\nA.5.5.1 Visualizing data\nimport matplotlib.pyplot as plt\nand before the training loop:\nplt.scatter(X_train, y_train, label=\"Train\")\nplt.scatter(X_test, y_test, color=\"red\", label=\"Test\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"Data\")\nplt.legend()\nplt.show()\n\n\nA.5.5.2 Visualizing loss\n# before training loop\nlosses = []\n\n# in your training loop\nlosses.append(loss.item())\n\n# after training loop\nplt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss over time\")\nplt.show()\nObservations:\n\nThe target is sinusoidal, a simple linear model cannot capture this (we’ve made bad assumptions for the model).\nThe loss is producing nans and going to inf.\nThe features are not normalized, making learning difficult.\n\n\n\n\nA.5.6 Step 5: Normalizing Features\nX_mean = X_train.mean(axis=0, keepdims=True)\nX_std = X_train.std(axis=0, keepdims=True)\ny_mean = y_train.mean(axis=0, keepdims=True)\ny_std = y_train.std(axis=0, keepdims=True)\nX_train_norm = (X_train - X_mean) / X_std\ny_train_norm = (y_train - y_mean) / y_std\nX_test_norm = (X_test - X_mean) / X_std\ny_test_norm = (y_test - y_mean) / y_std\nor using scikit-learn utils:\nx_scaler = StandardScaler()\ny_scaler = StandardScaler()\nX_train_norm = x_scaler.fit_transform(X_train)\ny_train_norm = y_scaler.fit_transform(y_train)\nX_test_norm = x_scaler.transform(X_test)\ny_test_norm = y_scaler.transform(y_test)\nUse these normalized tensors in the training loop instead of the raw values.\n\n\nA.5.7 Step 6: Visualizing Predictions\nNow the loss seems to go down, the code runs. Let’s look at the predictions. This code will help you visualize the predictions vs. the true values:\n# after the training loop\nwith torch.no_grad():\n    predictions = model(X_tensor)\n\nplt.figure(figsize=(8, 5))\nplt.scatter(X_tensor.numpy(), y_tensor.numpy(), label=\"True data\", color=\"blue\", alpha=0.5)\nplt.scatter(\n    X_tensor.numpy(), predictions.numpy(), label=\"Predictions\", color=\"red\", alpha=0.5\n)\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"True vs Predicted\")\nplt.legend()\nplt.show()\nObservation: It is pretty obvious that our model has not enough capacity to capture the data.\n\n\nA.5.8 Step 7: Adjusting Hyperparameters\nLet’s try to increase the model size.\nmodel = nn.Sequential(\n    nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 16), nn.ReLU(), nn.Linear(16, 1)\n)\nAnd re-run. Now we see the loss is not optimal.\n\nCan you adjust the learning rate and model size?\nWhat about the optimizer?\nAnd how are you going to keep track of what combination of hyperparameter values you have tried?\nAlso, you have several plots (prediction, loss) for each run which are helpful.\n\nFor this, we recommend using experiment trackers, such as Weights and Biases or Trackio.\nFirst, you start by defining your hyperparameters on top the file:\nhyperparameters = {\n    \"learning_rate\": 0.01,\n    \"model_layers\": [16, 16],\n    \"activation\": \"ReLU\",\n}\nand use them in your training script. For instance, your model definition becomes\nmodel_layers: list[int] = hyperparameters[\"model_layers\"]\nlayers = []\n\n# Build all layers including input and hidden layers -- this allows to just change the model_layers in your hyperparameters dictionary.\ncurrent_size = 1\nfor layer_size in model_layers:\n    layers.append(nn.Linear(current_size, layer_size))\n    if hyperparameters[\"activation\"] == \"ReLU\":\n        layers.append(nn.ReLU())\n    elif hyperparameters[\"activation\"] == \"Sigmoid\":\n        layers.append(nn.Sigmoid())\n    current_size = layer_size\n\n# Add output layer\nlayers.append(nn.Linear(current_size, 1))\n\nmodel = nn.Sequential(*layers)\noptimizer = optim.SGD(model.parameters(), lr=hyperparameters[\"learning_rate\"]) # see hyperparameter here\nThen, you log these hyperparameters for each experiment:\n wandb.init(project=\"example\", config=hyperparameters)\nIn your training loop:\nwandb.log({\"loss\": loss.item()})\nYou can even log an image of prediction vs. true data at each training step. See below.\n\n\nA.5.9 Final Code\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport trackio as wandb\nfrom sklearn.model_selection import train_test_split\n\nrng = np.random.default_rng(42)  # seed NumPy random\ntorch.manual_seed(42)  # seed PyTorch\ntorch.cuda.manual_seed(42)  # see PyTorch CUDA (for NVIDIA GPUs)\ntorch.backends.cudnn.deterministic = True  # tell PyTorch to use deterministic kernels\ntorch.backends.cudnn.benchmark = (\n    False  # removes internal optimizations that can cause non-determinism\n)\n\nhyperparameters = {\n    \"learning_rate\": 0.01,\n    \"model_layers\": [16, 16],\n    \"activation\": \"ReLU\",\n}\n\n\ndef visualize_data(\n    model: nn.Module,\n    X_test: np.ndarray,\n    y_test: np.ndarray,\n    epoch: int,\n    open_window: bool = False,\n    log: bool = False,\n):\n    \"\"\"Visualize the data and the predictions.\n\n    Args:\n        model: The model to visualize the predictions of.\n        X_test: The test data.\n        y_test: The test labels.\n        epoch: The epoch number.\n        open_window: Whether to open a window to display the plot.\n        log: Whether to log the plot to wandb.\n    \"\"\"\n    with torch.no_grad():\n        predictions = model(torch.tensor(X_test, dtype=torch.float32))\n    plt.scatter(X_test, y_test, color=\"red\", alpha=0.5, label=\"Data\")\n    plt.scatter(\n        X_test, predictions.numpy(), color=\"blue\", alpha=0.5, label=\"Predictions\"\n    )\n    plt.xlabel(\"X\")\n    plt.ylabel(\"y\")\n    plt.title(f\"Data - Epoch {epoch}\")\n    plt.legend()\n    if log:\n        plt.savefig(\"predictions.png\")\n        wandb.log({\"predictions\": wandb.Image(\"predictions.png\")})\n    if open_window:\n        plt.show()\n    plt.close()\n\n\nif __name__ == \"__main__\":\n    wandb.init(project=\"example\", config=hyperparameters)\n\n    X_all = np.linspace(0, 100, 500).reshape(-1, 1)\n    y_all = 5 * np.sin(0.1 * X_all) + rng.standard_normal(size=(500, 1))\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_all, y_all, test_size=0.2, random_state=42\n    )\n\n    plt.scatter(X_train, y_train, label=\"Train\")\n    plt.scatter(X_test, y_test, color=\"red\", label=\"Test\")\n    plt.xlabel(\"X\")\n    plt.ylabel(\"y\")\n    plt.title(\"Data\")\n    plt.legend()\n    plt.show()\n\n    # Normalize the data\n    X_mean = X_train.mean(axis=0, keepdims=True)\n    X_std = X_train.std(axis=0, keepdims=True)\n    y_mean = y_train.mean(axis=0, keepdims=True)\n    y_std = y_train.std(axis=0, keepdims=True)\n    X_train_norm = (X_train - X_mean) / X_std\n    y_train_norm = (y_train - y_mean) / y_std\n    X_test_norm = (X_test - X_mean) / X_std\n    y_test_norm = (y_test - y_mean) / y_std\n\n    model_layers: list[int] = hyperparameters[\"model_layers\"]\n    layers = []\n\n    # Build all layers including input and hidden layers\n    current_size = 1\n    for layer_size in model_layers:\n        layers.append(nn.Linear(current_size, layer_size))\n        if hyperparameters[\"activation\"] == \"ReLU\":\n            layers.append(nn.ReLU())\n        elif hyperparameters[\"activation\"] == \"Sigmoid\":\n            layers.append(nn.Sigmoid())\n        current_size = layer_size\n\n    # Add output layer\n    layers.append(nn.Linear(current_size, 1))\n\n    model = nn.Sequential(*layers)\n    optimizer = optim.Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n    loss_fn = nn.MSELoss()\n\n    losses = []\n    for epoch in range(500):\n        pred = model(torch.tensor(X_train_norm, dtype=torch.float32))\n        loss = loss_fn(pred, torch.tensor(y_train_norm, dtype=torch.float32))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        print(f\"Epoch {epoch} - Loss: {loss.item()}\")\n        wandb.log({\"loss\": loss.item()})\n        losses.append(loss.item())\n        if epoch % 20 == 0:\n            visualize_data(\n                model,\n                X_test_norm,\n                y_test_norm,\n                epoch=epoch,\n                open_window=False,\n                log=True,\n            )\n\n    plt.plot(losses)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss over time\")\n    plt.show()\n\n    # Show predictions plot at the end of training\n    visualize_data(\n        model, X_test_norm, y_test_norm, epoch=500, open_window=True, log=True\n    )\n    wandb.finish()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html",
    "href": "appendices/course_progression.html",
    "title": "Appendix B — Course Lecture Progression",
    "section": "",
    "text": "B.1 Part 0: Review and Foundations\nThis section describes an example progression through the course material based on my “Machine Learning for Mechanical Engineering” course at ETHZ. It assumes two lecture+exercise sessions twice a week, for a duration of 1 hour and 45 minutes each with a 5-10 minutes break in the middle of each session. This provides a sample of how you might work through the content in this book, and also acts as a reference to the students. My course is structured such that lectures and exercises occur in the same session, often alternating between lecture and in-class demonstrations or exercises, and this is frequently reflected in each of the book chapters. In class, for time reasons, I may skip some of the longer derivations in the book, since these can be effectively studied on their own.\nThese lectures introduce the course and also cover some background information that will be foundational and critical later in the course. For illustrative and notational purposes, we will use Linear Regression as a simple model to understand these foundations first, and this will allow us to build up to more complex models as we go through the course.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html#part-0-review-and-foundations",
    "href": "appendices/course_progression.html#part-0-review-and-foundations",
    "title": "Appendix B — Course Lecture Progression",
    "section": "",
    "text": "B.1.1 Lecture 1: Course Introduction and Review of ML Basics\n\nOverview the course structure and syllabus\nReview several basics that should have been covered in the prior Stochastics and Machine Learning course\n\nVisualizing Data Review using the California Housing Dataset Notebook\nReview of Cross Validation in Evaluating ML Models\nReview of Linear models\n\n(Re-)Introduction to Coding and Tooling basics to help with the rest of the course, such as Editors, Colab, Version Control, Basic Debugging. Read Helpful Tooling.\n\n\n\nB.1.2 Lecture 2: Review of (Stochastic) Gradient Descent\n\nRead Stochastic Gradient Descent\nRead Why Momentum Really Works\nReview of Regularization for Linear Regresson models, including weight decay (L2) and L1/sparsity control, and effects on loss functions.\nReview of Linear Unsupervised Learning (e.g., PCA, Sparse PCA, etc.)\n\n\n\nB.1.3 Lecture 3: Automatic Differentiation and Taking Derivatives\n\nRead Taking Derivatives, except for Advanced topics like Implicit Differentiation and JVP/HVP (this is for a later lecture)\nIn-Class example of Forward and Reverse AD on a simple function\nIn-Class example of AD through a Verlet Integrator\n\n\n\nB.1.4 Lecture 4: Advanced Differentiation\n\nRead the remaining parts of Taking Derivatives\nIn-Class example of Implicit Differentiation\nIn-Class example with JVPs for computing some example properties of Linear Models, for example, the Fisher Information Matrix.\n(Time Permitting) Examples of Projected Gradient or Proximal Gradient methods.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html#part-1-advanced-neural-network-models",
    "href": "appendices/course_progression.html#part-1-advanced-neural-network-models",
    "title": "Appendix B — Course Lecture Progression",
    "section": "B.2 Part 1: Advanced Neural Network Models",
    "text": "B.2 Part 1: Advanced Neural Network Models\n\nB.2.1 Lecture 5: Review of Neural Network Models\n\nRead Review of Neural Networks\nIn-Class example of Visualizing NN Layers using ConvNetJS\nIn-Class examples of Auto-Encoders on Simple Data, Auto-Encoders on Airfoil Data, and Least Volume Auto-Encoders (including Spectral Normalization)\n\n\n\nB.2.2 Lecture 6: Push-Forward Generative Models\n\nRead Push-Forward Generative Models\nIn-Class examples of GANs, VAEs, and Normalizing Flows on Synthetic Data and the Airfoil Problem\n\n\n\nB.2.3 Lecture 7: Interlude – Techniques for Building and Debugging ML Models\n\nRead Helpful Tooling for Working with and Debugging Machine Learning Models\nIn-Class Before and After example implementing the changes\nIn-Class example of improving/correcting an LLM-provided code solution\n\n\n\nB.2.4 Lecture 8: Optimal Transport\n\nRead Measuring Differences Between Distributions\nIn-Class example of an Optimal Transport Generative Model\n\n\n\nB.2.5 Lecture 9: Stochastic Generative Models\n\nRead Stochastic Generative Models\nIn-Class examples of Diffusion Models on Synthetic Data and the Airfoil Problem\nIn-Class examples of Sequence Data generation (e.g., the Tangrams Example)\n\n\n\nB.2.6 Lecture 10: Latent Generative Neural Models\n\nIn-Class example of Latent Diffusion Models via Least Volume AEs.\nIn-Class example of continuous+discrete models\nPractical Demonstrations of complex models on EngiBench\n\n\n\nB.2.7 Lecture 11:\n\nRead Introduction to Transformers\nIn-Class experiment with the Attention Mechanism\nIn-Class demonstrations of Latent Transformer models (e.g., VQGAN)\n\n\n\nB.2.8 Lecture 12: Review of Reinforcement Learning\n\nRead Reinforcement Learning\nIn-Class example of Linear Functional Q-Learning\n\n\n\nB.2.9 Lecture 13: Actor-Critic and Policy Gradient Methods\n\nPolicy Gradients\n\n\n\nB.2.10 Lecture 14: Advanced Policy Methods\n\nDiffusion Policies\nTransformer Policies\nLatent Generative Policies (e.g., Dreamer and variants)\n\n\n\nB.2.11 Lecture 15: Message Passing, Graph Neural Networks, and other Structured Data\n\nRead Graph Neural Networks\nLearning with Meshes and Point Clouds\nLearning with Signed Distance Fields",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html#part-2-probabilistic-models-and-kernels",
    "href": "appendices/course_progression.html#part-2-probabilistic-models-and-kernels",
    "title": "Appendix B — Course Lecture Progression",
    "section": "B.3 Part 2: Probabilistic Models and Kernels",
    "text": "B.3 Part 2: Probabilistic Models and Kernels\n\nB.3.1 Lecture 16: Review of Probabilistic Models\n\nRead ?sec-reviewprobability of Probabilistic Models, since this reviews the information from the prior course. We will revisit the later sections in a later lecture.\nIn-Class exercise deriving the MLE for various common distributions\n\n\n\nB.3.2 Lecture 17: Introduction to Probablistic Programming\n\nBayesian Linear Regression in a Probabilistic Programming Language\nIntroduction to Approximate Inference Methods, such as MCMC and Variational Inference\n\n\n\nB.3.3 Lecture 18: Large-Scale Bayesian Models and Debugging Probabilistic Inference\n\nStochastic Variational Inference and Stein Variational Gradient Descent\nPitfalls of Probabilistic Models and Debugging Inference\n\n\n\nB.3.4 Lecture 19: Kernel Basics\n\nRepresenter Theorem\nKernelized Ridge Regression\nRegularization of Kernels and effects of Fourier Spectra\n\n\n\nB.3.5 Lecture 20: Gaussian Processes and Bayesian Optimization\n\nBasics of Gaussian Processes\nBayesian Optimization\nEffect of Dimension on GPs and related variants (AdditiveGPs, etc.)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html#part-3-engineering-relevant-machine-learning-topics",
    "href": "appendices/course_progression.html#part-3-engineering-relevant-machine-learning-topics",
    "title": "Appendix B — Course Lecture Progression",
    "section": "B.4 Part 3: Engineering-relevant Machine Learning Topics",
    "text": "B.4 Part 3: Engineering-relevant Machine Learning Topics\n\nB.4.1 Lecture 21: Active Learning and Semi-Supervised Learning\n\nNotebook on Active Learning and Semi-Supervised Learning\n\n\n\nB.4.2 Lecture 22: Transfer Learning and Foundation Models\n\nFine-Tuning a Pre-trained model\nJointly embedded models\n\n\n\nB.4.3 Lecture 23: Integrating Everything Together\n\nIn-Class exercise integrating all prior topics using EngiBench/EngiOpt\n\n\n\nB.4.4 Lecture 24: Ethical and Legal Implications of ML within Engineering\n\nAdversarial Attacks and Defenses\nPrivacy-Preserving and Federated ML\nInterpretability Methods\nCase Study: National Algorithms Safety Board",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/review_of_singular_value_decomposition.html",
    "href": "appendices/review_of_singular_value_decomposition.html",
    "title": "Appendix C — Review of Matrices and the Singular Value Decomposition",
    "section": "",
    "text": "For more info after class on fundamental matrix properties, see the Matrix Cookbook\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom ipywidgets import interact\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nsns.set_palette(\"Set1\", 8, .75)\nsns.set_color_codes()\n\n# Below are just to tell NumPy to print things nicely\nnp.set_printoptions(precision=3)\nnp.set_printoptions(suppress=True)\nnp.set_printoptions(threshold=5)\n\nLet’s generate a simple circle of points, so that we can see what Matrices do to objects:\n\ndef circle_points(a=1,b=1):\n    ''' Generates points on a ellipsoid on axes length a,b'''\n    t = np.linspace(0,2*np.pi,25)   # Define a line\n    X = np.matrix([a*np.cos(t),b*np.sin(t)]) # Create circle using polar coords\n    return X\n\ndef plot_circle(X,title=None):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(X[0].flat, X[1].flat,c='g')\n    if(title):\n        plt.title(title)\n    plt.axis('equal')\n    plt.show()\n\n\nX = circle_points()\nprint('X:', X.shape)\nprint(X)\nplot_circle(X,'A Unit Circle')\n\nX: (2, 25)\n[[ 1.     0.966  0.866 ...  0.866  0.966  1.   ]\n [ 0.     0.259  0.5   ... -0.5   -0.259 -0.   ]]\n\n\n\n\n\n\n\n\n\n\nM = np.matrix([[2, 0],[0, 1]])\nprint('M'); print(M)\n\nM\n[[2 0]\n [0 1]]\n\n\n\ndef plot_transformed_circle(X,NewX, title=None):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plt.scatter(X[0].flat, X[1].flat, c='g')\n    plt.scatter(NewX[0].flat, NewX[1].flat, c='b')\n    plt.plot(X[0].flat, X[1].flat, color='g')\n    plt.plot(NewX[0].flat, NewX[1].flat, color='b')\n    if(title):\n        plt.title(title)\n    plt.axis('equal')\n    plt.show() \n    \nplot_transformed_circle(X,    # First show the original circle\n                        M*X)  # Then show the transformed circle\n\n\n\n\n\n\n\n\n\nM = np.matrix([[2, 0],[0, 2]])\nprint('M'); print(M)\nplot_transformed_circle(X,M*X)\n\nM\n[[2 0]\n [0 2]]\n\n\n\n\n\n\n\n\n\n\nM = np.matrix([[1,2],[0, 1]])\nprint('M'); print(M)\nplot_transformed_circle(X,M*X)\n\nM\n[[1 2]\n [0 1]]\n\n\n\n\n\n\n\n\n\n\nnp.random.seed(100); np.set_printoptions(precision=1)\n# Now just create a random 2x2 matrix\nR = np.matrix(np.random.rand(2,2))\nprint(R)\n\n# Then transform points by that matrix\nNX = R*X\nplot_transformed_circle(X,NX)\n\n[[0.5 0.3]\n [0.4 0.8]]\n\n\n\n\n\n\n\n\n\nYou can do this same thing to different dimensions of inputs, such as 1-D (lines):\n\nX_line = np.linspace(-1,1,10)  # Just create a 1-D set of points\nprint(\"X_line:\\n\",X_line)\nR[:,0]*X_line   # Convert it into \nprint(\"Transformed:\\n\",R[:,0]*X_line)\nplot_transformed_circle(np.vstack([X_line,np.zeros_like(X_line)]),\n                        R[:,0]*X_line)\n\nX_line:\n [-1.  -0.8 -0.6 ...  0.6  0.8  1. ]\nTransformed:\n [[-0.5 -0.4 -0.3 ...  0.3  0.4  0.5]\n [-0.4 -0.3 -0.2 ...  0.2  0.3  0.4]]\n\n\n\n\n\n\n\n\n\n\nD The Singular Value Decomposition\nFor \\(N\\) data points of \\(d\\) dimensions: \\[\nX_{d\\times N} = U_{d\\times d}  \\Sigma_{d\\times N} V^*_{N\\times N}\n\\] Where \\(U\\), \\(V\\) are orthogonal (\\(UU^T=I\\)), and \\(\\Sigma\\) is diagonal.\n\nNX\n\nmatrix([[ 0.5,  0.6,  0.6, ...,  0.3,  0.5,  0.5],\n        [ 0.4,  0.6,  0.8, ..., -0.1,  0.2,  0.4]])\n\n\n\nU,s,V = np.linalg.svd(NX,full_matrices=False) # Why is this useful?\nS = np.diag(s)\nprint('U:', U.shape)\nprint('S:', S.shape) # Why is this only 2x2, rather than 2x25?\nprint('V:', V.shape) # Why is this only 2x25, rather than 25x25?\nprint('S ='); print(S)\nprint('U*U.T ='); print(U*U.T)\n\nU: (2, 2)\nS: (2, 2)\nV: (2, 25)\nS =\n[[3.8 0. ]\n [0.  1.1]]\nU*U.T =\n[[1. 0.]\n [0. 1.]]\n\n\n\nV\n\nmatrix([[-0.2, -0.2, -0.3, ..., -0. , -0.1, -0.2],\n        [ 0.2,  0.2,  0.1, ...,  0.3,  0.3,  0.2]])\n\n\n\nprint('S ='); print(S)\n\n# Allow me to plot multiple circles on one figure\ndef plot_ax(ax,X,title=None):\n    ax.scatter(X[0].flat, X[1].flat,c='g')\n    ax.set_xlim([-1.5, 1.5])\n    ax.set_ylim([-1.5, 1.5])\n    if(title):\n        plt.title(title)   \n\nfig = plt.figure()\nplot_ax(fig.add_subplot(221, aspect='equal'),    NX, 'Original Data')\nplot_ax(fig.add_subplot(223, aspect='equal'),     V, 'V')\nplot_ax(fig.add_subplot(224, aspect='equal'),   S*V, 'S*V')\nplot_ax(fig.add_subplot(222, aspect='equal'), U*S*V, 'U*S*V')\nplt.show() # Essentially a \"Change of Basis\"\n# U and V are orthogonal matrices, so they just represent Rotations/Reflections\n\nS =\n[[3.8 0. ]\n [0.  1.1]]\n\n\n\n\n\n\n\n\n\n\nY = circle_points(2,2/5) # Create points on a different ellipse\nNY = R*Y  # Transform those points with a matrix\nplot_transformed_circle(Y,NY)\n\n\n\n\n\n\n\n\n\n# Do the SVD\nU,s,V = np.linalg.svd(NY,full_matrices=False)\nS = np.diag(s); print('S ='); print(S)\n\n# Plot the data and the various SVD transformations\nfig = plt.figure()\nplot_ax(fig.add_subplot(221, aspect='equal'),    NY, 'Original Data')\nplot_ax(fig.add_subplot(223, aspect='equal'),     V, 'V')\nplot_ax(fig.add_subplot(224, aspect='equal'),   S*V, 'S*V')\nplot_ax(fig.add_subplot(222, aspect='equal'), U*S*V, 'U*S*V')\nplt.show()\n\nS =\n[[5.1 0. ]\n [0.  0.7]]\n\n\n\n\n\n\n\n\n\n\n\nE What about different dimensions?\n\nM = np.matrix([[1,0,],[0,1],[2,0.5]])\nr = np.matrix([[1],[1]])\nprint(f\"M = {M}\")\nprint(f\"r = {r}\")    # What are the dimensions of r?\nprint(f\"M*r = {M*r}\") # What are the dimensions of M*r?\n\nM = [[1.  0. ]\n [0.  1. ]\n [2.  0.5]]\nr = [[1]\n [1]]\nM*r = [[1. ]\n [1. ]\n [2.5]]\n\n\n\n# Let's plot the points in 3D \ndef plot_3D_circle(X, elev=10., azim=50, title=None, c='b'):\n    \"\"\"\n    Plots 3D points from a (3, N) matrix X using matplotlib.\n\n    Parameters\n    ----------\n    X : np.ndarray or np.matrix\n        3xN array of points to plot.\n    elev : float, optional\n        Elevation angle for the 3D plot.\n    azim : float, optional\n        Azimuth angle for the 3D plot.\n    title : str, optional\n        Title for the plot.\n    c : str, optional\n        Color for the points.\n    \"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    x = np.array(X[0]).flatten()\n    y = np.array(X[1]).flatten()\n    z = np.array(X[2]).flatten()\n    ax.scatter(x, y, z, c=c)\n    # Create cubic bounding box to simulate equal aspect ratio\n    max_range = np.array([x.max()-x.min(), y.max()-y.min(), z.max()-z.min()]).max()\n    Xb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][0].flatten() + 0.5*(x.max()+x.min())\n    Yb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][1].flatten() + 0.5*(y.max()+y.min())\n    Zb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][2].flatten() + 0.5*(z.max()+z.min())\n    for xb, yb, zb in zip(Xb, Yb, Zb):\n        ax.plot([xb], [yb], [zb], 'w')\n    ax.view_init(elev=elev, azim=azim)\n    if title:\n        plt.title(title)\n    plt.show()\n    \ndef plot_3D_ax(ax, X, elev=10., azim=50, title=None,c='b'):\n    x = np.array(X[0].flat)\n    y = np.array(X[1].flat)\n    z = np.array(X[2].flat)\n    ax.scatter(x,y,z,c=c)\n    # Create cubic bounding box to simulate equal aspect ratio\n    max_range = np.array([x.max()-x.min(), y.max()-y.min(), z.max()-z.min()]).max()\n    Xb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][0].flatten() + 0.5*(x.max()+x.min())\n    Yb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][1].flatten() + 0.5*(y.max()+y.min())\n    Zb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][2].flatten() + 0.5*(z.max()+z.min())\n    # Comment or uncomment following both lines to test the fake bounding box:\n    for xb, yb, zb in zip(Xb, Yb, Zb):\n       ax.plot([xb], [yb], [zb], 'w')\n\n\nM = np.matrix([[1,0,],[0,1],[2,0.5]])\ninteractive_3D = lambda e,a: plot_3D_circle(M*X,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a=(0,360,30))\n\n\n\n\n&lt;function __main__.&lt;lambda&gt;(e, a)&gt;\n\n\n\nM*X\n\nmatrix([[ 1. ,  1. ,  0.9, ...,  0.9,  1. ,  1. ],\n        [ 0. ,  0.3,  0.5, ..., -0.5, -0.3, -0. ],\n        [ 2. ,  2.1,  2. , ...,  1.5,  1.8,  2. ]])\n\n\n\nM = np.matrix([[1,2],[-2,2],[2,0.5]])\nprint(M)\n\n[[ 1.   2. ]\n [-2.   2. ]\n [ 2.   0.5]]\n\n\n\ninteractive_3D = lambda e,a: plot_3D_circle(M*Y,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a = (0,360,30))\n\n\n\n\n&lt;function __main__.&lt;lambda&gt;(e, a)&gt;\n\n\n\n# M*Y is a 3D set of points\nU,s,V = np.linalg.svd(M*Y,full_matrices=False)\nS = np.diag(s)\nprint('U:', U.shape)\nprint('S:', S.shape)\nprint('V:', V.shape)\nprint('S =')\nprint(S)\n\nU: (3, 3)\nS: (3, 3)\nV: (3, 25)\nS =\n[[21.6  0.   0. ]\n [ 0.   4.   0. ]\n [ 0.   0.   0. ]]\n\n\n\nU*S\n\nmatrix([[ -7.1,   2.9,  -0. ],\n        [ 14.5,   2.5,   0. ],\n        [-14.4,   1. ,   0. ]])\n\n\n\nV\n\nmatrix([[-0.3, -0.3, -0.2, ..., -0.2, -0.3, -0.3],\n        [ 0. ,  0.1,  0.1, ..., -0.1, -0.1,  0. ],\n        [-0.8,  0.5,  0.1, ..., -0. ,  0.1,  0. ]])\n\n\n\nfig = plt.figure()\nplt.plot(np.diag(S),'o-')\nplt.title(\"Magnitude of Singular Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\ninteractive_3D = lambda e,a: plot_3D_circle(V,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a = (0,360,30))\n\n\n\n\n&lt;function __main__.&lt;lambda&gt;(e, a)&gt;\n\n\n\ninteractive_3D = lambda e,a: plot_3D_circle(S*V,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a = (0,360,30))\nplt.show()\n\n\n\n\n\ninteractive_3D = lambda e,a: plot_3D_circle(U*S*V,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a = (0,360,30))\n\n\n\n\n&lt;function __main__.&lt;lambda&gt;(e, a)&gt;\n\n\n\nprint('S ='); print(S)\nprint('V ='); print(V)\nprint('S*V ='); print(S*V)\n\nS =\n[[21.6  0.   0. ]\n [ 0.   4.   0. ]\n [ 0.   0.   0. ]]\nV =\n[[-0.3 -0.3 -0.2 ... -0.2 -0.3 -0.3]\n [ 0.   0.1  0.1 ... -0.1 -0.1  0. ]\n [-0.8  0.5  0.1 ... -0.   0.1  0. ]]\nS*V =\n[[-6.  -5.8 -5.1 ... -5.3 -5.8 -6. ]\n [ 0.   0.3  0.6 ... -0.5 -0.3  0. ]\n [-0.   0.   0.  ... -0.   0.   0. ]]\n\n\n\n# So if V's 3rd row doesn't matter, why don't we just get rid of it?\nVt = V[0:2,:]\nSt = S[:,0:2]\nprint('St ='); print(St)\nprint('Vt ='); print(Vt)\nprint('St*Vt ='); print(St*Vt)\n\nSt =\n[[21.6  0. ]\n [ 0.   4. ]\n [ 0.   0. ]]\nVt =\n[[-0.3 -0.3 -0.2 ... -0.2 -0.3 -0.3]\n [ 0.   0.1  0.1 ... -0.1 -0.1  0. ]]\nSt*Vt =\n[[-6.  -5.8 -5.1 ... -5.3 -5.8 -6. ]\n [ 0.   0.3  0.6 ... -0.5 -0.3  0. ]\n [ 0.   0.   0.  ...  0.   0.   0. ]]\n\n\n\nprint('U ='); print(U)\n\nU =\n[[-0.3  0.7 -0.6]\n [ 0.7  0.6  0.4]\n [-0.7  0.3  0.7]]\n\n\n\n# Truncate U. Now we have the \"Truncated SVD\"\nUt = U[:,0:2]\nSt = St[0:2,:]\nprint('Ut ='); print(Ut)\nprint('St ='); print(St)\nprint('Vt ='); print(Vt)\n\nUt =\n[[-0.3  0.7]\n [ 0.7  0.6]\n [-0.7  0.3]]\nSt =\n[[21.6  0. ]\n [ 0.   4. ]]\nVt =\n[[-0.3 -0.3 -0.2 ... -0.2 -0.3 -0.3]\n [ 0.   0.1  0.1 ... -0.1 -0.1  0. ]]\n\n\n\nprint('Ut*St*Vt ='); print(Ut*St*Vt) # Even though we threw away info...\nprint('M*Y = '); print(M*Y)\nprint(\"Are they equal?: \", np.allclose(M*Y,Ut*St*Vt))\n\nUt*St*Vt =\n[[ 2.   2.1  2.1 ...  1.3  1.7  2. ]\n [-4.  -3.7 -3.1 ... -3.9 -4.1 -4. ]\n [ 4.   3.9  3.6 ...  3.4  3.8  4. ]]\nM*Y = \n[[ 2.   2.1  2.1 ...  1.3  1.7  2. ]\n [-4.  -3.7 -3.1 ... -3.9 -4.1 -4. ]\n [ 4.   3.9  3.6 ...  3.4  3.8  4. ]]\nAre they equal?:  True\n\n\n\nprint(Vt)\nplot_circle(Vt) # The actual basis which preserves data variability\n\n[[-0.3 -0.3 -0.2 ... -0.2 -0.3 -0.3]\n [ 0.   0.1  0.1 ... -0.1 -0.1  0. ]]\n\n\n\n\n\n\n\n\n\n\n# Let's make things more difficult - add some noise:\nZ = M*X + np.random.normal(0,0.5,size=(M*X).shape)\ninteractive_3D = lambda e,a: plot_3D_circle(Z,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a = (0,360,30))\n\n\n\n\n&lt;function __main__.&lt;lambda&gt;(e, a)&gt;\n\n\n\nUe,se,Ve = np.linalg.svd(Z,full_matrices=False)\nSe = np.diag(se)\nprint('S ='); print(Se) # What is different, compared to no-noise?\n\nS =\n[[11.5  0.   0. ]\n [ 0.   9.7  0. ]\n [ 0.   0.   2.8]]\n\n\n\n# Truncate:\nUet = Ue[:,0:2]\nSet = Se[0:2,0:2]\nVet = Ve[0:2,:]\nprint(Z); print(); print(Uet*Set*Vet)\n\n[[ 1.5  1.7  2.  ...  0.2  1.1  0.8]\n [-2.  -1.3 -1.5 ... -3.  -2.9 -2.4]\n [ 2.1  2.3  1.6 ...  1.9  1.1  2.3]]\n\n[[ 1.3  1.7  1.5 ...  0.2  0.5  0.9]\n [-1.9 -1.3 -1.2 ... -3.  -2.5 -2.5]\n [ 2.3  2.3  2.  ...  1.9  1.9  2.2]]\n\n\n\nprint(\"Are they equal?: \", np.allclose(Z,Uet*Set*Vet)) # We lost info\n\nAre they equal?:  False\n\n\n\nx = np.arange(-.2,.3,.05)\ny = np.arange(-.6,.6,.1)\nvep = np.matrix(np.transpose([np.tile(x, len(y)), np.repeat(y, len(x))]))\nZt = Uet*Set*vep.T\n#Zt = Uet*Set*Vet\ndef compare_3D(Z, Zt, elev=10., azim=50):\n    \"\"\"\n    Plots two sets of 3D points for visual comparison.\n\n    Parameters\n    ----------\n    Z : np.ndarray or np.matrix\n        Original 3D data (shape: 3 x N).\n    Zt : np.ndarray or np.matrix\n        Transformed 3D data (shape: 3 x N).\n    elev : float, optional\n        Elevation angle for the 3D plot.\n    azim : float, optional\n        Azimuth angle for the 3D plot.\n    \"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    # Ensure shapes are (3, N)\n    Z = np.asarray(Z)\n    Zt = np.asarray(Zt)\n    # If Zt is 2D, pad with zeros for 3D visualization\n    if Zt.shape[0] == 2:\n        Zt = np.vstack([Zt, np.zeros(Zt.shape[1])])\n    ax.scatter(Z[0], Z[1], Z[2], c='b', label='Original')\n    ax.scatter(Zt[0], Zt[1], Zt[2], c='g', label='Transformed')\n    ax.view_init(elev=elev, azim=azim)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n\n\ninteractive_3D = lambda e,a: compare_3D(Z,Zt,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a = (0,360,30))\n\n\n\n\n&lt;function __main__.&lt;lambda&gt;(e, a)&gt;\n\n\nThe idea of uncovering structure, or reducing data-dimensions is one key goal of Unsupervised Learning. In particular, the SVD (among other methods) can be used for Principal Component Analysis: reducing the number of dimensions of a data-set, by finding a linear transformation to a smaller orthogonal basis which minimizes reconstruction error to the original space.\n\nfrom sklearn.decomposition import PCA\npcaY = PCA(n_components=2).fit_transform(np.asarray(Z).T)\npcaY = np.array([[-.1,0],[0,-.1]])@pcaY.T # Some scaling/flipping\nfig = plt.figure()\nplot_ax(fig.add_subplot(121, aspect='equal'), 4*pcaY, 'PCA')\nplot_ax(fig.add_subplot(122, aspect='equal'),  4*Vet, 'SVD')\nplt.show()\n# Note: result is (essentially) identical to PCA (up to scale/flipped axes)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Review of Matrices and the Singular Value Decomposition</span>"
    ]
  }
]