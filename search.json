[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning for Mechanical Engineering",
    "section": "",
    "text": "Preface\nThis is an open textbook to accompany my course notes for “Machine Learning for Mechanical Engineering” at ETH Zürich in the Department of Mechanical and Process Engineering (D-MAVT).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#runnable-and-interactive-code-elements",
    "href": "index.html#runnable-and-interactive-code-elements",
    "title": "Machine Learning for Mechanical Engineering",
    "section": "Runnable and Interactive Code Elements",
    "text": "Runnable and Interactive Code Elements\nMany of the elements of this book involve runnable code elements in Python, or interactive experiments that you can conduct or visualize while you are reading. These will appear largely static in the rendered book, since some may have long run-times, but you can download and run the corresponding notebook in a browser-based environment (e.g., CoLab) to be able to run the experiments. Most experiments in the book are designed to be run in class in the span of a few minutes, so browser-based environments should work well for this.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#callouts-with-notes-and-experiments",
    "href": "index.html#callouts-with-notes-and-experiments",
    "title": "Machine Learning for Mechanical Engineering",
    "section": "Callouts with Notes and Experiments",
    "text": "Callouts with Notes and Experiments\nThrough the book, I will occasionally use callouts like the one demonstrated below for a couple of use cases:\n\n\n\n\n\n\nNoteExample Collapsed Note, e.g., for a Derivation\n\n\n\n\n\nHere is what a note with a collapsable block structure will look like. We will also use this type of structure to hide details of long derivations, either so you can work them out on your own first, or because the details are not immediately central to following what comes next in the chapter or lecture.\n\n\n\n\n\n\n\n\n\nTipExample Experiment\n\n\n\nHere is an example where we might pose an experiment or task for you to do on your own or in class. Often wrestling with these experiments is a good way to build intuitive understanding and integrate some of the material into practice. When you see this type of callout, you should pause and do the exercise at this point in the chapter, rather than reading ahead, as it may build intuition which is useful later in the chapter.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part1/part1.html",
    "href": "part1/part1.html",
    "title": "Foundational Skills",
    "section": "",
    "text": "This part of the book covers useful foundational skills that should serve you well regardless of which model is State-of-the-Art at the time. We will use them throughout the book, but it is helpful to have some material and exercises together in a cohesive whole. Specifically, this part covers:\n\n?sec-mapping-models for ….\n?sec-evaluating-models for …\n?sec-designing-experiments for …\n5  Taking Derivatives covers …",
    "crumbs": [
      "Foundational Skills"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html",
    "href": "part1/reviewing_supervised_linear_models.html",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "",
    "text": "1.1 What is a Linear Model?\nWe will start by reviewing Linear Models, since they are likely already familiar to you from prior coursework, are widely used, and will serve as a useful (if simple) launching off point for later discussions of more advanced techniques. So, while some of what we will explore in this section might seem pretty basic at first glance, do not let it’s simplicity fool you, as we will revisit similar concepts throughout the rest of the notes, as these concepts will help you form a strong foundation that will serve us well once things get more complex.\nThis chapter will do this in three parts:\nWith this as a baseline model, the next chapter will review the concept of Cross-Validation and how we evaluate whether an Machine Learning model is “good”, and then the subsequent chapter will review (Stochastic) Gradient Descent, in the Linear Model context.\nLet’s start by trying to fit a model to the (admittedly simple) dataset below, where I have just sampled some (noisy) points from a periodic function:\nCode\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import SGDClassifier\nfrom ipywidgets import interact,interact_manual, FloatSlider\nfrom sklearn.datasets import make_regression\nimport seaborn as sns\nsns.set_context('poster')\nnp.random.seed(1)\n\n# Number of data points\nn_samples = 30\n\n# True Function we want to estimate\ntrue_fun = lambda X: np.cos(1.5 * np.pi * X)\n\n# Noisy Samples from the true function\nX = np.sort(np.random.rand(n_samples))\ny = true_fun(X) + np.random.randn(n_samples) * 0.1\n\nplt.figure(figsize=(10,10))\n# Plot the true function:\nX_plot = np.linspace(0, 1, 100)\nplt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n# Plot the data samples\nplt.scatter(X,y, label=\"Samples\")\nplt.legend(loc=\"best\")\nplt.show()\nIf we wanted to fit a line to this data, we would use what is called a linear model:\n\\[\ny = w_0+w_1\\cdot x\n\\]\nwhere \\(w_0\\) is the intercept and \\(w_1\\) is the slope of the line. We can write this more compactly using vector notation as: \\[\ny = \\mathbf{w}^T \\mathbf{x}\n\\] where w is the weight vector [\\(w_0\\), \\(w_1\\)] and \\(x\\) is the feature vector [1, x]. We can see here that taking the dot product between \\(w\\) and \\(x\\) is equivalent to the equation above. Importantly, even though the above equation represents a straight line with respect to x, we are not limited to using linear models only for this. For example, we could make:\n\\[\n\\mathbf{w} = [w_0, w_1, w_2, w_3];\\quad \\mathbf{x} = [1, x, x^2, x^3]\n\\]\nand in this way, we can model y as a cubic function of x, while \\(y = \\mathbf{w}^T \\mathbf{x}\\) remains a “linear model”, since it is still linear with respect to the weights \\(w\\). This is quite powerful, since by adding features (i.e., additional concatentated entries) to \\(\\mathbf{x}\\), we can fit functions that are apparently non-linear with respect to the original input variable \\(x\\), but will possess many useful properties of linear models that we will discuss later (e.g., convexity with respect to \\(w\\)).\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\n\n# Here is a list of different degree polynomials to try out\ndegrees = [1,2,3,5,10,15,20,30]\n\n# Generate samples of the true function + noise\nX = np.sort(np.random.rand(n_samples))\nnoise_amount = 0.1\ny = true_fun(X) + np.random.randn(n_samples) * noise_amount\n\n# For each of the different polynomial degrees we listed above\nfor d in degrees:\n    plt.figure(figsize=(7, 7)) # Make a new figure\n    # Construct the polynomial features\n    polynomial_features = PolynomialFeatures(degree=d,\n                                             include_bias=False)\n    # Construct linear regression model\n    linear_regression = LinearRegression()\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    # Now fit the data first through the \n    # polynomial basis, then do regression\n    pipeline.fit(X[:, np.newaxis], y)\n    \n    # Get the accuracy score of the trained model\n    # on the original training data\n    score = pipeline.score(X[:, np.newaxis],y)\n\n    # Plot the results\n    X_plot = np.linspace(0, 1, 100)\n    plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label=\"Model\")\n    plt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n    plt.scatter(X, y, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    \n    # Print the polynomial degree and the training\n    # accuracy in the title of the graph\n    plt.title(\"Degree {}\\nTrain score = {:.3f}\".format(\n        d, score))\n    plt.show()\nWe can see that even though we are fitting a linear model every time, the behavior with respect to x is markedly non-linear. Moreover, we see some strange behavior as we increase the polynomial degree. What is going on here and why is it behaving in this way? To build some intuition, we can take a look at the learned weight coefficients, by accessing the coef_ attribute of the fitted model:\nCode\nlinear_regression.coef_\n\n\narray([ 4.29e+04, -6.41e+05,  4.46e+06, -5.47e+06, -1.61e+08,  1.50e+09,\n       -7.03e+09,  2.01e+10, -3.46e+10,  2.83e+10,  9.52e+09, -3.50e+10,\n        5.36e+08,  3.61e+10,  3.50e+09, -3.63e+10, -1.74e+10,  2.94e+10,\n        3.42e+10, -1.02e+10, -4.27e+10, -1.74e+10,  3.46e+10,  4.11e+10,\n       -1.48e+10, -5.44e+10,  1.25e+09,  6.96e+10, -5.11e+10,  1.14e+10])\nWhat is going on here? To understand this, we need to understand something about how the model is optimizing error and how we might control this behavior.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#what-is-a-linear-model",
    "href": "part1/reviewing_supervised_linear_models.html#what-is-a-linear-model",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "",
    "text": "TipExperiment: Effect of Polynomial Degree on Model Fit\n\n\n\nWe can see this in the below experiment, where we fit progressively higher order polynomial functions to the above training data. In each case we are fitting a linear model, by virtue of appending additional polynomial features to x. When you do this experiment, ask yourself:\n\nHow does the model fit change as we increase the polynomial degree?\nWhat happens to the training score (\\(R^2\\)) as we increase the polynomial degree?\nDoes this increase in the training score reflect what you intuitively expect? Why or why not?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#regularization-and-controlling-model-complexity",
    "href": "part1/reviewing_supervised_linear_models.html#regularization-and-controlling-model-complexity",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.2 Regularization and Controlling Model Complexity",
    "text": "1.2 Regularization and Controlling Model Complexity\nIn the base case above, all the model is trying to do is minimize the Mean Squared Error (MSE) with respect to the training error, and with a sufficient number of parameters (polynomial features, in this case), it becomes possible to always achieve perfect accuracy (i.e., fit every point) in the training data. Unfortunately, as we can see, fitting the training data perfectly does not necessarily lead to a model that generalizes well to new data. To help us control this behavior, we can introduce the concept of Regularization, which is a way of penalizing overly complex models. Specifically, we can modify our cost function to be something like:\n\\[\nCost = Loss(w,D) + \\Omega(w)\n\\]\nwhere \\(\\Omega(w)\\) represents what we call a “Regularization” of the function or a “Penalty Term” The purpose of \\(\\Omega(w)\\) is to help us prevent the (otherwise complex) model from being overly complicated, by penalizing this complexity. There are many ways to do this that we will see later on, but one common way to do this for linear models is to penalize the total weight that you allow all of the \\(w_i\\) to have. Specifically how one calculates this total weight turns out to matter a lot, and we shall see it return in later chapters and sections. But to get us started in un-packing how to do this, we first need to talk about what a Norm is, how it relates to Linear Regression weights, and how it helps us perform Regularization.\n\n1.2.1 Norms and their relationship to Regularization\nA Norm is a concept in mathematics that allows us to essentially measure length or size, typically of vectors. Any time you have tried to compute the distance between two points in space (say, by using the Pythagorean Theorem), or the magnitude of an applied Force vector, you have been using a Norm — most likely the Euclidean Norm or Euclidean Distance. For example, for a vector \\(\\mathbf{x}\\) with \\(n\\) dimensions, the Euclidean Norm looks like this: \\[\n||\\mathbf{x}||_2 = \\sqrt{ x_1^2 + x_2^2 + \\cdots x_n^2 }\n\\] If you have ever had to compute the total Force Magnitude given its x and y components (for example, in Statics class), you have used the Euclidean Norm to do so. In that context, it served to take multiple components of a Force aggregate them in such a way as to tell you something about the total force – by analogy, we will do the same thing here with linear regression, where each weight is like a component and we can use the Euclidean Norm to compute the total weight.\nWhile Euclidean Norms may be quite useful or familiar to Engineers, it turns out that they are a special case of a much wider family of Norms called p-norms, which are defined as: \\[\n||\\mathbf{x}||_p = \\left(|x_1|^p + |x_2|^p+\\cdots + |x_n|^p\\right)^{1/p} = \\left(\\sum_{i=1}^n \\left| x_i \\right|^p \\right)^p\n\\]\nSpecifically, the Euclidean Norm is called the L2-norm, or sometimes just the 2-norm. To see why this is, just set \\(p=2\\) in the above, and note how it corresponds to the Euclidean Norm that we all know and love. So, by setting \\(p\\) to a number between \\(0\\) and \\(\\infty\\), we can modify what the total weight means, and setting \\(p=2\\) is the setting which we are all most familiar with. To get a visual sense of how norms vary, see below, which visualizes a line of “circle” of radius 1, but where the length of the line is determined by the p-norm. You will see that when p=2 this corresponds to what we are familiar with, but when p goes up or down things change.\n\n\nCode\nfrom ipywidgets import interact, FloatSlider\n\n# Define a function to compute and plot the p-norm unit ball in 2D\ndef plot_p_norm(p=2.0):\n    # Avoid invalid p values\n    if p &lt;= 0:\n        print(\"p must be &gt; 0\")\n        return\n    \n    theta = np.linspace(0, 2*np.pi, 400)\n    # Parametric form of p-norm unit circle\n    x = np.cos(theta)\n    y = np.sin(theta)\n    \n    # Normalize to p-norm = 1\n    denom = (np.abs(x)**p + np.abs(y)**p)**(1/p)\n    x_unit = x / denom\n    y_unit = y / denom\n    \n    plt.figure(figsize=(5,5))\n    plt.plot(x_unit, y_unit, label=f'p = {p:.2f}')\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.title(f'Unit Ball in p-norm (p={p:.2f})')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Interactive slider for p\ninteract(\n    plot_p_norm,\n    p=FloatSlider(value=2.0, min=0.1, max=10.0, step=0.1, description='p')\n)\n\n\n\n\n\n&lt;function __main__.plot_p_norm(p=2.0)&gt;\n\n\nFor today, we will just focus on the L2-Norm, however we will revist norms again later where we will see how changing the one we are using can have positive or negative effects in certain circumstances.\nWe will use the L2-Norm to help us penalize having linear regression models with really large weights, by essentially putting a cost on the total weight of the weight vector, where the total is measured by the L2-Norm. That is: \\[\nCost = \\sum_{n=1}^{N}||y-w\\cdot x||^2 + \\alpha \\cdot ||w||^2\n\\] Where \\(\\alpha\\) is the price we pay for including more weight in the linear model. If it reduces our error cost enough to offset the cost of the increased weight, then that may be worth it to us. Otherwise, we would err on the side of using less weight overall.\nThis Regularization (i.e., increasing \\(\\alpha\\)) essentially allows you to trade off bias and variance, as we will see below.\n\n\n\n\n\n\nTipExperiment: Effect of Increasing Regularization Strength on Polynomial Fit\n\n\n\nIn the below experiment, we will increase the regularization strength (\\(\\alpha\\)) for a fixed 15-degree polynomial, and observe its effect on the overfitting problem before. Consider the following questions as you observe the experiment below:\n\nWhat happens when we set \\(\\alpha\\) to a low (close to zero) value?\nWhat happens when we set \\(\\alpha\\) to a high value?\n\n\n\n\n\nCode\n# Import Ridge Regression\nfrom sklearn.linear_model import Ridge\n\n# alpha determines how much of \n# a penalty the weights incur\nalphas = [0, 1e-20, 1e-10, 1e-7, 1e-5, 1, 10, 100]\n\n# For the below example, let's\n# just consider a 15-degree polynomial\nd=15\nnp.random.seed(100)\n\nfor a in alphas:\n    plt.figure(figsize=(7, 7))\n    polynomial_features = PolynomialFeatures(degree=d,\n                                             include_bias=False)\n    #linear_regression = LinearRegression() #&lt;- Note difference with next line\n    linear_regression = Ridge(alpha=a)\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    # Fit model\n    pipeline.fit(X[:, np.newaxis], y)\n    # Get Training Accuracy\n    score = pipeline.score(X[:, np.newaxis],y)\n\n    # Plot things\n    X_plot = np.linspace(0, 1, 100)\n    plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label=\"Model\")\n    plt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n    plt.scatter(X, y, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    \n    plt.title(\"Degree={}, $\\\\alpha$={}\\nTrain score = {:.3f}\".format(\n        d, a, score))\n    plt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#examples-of-other-commonly-used-loss-functions-in-linear-models",
    "href": "part1/reviewing_supervised_linear_models.html#examples-of-other-commonly-used-loss-functions-in-linear-models",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.3 Examples of Other Commonly used Loss Functions in Linear Models",
    "text": "1.3 Examples of Other Commonly used Loss Functions in Linear Models\nThus far we have been discussing Linear Models in their most familiar context — minimizing the Mean Squared Error (MSE) with respect to the training data, optionally with an L2 regularization on the weight vector:\n\\[\n\\mathcal{L}(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 + \\alpha ||\\mathbf{w}||_2^2\n\\]\nFor now, let us ignore the regularization term, and just focus on the Loss term. Why should we minimize the squared error?1 Why not the absolute error or other possible loss functions? Let’s explore a few of those options and then see, in practice, how they affect the learned linear model.\n1 It turns out that there are good theoretical reasons for this, for example, that a Linear Model trained via an L2/MSE Loss is the Best Linear Unbiased Estimate (BLUE) of the Linear Model, according to the Gauss-Markov Theorem, but, as we will see, there are other reasons to forgo these advantages.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#loss-functions-for-regression",
    "href": "part1/reviewing_supervised_linear_models.html#loss-functions-for-regression",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.4 Loss Functions for Regression",
    "text": "1.4 Loss Functions for Regression\nBeyond classical MSE, there are two main types of variants that are commonly used for regression problems:\n\nRobust Loss Functions, that minimize the quadratic effect of the MSE loss for very large errors. These are typically used to make the trained model less sensitive to outliers in the training data.\nEpsilon-Insensitive Loss functions, that ignore errors that are sufficiently small (within an \\(\\epsilon\\) margin). These are typically used to incur some advantages in terms of sparsity in the learned model (for example, in Support Vector Methods, which we will see later).\n\nIn reality, these two variants can be combined in different ways, which we will see reflected below, but as a summary, these are:\n\nAbsolute Loss: \\(|y - \\hat{y}|\\)\nHuber Loss: A squared loss for small errors, and then transitioning to an absolute loss for large errors.\nEpsilon-Insensitive Loss: A loss that is zero for errors up to \\(\\epsilon\\) and then uses absolute loss for larger errors.\nSquared Epsilon-Insensitive Loss: Similar to Epsilon-Insensitive Loss, but uses squared loss for errors larger than \\(\\epsilon\\).\n\nOf course, you could imagine more complex variants and combinations of these properties, but these capture the main properties and benefits that we will see below.\n\n\nCode\ndef modified_huber_loss(y):\n    if(abs(y)&lt;1):\n        return y**2\n    else:\n        return 2*abs(y)-1\nmhuber = np.vectorize(modified_huber_loss)\n\neps = 0.7\ndef sq_esp_insensitive(y):\n    if(abs(y)&lt;eps):\n        return 0\n    else:\n        return (abs(y)-eps)**2\nsq_eps_ins = np.vectorize(sq_esp_insensitive)\n\n\n\n\nCode\nxmin, xmax = -4, 4\nxx = np.linspace(xmin, xmax, 100)\n\nplt.figure(figsize=(10,5))\nplt.plot(xx, xx**2, 'g-',\n         label=\"Squared Loss\")\nplt.plot(xx, abs(xx), 'g--',\n         label=\"Absolute Loss\")\n\nplt.plot(xx, abs(xx)-eps, 'b--',\n         label=\"Epsilon-Insensitive Loss\")\n\nplt.plot(xx, sq_eps_ins(xx), 'b-',\n         label=\"Sq-Epsilon-Insensitive Loss\")\nplt.plot(xx, mhuber(xx), 'r-',\n         label=\"Modified-Huber Loss\")\n\nplt.ylim((0, 8))\nplt.legend(loc=\"upper center\")\nplt.xlabel(\"Error\")\nplt.ylabel(\"$L(y, f(x))$\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.4.1 Handling Outliers using Robust Loss Functions\n\n\nCode\n# Create some data with Outliers\nn_samples = 1000\nn_outliers = 50\n\nXr, yr, coef = make_regression(n_samples=n_samples, n_features=1,\n                              n_informative=1, noise=10,\n                              coef=True, random_state=0)\n# Add outlier data\nnp.random.seed(0)\nXr[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))\nyr[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)\nyr/=10\nyr += 10\n\nline_X = np.arange(-5, 5)\nfigure = plt.figure(figsize=(9, 9))\n\nplt.scatter(Xr, yr,facecolors='None',edgecolors='k',alpha=0.5)\n\n# Loss Options: huber, squared_error, epsilon_insensitive, squared_epsilon_insensitive\nlosses = ['huber', 'squared_error']\nfor loss in losses:\n    model = SGDRegressor(loss=loss, fit_intercept=True, max_iter = 2000,\n                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\n    model.fit(Xr, yr)\n\n    # Predict data of estimated models\n    line_y = model.predict(line_X[:, np.newaxis])\n    plt.plot(line_X, line_y, '-', label=loss,alpha=1)\n\nplt.axis('tight')\nplt.legend(loc='best')\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#loss-functions-for-linear-classification",
    "href": "part1/reviewing_supervised_linear_models.html#loss-functions-for-linear-classification",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.5 Loss Functions for Linear Classification",
    "text": "1.5 Loss Functions for Linear Classification\nThus far we have only discussed Regression problems, where we are modeling a continuous output (e.g., \\(y=w^T\\cdot x\\)). However, Linear Models can also be used for Classification problems, where the output is discrete (e.g., \\(y \\in \\{0,1\\}\\) or \\(y \\in \\{-1,1\\}\\)). A naive approach to handling this, would be to just train a regression model as per before, but then just threshold the output at some value to derive the class label. For example, we can take the below binary data:\n\n\nCode\nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_features=1, n_redundant=0, \n                           n_informative=1,\n                           random_state=1,\n                           n_clusters_per_class=1,\n                           flip_y=0.0, class_sep=1)\nplt.figure()\nplt.xlabel('x')\nplt.ylabel('True or False')\nplt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\nplt.show()\n\n\n\n\n\n\n\n\n\nand then fit a regular linear model to this:\n\n\nCode\nmodel = SGDRegressor(loss='squared_error', fit_intercept=True, max_iter = 2000,\n                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\nmodel.fit(X, y)\nXp = np.linspace(X.min(),X.max(),100)\nXp = Xp[:, np.newaxis]\nplt.figure()\nplt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\nplt.plot(Xp,model.predict(Xp))\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see from the above that this is not entirely what we want – for example, just setting the class cutoff at 0.5 would not produce a classification boundary that would be optimal. In contrast, we can modify the loss function to more accurately project the \\(w^T\\cdot x\\) linear model into a classification context.\nWe do this by modifying the loss function from Mean Squared Error to something like: \\[\ny_i\\cdot(w\\cdot x_i)\n\\] where \\(y_i = \\pm 1\\) such that if \\(y_i\\) and \\(w\\cdot x_i\\) point have similar signs, then the decision function is positive, otherwise it is negative.\nWith this change, we are now interested primarily with how the behavior of the loss function when we are in the negative regime (i.e., misclassified points):\n\n\nCode\n# From: http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html\ndef modified_huber_loss(y_true, y_pred):\n    z = y_pred * y_true\n    loss = -4 * z\n    loss[z &gt;= -1] = (1 - z[z &gt;= -1]) ** 2\n    loss[z &gt;= 1.] = 0\n    return loss\n\n\nxmin, xmax = -4, 4\nxx = np.linspace(xmin, xmax, 100)\nlw = 2\nfig = plt.figure(figsize=(15,8))\nplt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], color='gold', lw=lw,\n         label=\"Zero-one loss\")\nplt.plot(xx, -np.minimum(xx, 0), color='yellowgreen', lw=lw,\n         label=\"Perceptron loss\")\nplt.plot(xx, modified_huber_loss(xx, 1), color='darkorchid', lw=lw,\n         linestyle='--', label=\"Modified Huber loss\")\nplt.plot(xx, np.log2(1 + np.exp(-xx)), color='cornflowerblue', lw=lw,\n         label=\"Log loss\")\nplt.plot(xx, np.where(xx &lt; 1, 1 - xx, 0), color='teal', lw=lw,\n         label=\"Hinge loss\")\nplt.plot(xx, np.where(xx &lt; 1, 1 - xx, 0) ** 2, color='orange', lw=lw,\n         label=\"Squared hinge loss\")\nplt.ylim((0, 8))\nplt.legend(loc=\"upper right\")\nplt.xlabel(r\"Decision function $f(x)$\")\nplt.ylabel(\"$L(y, f(x))$\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n####### Try Changing the Below ######\n#loss = 'squared_error'\nloss = 'log_loss'\n#loss = 'hinge'\n#####################################\n\nmodel = SGDClassifier(loss=loss, fit_intercept=True, max_iter = 2000,\n                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\nmodel.fit(X, y)\nXp = np.linspace(X.min(),X.max(),100)\nXp = Xp[:, np.newaxis]\nplt.figure()\nplt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\ntry:\n    plt.plot(Xp,model.predict_proba(Xp)[:,1],label='probability')\nexcept:\n    pass\nplt.plot(Xp,model.predict(Xp))\nplt.title(\"Classifier with loss = {}\".format(loss))\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also try this with 2D data to get a better sense of how some of the other loss functions behave:\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\n\n\nX, y = make_classification(n_features=2, n_redundant=0, \n                           n_informative=2,\n                           random_state=1,\n                           n_clusters_per_class=1,\n                           flip_y=0.0, class_sep=0.7)\n\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\n# Change: try 0,1, or 2\nds = datasets[2]\n\nX, y = ds\nX = StandardScaler().fit_transform(X)\n\nplt.figure(figsize=(9, 9))\nplt.scatter(X[y==1,0],X[y==1,1],marker='+')\nplt.scatter(X[y==0,0],X[y==0,1],marker='o',facecolors='none',edgecolors='k')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Effect of Linear Model Classification Losses\n\n\n\nIn the below experiment, try modifying the different classification loss functions and re-running the model. You will see a dark solid line representing the decision boundary, and dashed lines representing where the decision boundary is +1 or -1. What do you notice?:\n\nFor the perceptron loss, what behavior do you observe if you re-run this model multiple times? Why do you observe this behavior?\nComparing the hinge loss to the perceptron loss, the loss functions look remarkably similar, yet they have very different behavior in the model. Why do you think this is?\nComparing the squared error versus log loss versus hinge, what sorts of differences in behavior do you observe? Thinking about the location of the decision boundary and the shape of the loss functions, why do you think they behave differently?\n\n\n\n\n\nCode\nfrom sklearn.svm import LinearSVC\n\n# Try modifying these:\n#====================\nloss = 'squared_error'\n#loss = 'perceptron'\n#loss = 'log_loss'\n#loss = 'hinge'\n#loss = 'modified_huber'\n#loss = 'squared_hinge'\n\n# Also try the effect of Alpha:\n# e.g., between ranges 1e-20 and 1e0\n#=============================\nalpha=1e-3\n\n# You can also try other models by commenting out the below:\nmodel = SGDClassifier(loss=loss, fit_intercept=True,\n                      max_iter=2000,tol=1e-5, n_iter_no_change =100,\n                      penalty='l2',alpha=alpha) \n# If you would like to compare the SGDClassifier with hinge loss to LinearSVC, you can uncomment the below:\n#model = LinearSVC(loss='hinge',C=1e3)\nmodel.fit(X, y)\n\nplt.figure(figsize=(9, 9))\n\nh=0.01\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nif hasattr(model, \"decision_function\"):\n    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\nelse:\n    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\nZ = Z.reshape(xx.shape)\n\nvmax = max(abs(Z.min()),abs(Z.max()))\ncm = plt.cm.RdBu\nplt.contourf(xx, yy, Z, cmap=cm, alpha=.5, vmax = vmax, vmin = -vmax)\nlevels = [-1.0, 0.0, 1.0]\nlinestyles = ['dashed', 'solid', 'dashed']\ncolors = 'k'\nplt.contour(xx, yy, Z, levels, colors=colors, linestyles=linestyles)\nplt.scatter(X[y==1,0],X[y==1,1],marker='+')\nplt.scatter(X[y==0,0],X[y==0,1],marker='o',facecolors='none',edgecolors='k')\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#penalty-functions-example-of-how-l_p-penalty-changes-loss-optima",
    "href": "part1/reviewing_supervised_linear_models.html#penalty-functions-example-of-how-l_p-penalty-changes-loss-optima",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.6 Penalty Functions: Example of How \\(L_p\\) Penalty Changes Loss Optima",
    "text": "1.6 Penalty Functions: Example of How \\(L_p\\) Penalty Changes Loss Optima\nOK, so we have seen how different loss functions can affect the learned linear model. But what about the penalty function? How does changing the penalty function affect the learned model? Let’s explore this with a simple example by again returning to fitting a line. In this case, we will fit an actual “line” to the data, by which I mean:\n\\[\ny= w_0 + w_1 \\cdot x = \\mathbf{w}^T \\cdot \\mathbf{x}\n\\]\n\n\nCode\n# Generate noisy data from a line\n\n######################\n# Change Me!\nw1_true = 5\nw0_true = 2\nnoise = 0.001\n#################\n\ntrue_func = lambda x: w1_true*x+w0_true\n\nnum_samples = 50\nx = (np.random.rand(num_samples)-0.5)*20\ny = true_func(x)+np.random.normal(scale=noise,size=num_samples)\n\n\n\n\nCode\nplt.figure()\nplt.scatter(x,y)\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this case, since we have very little noise (although you can play with this if you would like), you can see that if we put in the true intercept and slope, we get zero (or close to zero) training objective.\n\nfrom numpy.linalg import norm\ndef loss(a,b,alpha,order=2):\n    return np.average((y - (a*x+b))**2) + alpha*norm([a,b],ord=order)\n\n#example\nprint(f\"Objective: {loss(a=5,b=2,alpha=0)}\")\n\nObjective: 1.3389235788421472e-06\n\n\nThis tells us the objective at the true parameters, but now let’s visualize how the total objective (training error + penalty) varies as we change the parameters. To do this, we can compute the total objective over a grid of possible values for \\(w_0\\) and \\(w_1\\), and then plot the contours of this objective function:\n\n\nCode\nA, B = np.meshgrid(np.linspace(-10, 10, 201), np.linspace(-10, 10, 201))\nN,M = A.shape\nfloss = np.vectorize(loss)\n\ndef generate_new_data(a=5,b=5):\n    x = (np.random.rand(num_samples)-0.5)*20\n    #y = true_func(x)+np.random.normal(scale=1,size=num_samples)\n    y = a*x+b+np.random.normal(scale=1,size=num_samples)\n    return x,y\n\ndef generate_z_grid(alpha,order):\n    Z_noalpha = floss(A.flatten(),B.flatten(),0).reshape((N,M))\n    alpha=alpha\n    Z = floss(A.flatten(),B.flatten(),alpha,order)\n    min_ind = np.argmin(Z)\n    Amin = A.flatten()[min_ind]\n    Bmin = B.flatten()[min_ind]\n    Z = Z.reshape((N,M))\n    return Z_noalpha, Z, Amin, Bmin\n\nget_levels = lambda z: [np.percentile(z.flatten(),i) for i in [0,0.5,1,2,5,10,15,30,40,50,75,95,100]]\n#levels = [np.percentile(allz,i) for i in [0,0.5,1,2,5,10,15,30,40,50,75,95,100]]\n#levels = [np.percentile(allz,i) for i in np.logspace(-2,3,10,base=3)]\n\ndef plot_objective(alpha=0,order=2):\n    Z_noalpha, Z, Amin, Bmin = generate_z_grid(alpha,order)\n    plt.figure(figsize=(7,7))\n    plt.vlines(0,-10,10,alpha=0.25,colors='r',linestyles='solid')\n    plt.hlines(0,-10,10,alpha=0.25,colors='r',linestyles='solid')\n    plt.contour(A,B,Z_noalpha,10,levels=get_levels(Z_noalpha),linestyles='solid',cmap='Greys_r',alpha=0.05)\n    plt.contour(A,B,Z,10,levels=get_levels(Z),linestyles='solid',cmap='Greys_r',alpha=0.5)\n    plt.scatter([0],[0],marker='D',s=50,c='r')\n    plt.scatter([w1_true],[w0_true],marker='*',s=400)\n    plt.scatter([Amin],[Bmin])\n    plt.xlabel('a')\n    plt.ylabel('b')\n    plt.title('Optima: a={:.2f}, b={:.2f}'.format(Amin,Bmin))\n    plt.show()\n\n\n\ninteract(plot_objective,\n         alpha=np.logspace(-2,5,8),\n         order=FloatSlider(min=0,max=10,step=0.1,continuous_update=False,value=2.0))\n\n\n\n\n&lt;function __main__.plot_objective(alpha=0, order=2)&gt;\n\n\nHere we can see the true optimal parameters as a blue star, the objective function contours as dark gray lines, and the red diamond shows the point where both weights are zero. The orange circle shows the minimum point of the objective function. If you increase the value of alpha in the drop down, this will increase the penalty weight, and you can see how both the objective and the optimal point change. Moreover, you can change the p-norm order in the penalty to see what effect this has on the total objective function.\n\n\n\n\n\n\nTipExperiment: How do the different penalty terms affect the Objective Function and the optimal weight?\n\n\n\nTry modifying the following:\n\nAs you increase the penalty weight under the L2 norm, how does the objective landscape change?\nIf you use a p-order that is less than 2, how does this alter the objective landscape?\nThe L1 norm is known to induce sparsity in the optimal weights. Do you observe this in the objective landscape? Why or why not?\nDifferent types of regularization within the p-norm family are often referred to as “Shrinkage” operators. Why do you think this is the case, based on what you observe?\nIf my goal is to induce sparsity, it would make sense to consider the L0 norm, which just counts the number of non-zero entries in a vector. Based on the plots below, why won’t this work?\n\n\n\nTo help visualize the experiment above, let’s try plotting, for different norm orders, the path that the coefficients take as we set alpha = 0 (no regularization) to a large number (essentially fully regularized). Now the light green contour represents just the contribution to the objective from the training error (no regularization), and the light gray contour shows the contribution from the penalty term (no training error). The blue line shows how the optimal weight changes as we increase the penalty weight from 0 to a large number.2\n2 Note how I can visually find the optimal weight by fixing a given iso-contour of the regularization term (the gray lines) and then finding the point along that iso-contour where the green contour is minimized.\n\nCode\nfrom scipy.optimize import fmin\nimport warnings\nwarnings.simplefilter('ignore', RuntimeWarning)\n\nalpha_range = np.logspace(-1,5,14)\norder_range = [0,0.25,.5,.75,1,1.5,2,3,5,10,20,100]\n\nAl = len(alpha_range)\nOl = len(order_range)\nresults = np.zeros((Al,Ol,2))\n\nfor j,o in enumerate(order_range):\n    prev_opt = [5,5]\n    for i,a in enumerate(alpha_range):\n        f = lambda x: loss(x[0],x[1],alpha=a,order=o)\n        x_opt = fmin(f,prev_opt,disp=False)\n        results[i,j] = x_opt\n        prev_opt = x_opt\n\n\n\n\nCode\nfor j,o in enumerate(order_range):\n    fig = plt.figure(figsize=(12,7))\n    Z_noalpha, Z, Amin, Bmin = generate_z_grid(10000,o)\n    plt.contour(A,B,Z_noalpha,10,levels=get_levels(Z_noalpha),linestyles='solid',alpha=0.2,colors='g')\n    plt.contour(A,B,Z,10,levels=get_levels(Z),linestyles='solid',alpha=0.2,colors='k')\n    plt.plot(results[:,j,0],results[:,j,1],marker='o',alpha=0.75)\n    plt.title('L-{} Norm'.format(o))\n    plt.axis('equal')\n    plt.xlim([-1,6])\n    plt.ylim([-1,3])\n    plt.show()\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#summary-of-how-to-select-a-loss-or-penalty-function",
    "href": "part1/reviewing_supervised_linear_models.html#summary-of-how-to-select-a-loss-or-penalty-function",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.7 Summary of How to Select a Loss or Penalty function",
    "text": "1.7 Summary of How to Select a Loss or Penalty function\nIn class, we reviewed a couple of different forms of loss functions and penalty functions and talked a bit about the criteria for selecting them. Below is a very short summary of these.\n\n1.7.1 Loss Functions for Regression:\n\nIf you have no prior knowledge of the function or data, then selecting an L2 type loss (like the Mean Squared Error) is reasonable. When data nicely behaves w.r.t. a linear model (e.g., features are uncorrelated, errors in the linear model are uncorrelated, have equal variances, and expected error of zero around the linear model, etc.) then a Linear Model with an L2 Loss is the Best Linear Unbiased Estimate (BLUE) according to the Gauss-Markov Theorem.\nIf you have reason to believe that the data will have outliers or otherwise need to be robust to spurious large samples, then L2 loss will not be robust to this (as we saw in Lecture). For this, moving to an L1 type loss (like an Absolute Loss or Huber loss) will make the model less sensitive to outliers. It is one approach to handling Robust Regression.\nIf you need to have the model’s loss be dominated only by a handful of points/data as opposed to all of the data, then epsilon-insensitive loss is appropriate since many points well-fit by the model will have “zero” loss. For things like Linear Models, this has limited usefulness right now. However, when we “kernalize” Linear models in “Kernels” week, you will see that this is a big deal, and it is what gives rise to the “Support Vector” part of “Support Vector Machines”. Specifically, decreasing epsilon towards zero increases the number of needed Support Vectors (can be a bad thing), and increasing epsilon can decrease the number of needed Support Vectors (can be a good thing). This will make more sense in a few week’s time.\n\n\n\n1.7.2 Loss Functions for Classification:\n\nZero-One loss sounds nice, but is not useful in practice, since it is not differentiable.\nPerceptron loss, while of historical importance, is not terribly useful in practice, since it does not converge to a unique solution and an SVM (i.e., Hinge Loss below) has all of the same benefits.\nIf you need a simple linear model which outputs actual probabilities (like, 95% sure the component has failed), then the log-loss does this, via Logistic Regression and allows you to calculate classification probabilities in closed form.\nIf you want something that maximizes the margin of the classifier, then the Hinge Loss can get close to this. It is the basis of Linear Support Vector Machines\n\n\n\n1.7.3 Penalty Terms (Lp Norms) for Linear Models:\n\n\\(L_2\\) Norm penalties on the weight vector essentially “shrink” the weights towards zero as you increase the penalty weight. Adding this kind of penalty to a linear model has different names, depending on which community of people you are talking with. Some of these other names are: (1) Tikhonov regularization, (2) Ridge Regression, (3) \\(L_2\\) Shrinkage Estimators, or (4) Gaussian Weight Priors. I find looking at the penalty term itself more helpful at understanding the effects rather than memorizing the different names.\n\\(L_0\\) Norm penalties, while conceptually interesting since they essentially “count” entries in the weight vector, are not practically useful since they are not differentiable and are thus difficult to optimize.\n\\(L_1\\) Norm penalties are a compromise between \\(L_2\\) and \\(L_0\\) in that they promote sparsity in the weights (some weights will become zero) but are (largely) differentiable, meaning that you can meaningfully optimize them (unlike \\(L_0\\)). Shrinking certain weights to zero in this way can be useful when (1) you are in the \\(n \\ll p\\) regime (many more features than data points) where the model is underdetermined and (2) you want some degree of model interpretability (it sets many weights to zero). Some of these other names for this kind of linear regression with this penalty are the LASSO (least absolute shrinkage and selection operator). \n\\(L_\\infty\\) (where p is really large) essentially penalize the size of the biggest element of the weight vector (w). While there are some niche instances where this kind of norm is useful for a Loss function (e.g., Chebyshev Regression), I have rarely seen meaningful use cases in practice where this makes sense as a penalty term for the weight vector.\nCombinations of penalties. For example, a common combination is combining both an \\(L_2\\) and \\(L_1\\) penalty on the weights, as in: \\(\\Omega(w) = \\alpha ||w||_2 + \\beta ||w||_1\\). This particular combination is called “Elastic Net” and exhibits some of the good properties of \\(L_2\\) penalities with the sparsity inducing properties of \\(L_1\\) regularization.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html",
    "href": "notebooks/cross_validation_linear_regression.html",
    "title": "2  Evaluating Machine Learning Models",
    "section": "",
    "text": "2.1 Getting an Unbiased Estimate of Out-of-Sample Performance\nIn the prior chapter, we covered how different loss functions and regularization terms affected a linear model, in terms of the model’s qualitative performance and its affect on the training score. However, as we saw, the training score an be a misleading performance indicator. How might we judge the model’s performance more rigorously? This chapter addresses this by reviewing the why and how of performing Cross Validation, and also what this means regarding optimizing the hyperparameters of a model.\nWhen we train a machine learning model, we are typically interested in how well the model will perform on data it has not seen before. This is often referred to as the model’s generalization performance. However, if we evaluate the model’s performance on the same data it was trained on, we may get an overly optimistic estimate of its true performance. This is because the model may have simply memorized the training data, rather than learning the underlying patterns. One popular way to assess this is through Cross Validation:\n# Now let's split the data into training and test data:\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.2, random_state=0)\nLet’s take a look at what the above has actually done.\nCode\nprint('X_train\\n',X_train,'\\n')\nprint('X_test\\n',X_test,'\\n')\nprint('y_train\\n',y_train,'\\n')\nprint('y_test\\n',y_test,'\\n')\n\nplt.figure(figsize=(8,8))\nplt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n# plot the training and testing points in colors\nplt.scatter(X_train,y_train, label=\"Training data\")\nplt.scatter(X_test,y_test, label=\"Testing data\")\nplt.legend(loc=\"best\")\nplt.show()\n\n\nX_train\n [8.78142503e-01 3.02332573e-01 4.19194514e-01 6.92322616e-01\n 1.40386939e-01 4.17304802e-01 1.86260211e-01 3.96767474e-01\n 7.20324493e-01 6.70467510e-01 2.73875932e-02 9.68261576e-01\n 1.46755891e-01 9.23385948e-02 5.38816734e-01 5.58689828e-01\n 1.98101489e-01 1.69830420e-01 8.76389152e-01 8.50442114e-02\n 1.14374817e-04 6.85219500e-01 4.17022005e-01 3.13424178e-01] \n\nX_test\n [0.03905478 0.89460666 0.34556073 0.20445225 0.87811744 0.80074457] \n\ny_train\n [-0.37395132  0.05199163 -0.47818202 -0.82672018  0.90350849 -0.45417721\n  0.72898424 -0.36366041 -0.89399733 -1.11157064  0.90389734 -0.21270638\n  0.86040424  0.79675107 -0.89105816 -0.87458209  0.52662674  0.74673588\n -0.63887828  0.97904574  0.98275703 -0.97273902 -0.42390528  0.0668933 ] \n\ny_test\n [ 0.98733352 -0.47140604 -0.00455281  0.55839434 -0.61801179 -0.82613321]\nThe key idea in cross validation is to test the model on data that was separate from the data you trained on, therefore establishing two datasets: a training set and a test set. The training set is used to fit the model, while the test set is used to evaluate its performance. This way, we can get a more realistic estimate of how well the model will perform on new, unseen data.\nCode\nalphas = [0, 1e-20, 1e-10, 1e-7, 1e-5, 1,10]\nd=15\nfor a in alphas:\n    plt.figure(figsize=(7, 7))\n    #plt.setp(ax, xticks=(), yticks=())\n    polynomial_features = PolynomialFeatures(degree=d,\n                                             include_bias=False)\n    linear_regression = LinearRegression()\n    linear_regression = Ridge(alpha=a)\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    #pipeline.fit(X[:, np.newaxis], y)\n    pipeline.fit(X_train[:, np.newaxis], y_train)\n    # Evaluate the models using crossvalidation\n    #scores = cross_validation.cross_val_score(pipeline,\n    #    X[:, np.newaxis], y, scoring=\"mean_squared_error\", cv=10)\n    \n    testing_score = pipeline.score(X_test[:, np.newaxis],y_test)\n    training_score = pipeline.score(X_train[:, np.newaxis],y_train)\n\n    X_plot = np.linspace(0, 1, 100)\n    plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label=\"Model\")\n    plt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n    plt.scatter(X, y, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    \n    plt.title(\"Degree {}, Alpha {}\\nTest score = {:.3f}\\nTraining score = {:.3f}\".format(\n        d, a, testing_score,training_score))\n    plt.show()\nThis is a simplified type of cross-validation often referred to as “Shuffle Splitting” and is one of the most common, but it is useful to review other types of cross-validation via this nice summary page from the SKLearn library, which covers a variety of important variants including:\nDiscussion Point: Under what conditions or situations would using each type of cross-validation above be appropriate versus inappropriate?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#getting-an-unbiased-estimate-of-out-of-sample-performance",
    "href": "notebooks/cross_validation_linear_regression.html#getting-an-unbiased-estimate-of-out-of-sample-performance",
    "title": "2  Evaluating Machine Learning Models",
    "section": "",
    "text": "K-Fold Cross Validation\nLeave-One-Out Cross Validation\nStratified Cross Validation\nGroup-wise Cross Validation\nTime Series Split Cross Validation",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#finding-the-optimal-hyper-parameters",
    "href": "notebooks/cross_validation_linear_regression.html#finding-the-optimal-hyper-parameters",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.2 Finding the Optimal Hyper-parameters",
    "text": "2.2 Finding the Optimal Hyper-parameters\nNow that we have introduced the usage of hyper-parameters and cross-validation, a natural question arises: How do we choose the hyper-parameters? There are many ways to do this, and this section will describe the most common and basic ones, while leaving more advanced techniques (like Implicit Differentiation) for later. Specifically, this section will: 1. Define the concepts of Grid and Random Hyper-parameter search. 2. Use Grid and Random search to optimize hyper-parameters of a model. 2. Distinguish when Randomized Search is much better than grid search. 3. Describe how Global Optimization procedures such as Bayesian Optimization work. 4. Recognize why none of those at all work in High Dimensions and describe the “Curse of Dimensionality”\nIn future chapters once we cover more advanced derivative methods, we can discuss how to use tools like Implicit Differentiation to directly compute the gradient of the cross-validation score with respect to hyper-parameters, and then use this gradient to optimize the hyper-parameters using standard gradient-based optimization methods. However, for now, let’s focus on more basic derivative-free methods, since they are more widely used and easier to understand.\nLet’s start by returning to our Polynomial example, and this time focus on finding the best combination of degree and penalty weight for a linear model.\n\n\nCode\n# from sklearn.preprocessing import PolynomialFeatures\n# from sklearn.pipeline import Pipeline\n# from sklearn.linear_model import Ridge\n# from sklearn.metrics import mean_squared_error\n\n\n\n\nCode\nfrom sklearn import model_selection\n\n# Let's plot the behavior of a fixed degree polynomial\ndegree = 15\n# (i.e., f(x) = w_1*x + w_2*x^2 + ... + w_15*x^15)\n# but where we change alpha.\nalphas = np.logspace(start=-13,stop=4,num=20)\npolynomial_features = PolynomialFeatures(degree=degree,\n                                         include_bias=False)\nscores = []\nfor a in alphas:\n    linear_regression = Ridge(alpha=a)\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    cv_scores = model_selection.cross_val_score(pipeline,\n        X[:,np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=20)\n    scores.append(cv_scores)\n\nscores = np.array(scores)\n\nplt.figure(figsize=(7,3))\nplt.semilogx(alphas,-np.mean(scores,axis=1),'-')\nplt.ylabel('Test MSE')\nplt.xlabel('Alpha ($\\\\alpha$)')\nsns.despine()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#what-if-we-have-more-than-one-variable",
    "href": "notebooks/cross_validation_linear_regression.html#what-if-we-have-more-than-one-variable",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.3 What if we have more than one variable?",
    "text": "2.3 What if we have more than one variable?\nLet’s look at both polynomial degree and regularization weight\n\n\nCode\nscores = []\nalphas = np.logspace(start=-13, # Start at 1e-13\n                     stop=4,    # Stop at 1e4\n                     num=40)    # Split that into 40 pieces\ndegrees = range(1,16) # This will only go to 15, due to how range works\n\nscores = np.zeros(shape=(len(degrees), # i.e., 15\n                         len(alphas))) # i.e., 20\n\nfor i, degree in enumerate(degrees): # For each degree\n    polynomial_features = PolynomialFeatures(degree=degree,\n                                             include_bias=False)\n    \n    for j,a in enumerate(alphas):    # For each alpha\n        linear_regression = Ridge(alpha=a)\n        pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                             (\"linear_regression\", linear_regression)])\n        cv_scores = model_selection.cross_val_score(pipeline,\n            X[:,np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=20)\n        scores[i][j] = -np.mean(cv_scores)\n\n\n\n\nCode\nfig = plt.figure(figsize=(10,7))\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nax = fig.add_subplot(111, projection='3d')\n\nXs, Ys = np.meshgrid(range(len(degrees)), range(len(alphas)))\nzs = np.array([scores[i,j] for i,j in zip(np.ravel(Xs), np.ravel(Ys))])\nZs = zs.reshape(Xs.shape)\n\nXs, Ys = np.meshgrid(degrees, np.log(alphas))\n\nax.plot_surface(Xs, Ys, Zs, rstride=1, cstride=1, cmap=cm.coolwarm,\n    linewidth=0, antialiased=False)\n\n# Label the Axes\nax.set_xlabel('Degree')\nax.set_ylabel('Regularization')\nax.set_zlabel('MSE')\n\n# Rotate the image\nax.view_init(30, # larger # goes \"higher\"\n             30) # larger # \"circles around\"\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(5,10))\nplt.imshow(Zs,\n           cmap=cm.coolwarm, # Allows you to set the color\n           vmin=Zs.min(), vmax=0.2, # The min and max Z-Values (for coloring purposes)\n           extent=[Xs.min(), Xs.max(),   # How far on X-Axis you want to plot\n                   Ys.min(), Ys.max()],  # How far on Y-Axis\n           interpolation='spline16',      # How do you want to interpolate values between data?\n           origin='lower')\nplt.title('Mean Squared Error')\nplt.xlabel('Degree')\nplt.ylabel('Regularization')\nplt.colorbar()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#optimization",
    "href": "notebooks/cross_validation_linear_regression.html#optimization",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.4 Optimization",
    "text": "2.4 Optimization\nAt the end of the day, all this is doing is optimization/search over different parameters.\nHow should we go about automating this?\nMost common: Grid Search.\n\n\nCode\nprint('parameters we could change:')\nfor k in pipeline.get_params().keys():\n    print(\" \",k)\n\n\nparameters we could change:\n  memory\n  steps\n  transform_input\n  verbose\n  polynomial_features\n  linear_regression\n  polynomial_features__degree\n  polynomial_features__include_bias\n  polynomial_features__interaction_only\n  polynomial_features__order\n  linear_regression__alpha\n  linear_regression__copy_X\n  linear_regression__fit_intercept\n  linear_regression__max_iter\n  linear_regression__positive\n  linear_regression__random_state\n  linear_regression__solver\n  linear_regression__tol\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'polynomial_features__degree': list(range(1,16)), # 15 possible\n              'linear_regression__alpha': np.logspace(start=-13,stop=4,num=10),\n              'polynomial_features__include_bias':[True, False]}\n\n\n\n\nCode\n# How do we want to do cross-validation?\nfrom sklearn import model_selection\nnum_data_points = len(y)\n\n# 4-fold CV\nkfold_cv = model_selection.KFold(n_splits = 4) \n\n# Or maybe you want randomized splits?\nshuffle_cv = model_selection.ShuffleSplit(n_splits = 20,     # How many iterations?\n                                          test_size=0.2    # What % should we keep for test?\n                                         )\n\n\n\n\nCode\nX=X[:,np.newaxis]\ngrid_search = GridSearchCV(pipeline,    # The thing we want to optimize\n                           parameters,  # The parameters we will change\n                           cv=shuffle_cv, # How do you want to cross-validate?\n                           scoring = 'neg_mean_squared_error'\n                          )\ngrid_search.fit(X, y) # This runs the cross-validation\n\n\nGridSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),\n             estimator=Pipeline(steps=[('polynomial_features',\n                                        PolynomialFeatures(degree=15,\n                                                           include_bias=False)),\n                                       ('linear_regression',\n                                        Ridge(alpha=np.float64(10000.0)))]),\n             param_grid={'linear_regression__alpha': array([1.00000000e-13, 7.74263683e-12, 5.99484250e-10, 4.64158883e-08,\n       3.59381366e-06, 2.78255940e-04, 2.15443469e-02, 1.66810054e+00,\n       1.29154967e+02, 1.00000000e+04]),\n                         'polynomial_features__degree': [1, 2, 3, 4, 5, 6, 7, 8,\n                                                         9, 10, 11, 12, 13, 14,\n                                                         15],\n                         'polynomial_features__include_bias': [True, False]},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),\n             estimator=Pipeline(steps=[('polynomial_features',\n                                        PolynomialFeatures(degree=15,\n                                                           include_bias=False)),\n                                       ('linear_regression',\n                                        Ridge(alpha=np.float64(10000.0)))]),\n             param_grid={'linear_regression__alpha': array([1.00000000e-13, 7.74263683e-12, 5.99484250e-10, 4.64158883e-08,\n       3.59381366e-06, 2.78255940e-04, 2.15443469e-02, 1.66810054e+00,\n       1.29154967e+02, 1.00000000e+04]),\n                         'polynomial_features__degree': [1, 2, 3, 4, 5, 6, 7, 8,\n                                                         9, 10, 11, 12, 13, 14,\n                                                         15],\n                         'polynomial_features__include_bias': [True, False]},\n             scoring='neg_mean_squared_error') best_estimator_: PipelinePipeline(steps=[('polynomial_features', PolynomialFeatures(degree=4)),\n                ('linear_regression',\n                 Ridge(alpha=np.float64(3.5938136638046257e-06)))]) PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=4) Ridge?Documentation for RidgeRidge(alpha=np.float64(3.5938136638046257e-06)) \n\n\n\n\nCode\ngrid_search.best_params_ # Once finished, you can see what the best parameters are\n\n\n{'linear_regression__alpha': np.float64(3.5938136638046257e-06),\n 'polynomial_features__degree': 4,\n 'polynomial_features__include_bias': True}\n\n\n\n\nCode\nprint(\"Best MSE for Grid Search: {:.2e}\".format(-grid_search.best_score_))\n\n\nBest MSE for Grid Search: 6.93e-03\n\n\n\n\nCode\ngrid_search.predict(X)  # You can also use the best model directly (in sklearn)\n\n\narray([ 0.92597995,  0.97112185,  0.97915236,  0.95343228,  0.94185444,\n        0.8235348 ,  0.80306679,  0.72117767,  0.65636685,  0.60679754,\n        0.57933728,  0.10937232,  0.05389104, -0.10522154, -0.34633312,\n       -0.43501599, -0.43622117, -0.44424987, -0.84518448, -0.88774347,\n       -0.98056731, -0.97399224, -0.96929348, -0.94126767, -0.78277808,\n       -0.54827761, -0.54221084, -0.54212267, -0.48316009, -0.20435414])\n\n\n\n\nCode\nbest_degree = grid_search.best_params_['polynomial_features__degree']\nbest_alpha = grid_search.best_params_['linear_regression__alpha']\nX_plot = X_plot[:,np.newaxis]\nplt.figure(figsize=(7, 7))\nplt.plot(X_plot, grid_search.predict(X_plot),'-',label=\"Model\",alpha=0.5)\nplt.plot(X_plot, true_fun(X_plot), ':',label=\"True function\",alpha=1)\nplt.scatter(X,y, c='Blue', s=20, edgecolors='none')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xlim((0, 1))\nplt.ylim((-2, 2))\nsns.despine()\nplt.title(\"Degree {}, Alpha {:.1e}\".format(best_degree,best_alpha))\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#randomized-search",
    "href": "notebooks/cross_validation_linear_regression.html#randomized-search",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.5 Randomized Search",
    "text": "2.5 Randomized Search\nIn reality, grid search is wasteful and not easy to control. A better (and still easy way) is to randomize the search.\n\n\nCode\n# Now, instead of specifying exact which points to test, we instead\n# have to specify a distribution to sample from.\n# For example, things from http://docs.scipy.org/doc/scipy/reference/stats.html\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import lognorm as sp_lognorm\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nparameters = {'polynomial_features__degree': sp_randint(1,20), # We want an integer\n              'linear_regression__alpha': sp_lognorm(1),\n              'polynomial_features__include_bias':[True, False]} # Selecting from two is fine\n\n\nNeed something whose logarithmic distribution we can control. How about a lognormal? \\[\n\\mathcal{N}(\\ln x;\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left[-\\frac {(\\ln x - \\mu)^2} {2\\sigma^2}\\right].\n\\]\n\n\nCode\nsigma=6\nrv = sp_lognorm(sigma,scale=1e-7)\n\nplt.figure()\nplt.hist(rv.rvs(size=1000),bins=np.logspace(-20, 2, 22))\nplt.xscale('log')\nplt.xlabel('x')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nparameters = {'polynomial_features__degree': sp_randint(1,20), # We want an integer\n              'linear_regression__alpha': sp_lognorm(sigma,scale=1e-7),\n              'polynomial_features__include_bias':[True, False]} # Selecting from two is fine\n\n\n\n\nCode\n# Fitting the high degree polynomial makes the linear system almost\n# singular, which makes Numpy issue a Runtime warning.\n# This is not a problem here, except that it pops up the warning box\n# So I will disable it just for pedagogical purposes\nimport warnings\nwarnings.simplefilter('ignore',RuntimeWarning)\n\n# specify parameters and distributions to sample from\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# run randomized search\n#n_iter_search = 300 # How many random parameter settings should we try?\nn_iter_search = len(grid_search.cv_results_['params']) # Give it same # as grid search, to be fair\nrandom_search = RandomizedSearchCV(pipeline,\n                                   param_distributions=parameters,\n                                   n_iter=n_iter_search, \n                                   cv=shuffle_cv, # How do you want to cross-validate?\n                                   scoring = 'neg_mean_squared_error')\nrandom_search.fit(X, y)\n\n\nRandomizedSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),\n                   estimator=Pipeline(steps=[('polynomial_features',\n                                              PolynomialFeatures(degree=15,\n                                                                 include_bias=False)),\n                                             ('linear_regression',\n                                              Ridge(alpha=np.float64(10000.0)))]),\n                   n_iter=300,\n                   param_distributions={'linear_regression__alpha': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000023215096120&gt;,\n                                        'polynomial_features__degree': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000002320FEBFF80&gt;,\n                                        'polynomial_features__include_bias': [True,\n                                                                              False]},\n                   scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),\n                   estimator=Pipeline(steps=[('polynomial_features',\n                                              PolynomialFeatures(degree=15,\n                                                                 include_bias=False)),\n                                             ('linear_regression',\n                                              Ridge(alpha=np.float64(10000.0)))]),\n                   n_iter=300,\n                   param_distributions={'linear_regression__alpha': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000023215096120&gt;,\n                                        'polynomial_features__degree': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000002320FEBFF80&gt;,\n                                        'polynomial_features__include_bias': [True,\n                                                                              False]},\n                   scoring='neg_mean_squared_error') best_estimator_: PipelinePipeline(steps=[('polynomial_features', PolynomialFeatures(degree=4)),\n                ('linear_regression',\n                 Ridge(alpha=np.float64(3.3921813538938043e-06)))]) PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=4) Ridge?Documentation for RidgeRidge(alpha=np.float64(3.3921813538938043e-06)) \n\n\n\n\nCode\nrandom_search.best_params_ # Once finished, you can see what the best parameters are\n\n\n{'linear_regression__alpha': np.float64(3.3921813538938043e-06),\n 'polynomial_features__degree': 4,\n 'polynomial_features__include_bias': True}\n\n\n\n\nCode\nprint(\"Best MSE for Random Search: {:.2e}\".format(-random_search.best_score_))\n\n\nBest MSE for Random Search: 7.48e-03\n\n\n\n\nCode\nbest_degree = random_search.best_params_['polynomial_features__degree']\nbest_alpha = random_search.best_params_['linear_regression__alpha']\n\nplt.figure(figsize=(7, 7))\nplt.plot(X_plot, random_search.predict(X_plot),'-',label=\"Model\",alpha=0.5)\nplt.plot(X_plot, true_fun(X_plot), ':',label=\"True function\",alpha=1)\nplt.scatter(X,y, c='Blue', s=20, edgecolors='none')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xlim((0, 1))\nplt.ylim((-2, 2))\nsns.despine()\nplt.title(\"Degree {}, Alpha {:.1e}\".format(best_degree,best_alpha))\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#global-bayesian-optimization",
    "href": "notebooks/cross_validation_linear_regression.html#global-bayesian-optimization",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.6 Global Bayesian Optimization",
    "text": "2.6 Global Bayesian Optimization\nSurely, since we are essentially doing optimization, we could approach hyper-parameter selection as an optimization problem as well, right?\nEnter techniques like Global Bayesian Optimization below:\n\n\nCode\ndef f(x):\n    \"\"\"The function to predict.\"\"\"\n    return x * np.sin(x)\n    # Try others!\n    #return 5 * np.sinc(x)\n    #return x\n    \nX = np.atleast_2d(np.linspace(0, 10, 200)).T\n\n# Observations\ny = f(X).ravel()\n\nplt.figure()\nplt.plot(X,y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n########################################################\n# This is just a helper function, no need to worry about\n# The internals.\n# We will return to this example in Week 14\n########################################################\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nnp.random.seed(1)\n\n# Mesh the input space for evaluations of the real function, the prediction and\n# its MSE\nx = np.atleast_2d(np.linspace(0, 10, 1000)).T\n\n# Create a Gaussian Process model\n#kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\nkernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n#gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\nkernel = C(3.0)*RBF(1.5)\ngp = GaussianProcessRegressor(kernel=kernel,alpha=1e-6,optimizer=None)\n#gp = GaussianProcess(corr='cubic', theta0=1e-2, thetaL=1e-4, thetaU=1e-1,random_start=100)\n\n# Now, ready to begin learning:\ntrain_ind ={\n    'Upper CB':   np.zeros(len(X),dtype=bool),\n    'Random':np.zeros(len(X),dtype=bool)\n}\noptions = train_ind.keys()\n\npossible_points = np.array(list(range(len(X))))\n# Possible Initialization options\n# 1. Select different points randomly\n#for i in range(2):\n#    for o in options:\n#        ind = np.random.choice(possible_points[~train_ind[o]],1)\n#        train_ind[o][ind] = True\n\n# 2. Start with end-points\n#for o in options:\n#    train_ind[o][0] = True\n#    train_ind[o][-1] = True\n\n# 3. Start with same random points\nfor ind in np.random.choice(possible_points,2):\n    for o in options:\n        train_ind[o][ind] = True\n\nplot_list = np.array([5,10,20,30,40,50,len(X)])\nfor i in range(10):\n    # As i increases, we increase the number of points\n    plt.figure(figsize=(16,6))\n    for j,o in enumerate(options):\n        plt.subplot(1,2,j+1)\n        gp.fit(X[train_ind[o],:],y[train_ind[o]])\n        yp,sigma = gp.predict(X[~train_ind[o],:], return_std=True)\n        ucb = yp + 1.96*sigma\n        if o == 'Upper CB':\n            #candidates = np.extract(MSE == np.amax(MSE),X[~train_ind[o],:])\n            candidates = np.extract(ucb == np.amax(ucb),X[~train_ind[o],:])\n            next_point = np.random.choice(candidates.flatten())\n            next_ind = np.argwhere(X.flatten() == next_point)\n        elif o == 'Random':\n            next_ind = np.random.choice(possible_points[~train_ind[o]],1)\n        train_ind[o][next_ind] = True\n        \n        # Plot intermediate results\n        yp,sigma = gp.predict(x, return_std=True)\n        plt.fill(np.concatenate([x, x[::-1]]),\n                np.concatenate([yp - 1.9600 * sigma,\n                               (yp + 1.9600 * sigma)[::-1]]),'b',\n                alpha=0.05,  ec='g', label='95% confidence interval')\n    \n        n_train = np.count_nonzero(train_ind[o])\n\n        gp.fit(X[train_ind[o],:],y[train_ind[o]])\n        # Show progress\n        yp,sigma = gp.predict(x, return_std=True)\n        yt = f(x)\n        error = np.linalg.norm(yp-yt.flatten())\n\n        plt.fill(np.concatenate([x, x[::-1]]),\n                np.concatenate([yp - 1.9600 * sigma,\n                               (yp + 1.9600 * sigma)[::-1]]),'b',\n                alpha=0.3,  ec='None', label='95% confidence interval')\n        \n        plt.plot(x,yt,'k--',alpha=1)\n        plt.plot(x,yp,'r-',alpha=1)\n        plt.scatter(X[train_ind[o],:],y[train_ind[o]],color='g',s=100)\n        plt.scatter(X[next_ind,:].flatten(),y[next_ind].flatten(),color='r',s=150)\n        plt.ylim([-10,15])\n        plt.xlim([0,10])\n        plt.title(\"%s\\n%d training points\\n%.2f error\"%(o,n_train,error))\n    plt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#the-curse-of-dimensionality",
    "href": "notebooks/cross_validation_linear_regression.html#the-curse-of-dimensionality",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.7 The Curse of Dimensionality",
    "text": "2.7 The Curse of Dimensionality\nDiscuss on board examples of the Curse of Dimensionality and how it affects algorithms dependent on calculating distances.\n\nSpace-filling properties of inscribed hyper-cube\nDistance ratio between min and max distances\nEffects on nearest neighbor graphs\nEffects on Gaussian Density\n\n\n\nCode\nfrom math import gamma\nV_sphere = lambda d: np.pi**(d/2.0)\nV_cube = lambda d: d*2**(d-1)*gamma(d/2.0)\nvolume_ratio = lambda d: V_sphere(d)/V_cube(d)\n\nd = range(2,50)\nratio = [volume_ratio(i) for i in d]\nplt.figure(figsize=(10,10))\nplt.plot(d,ratio)\nplt.semilogy(d,ratio)\nplt.ylabel(\"Ratio of Hyper-Sphere Vol. to Hyper-Cube Vol.\")\nplt.xlabel(\"Number of Dimensions\")\nplt.show()\n\n# TODO: Add distance min/max example",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/supervised_linear_models.html",
    "href": "notebooks/supervised_linear_models.html",
    "title": "3  Introduction to Gradient Descent",
    "section": "",
    "text": "3.1 Experiment:\nWe will now review Linear Regression from the standpoint of Gradient Descent (instead of the normal equations), so as to build our intuition about how Gradient Descent works, and also introduce the concept of Stochastic Gradient Descent (SGD).\nLet’s first set up our notation for the problem: \\[\ny = w\\cdot x + b + \\epsilon\n\\] Or, if we consider \\(x = [1, x]\\) then: \\[\ny = \\mathbf{w^T\\cdot x} + \\epsilon\n\\]\nFrom your earlier statistics classes, you likely learned how to solve for the linear regression weights using the Normal Equations: \\[\n\\hat{w} = (X^T X)^{-1}X^T y\n\\]\nThere are ways of solving the normal equations directly without needing to take the inverse (such as using the Cholesky decomposition), however today we are going to focus on a different kind of solver that has more broader applications: Gradient Descent and it’s cousin Stochastic Gradient Descent (SGD).\nWe first need to start with some sort of Cost function that we wish to minimize. In general, we will consider costs of the form: \\[\nLoss = Error + \\alpha\\cdot Penalty\n\\]\nSpecifically for Linear Models, we will talk about costs (which I’ll call \\(J\\)) of the form:\n\\[\nJ(w,X) = \\frac{1}{N}\\Sigma_{i=1}^N \\left(\\mathbf{y}_i - f(\\mathbf{w},\\mathbf{x}_i)\\right)^2 + \\alpha\\cdot\\Omega(\\mathbf{w})\n\\]\nwhere for Linear Models \\(f(w,X) = \\mathbf{w\\cdot X}\\), so that our overall cost becomes:\n\\[\nJ(w,X) = \\frac{1}{N}\\Sigma_{i=1}^N \\left(\\mathbf{y}_i - \\mathbf{w\\cdot \\mathbf{x}_i}\\right)^2 + \\alpha\\cdot\\Omega(\\mathbf{w})\n\\]\nWe’ll consider the no-penalty case (\\(\\alpha=0\\)), so that our loss is just:\n\\[\nJ(w,X) = \\frac{1}{N}\\Sigma_{i=1}^N \\left(\\mathbf{y}_i - \\mathbf{w\\cdot \\mathbf{x}_i}\\right)^2\n\\]\nLet’s plot this cost as a function of the line slope, just to get an idea of what it looks like:\nWhile it might seem clear to us, visually, where the lowest cost is, actually finding this point automatically via a computer with minimal effort is another story. This is essentially what the field of Optimization tries to do. One simple (but powerful) method of optimization is Gradient Descent. It works by taking a (possibly random) starting point (e.g., w=60), and then computing the gradient of the function at that point. Since gradients will point upwards, and we want to minimize the cost, we will instead walk in the negative gradient direction, which should move us closer to the bottom. Let’s see this on an example, by computing the gradient of our cost function above with respect to the slope (w):\n\\[\n\\begin{aligned}\n\\frac{\\partial J}{\\partial w} &=& \\frac{\\partial}{\\partial w} \\left( \\frac{1}{N}\\Sigma_{i=1}^N \\left(\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i\\right)^2 \\right) \\\\\n&=&\\frac{1}{N}\\Sigma_{i=1}^N  \\frac{\\partial}{\\partial w} \\left(\\left(\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i\\right)^2 \\right) \\\\\n&=&\\frac{2}{N}\\Sigma_{i=1}^N (\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i) \\frac{\\partial}{\\partial w} \\left(\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i \\right) \\\\\n&=&-\\frac{2}{N}\\Sigma_{i=1}^N (\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i) \\cdot \\mathbf{x}_i\n\\end{aligned}\n\\]\nLet’s plot this:\nOnce we have 1) a starting point, and 2) the gradient at a point, the idea with gradient descent is to take a small step (\\(\\alpha\\)) in the direction of the negative gradient:\n\\[\nw_{t+1} = w_t - \\alpha \\frac{\\partial J}{\\partial w}\n\\]\nNote: here we are just considering a single parameter (the slope, w), but this method extends to multiple parameters (\\(\\mathbf{\\theta}\\)), via the gradient operator:\n\\[\n\\mathbf{\\theta}_{t+1} = \\mathbf{\\theta}_t - \\alpha \\nabla_\\theta J\n\\]\nTry modifying the initial guess wg and the step size alpha and re-running the below cells. What do you observe?\n######################\n# Try changing the below\nwg = 80 # Initial guess at slope; Try changing this\nalpha = 0.1  # Try changing alpha (both big and small)\n# What do you notice?\n###########################\nnum_steps = 20 # Take 20 steps\nweights = np.zeros(num_steps)\nweights[0] = wg  # Set the initial weight\nfor i in range(1,num_steps): \n    weights[i] = grad_step(weights[i-1], X, alpha)\nprint(\"Final weight from Gradient Descent is {:.2f}\".format(weights[i]))\nprint(\"Compared to {:.2f} from the Normal Equations\".format(wn))\n\nFinal weight from Gradient Descent is 43.06\nCompared to 42.57 from the Normal Equations\nweight_cost = [loss(w) for w in weights]\nplt.figure(figsize=(10,10))\nplt.plot(wp,cost)\nplt.scatter(weights,weight_cost,facecolors='none', edgecolors='r',linewidth=1)\nax = plt.gca()\nfor i,w in enumerate(weights):\n    ax.annotate('{}'.format(i), xy=(w, weight_cost[i]-10), \n                xytext=(w+1e-8, 10+50*np.random.rand()),\n                ha='center',fontsize=8,\n                arrowprops=dict(facecolor='white', edgecolor='grey',\n                                shrink=0.05,\n                            width=1, headwidth=1)\n               )\nplt.ylabel('Cost')\nplt.xlabel('slope (w)')\nplt.show()\n# Plot how the weights progress\nplt.figure(figsize=(10,5))\nplt.plot(range(len(weights)),weights, label='GD')\nplt.hlines(wn, 0, len(weights), \n           label = \"Optimal Weight\", \n           color='k', linestyle=\"--\")\nplt.xlabel('Gradient Descent Iteration')\nplt.ylabel('Weight')\nplt.legend()\nplt.show()\n# Copying for comparison later\ngd_weights = weights",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Gradient Descent</span>"
    ]
  },
  {
    "objectID": "notebooks/supervised_linear_models.html#experiment-1",
    "href": "notebooks/supervised_linear_models.html#experiment-1",
    "title": "3  Introduction to Gradient Descent",
    "section": "4.1 Experiment:",
    "text": "4.1 Experiment:\nWhat do you notice about the behavior of SGD? What happens when alpha is small vs large? What happens when you take multiple passes through the data? If I keep doing more passes, do I eventually converge?\nWhat you are seeing is a result of Stochastic Approximation (trying to approximate a gradient of a function using noisy estimates of that gradient (where the noise here comes from evaluating the gradient using only one data point).\nThis behavior was studied by multiple people in the 1950s and 60s, with one key result coming Herbert Robbins and Sutton Monro, in what is now called the Robbins-Monro Algorithm. The central idea is rather than defining a single step size, we should let the step size decrease over time. Initially, we need to move the weights a lot, but as we get closer to the goal, they exert less influence so that we settle at some point. What they showed was that SGD would converge to the right estimator so long as the sequence satisfies the following properties:\n\\[\n\\begin{aligned}\n\\sum_{n=1}^{\\infty} a_n &= \\infty \\\\\n\\sum_{n=1}^{\\infty} a_n^2 &&lt; \\infty\n\\end{aligned}\n\\]\nFor sequences where \\(a_n&gt;0~\\forall~n&gt;0\\), Robbins and Monro recommended the \\(a_n = a/n\\), however this rate is based on some assumptions about smoothness and convexity which sometimes don’t work well in practice. People generally use decay rates on the order of \\(O(1/\\sqrt(n))\\), however there are entire fields of researchers working on this “optimal step size” problem for SGD and there are many great alternative procedures out there if you know certain things about the function (e.g., can compute things like hessians, etc.)\n\n#################################\n# Try Changing the below\nwg = 80 # Initial guess at slope\nalpha_i = 0.5  # Initial Step Size\n#alpha = lambda n: alpha_i\n#alpha = lambda n: alpha_i/n\nalpha = lambda n: alpha_i/np.sqrt(n)\nnum_passes = 5  # Number of times we pass through the data\nshuffle_after_pass = True  # Whether to shuffle the data\n##########################\n# What do you find?\n\nN = len(y)\nweights = np.zeros(N*num_passes+1)\nk=0\nweights[k] = wg  # Set the initial weight\nprint('Initial weight: ',weights[0])\n\nindex = list(range(N))\nfor n in range(num_passes):\n    if shuffle_after_pass:\n        np.random.shuffle(index)\n    for i in index:\n        k+=1\n        xi = X[i,0]\n        yi = y[i]\n        weights[k] = weights[k-1] - alpha(k)*sgd_dloss(weights[k-1],yi,xi)\nprint('Final Weight from SGD: {:.2f}'.format(weights[-1]))\nprint(\"Compared to {:.2f} (Normal Equations)\".format(wn))\n\nplt.figure(figsize=(10,5))\n# Plot SGD Weights\nplt.plot(range(len(weights)), weights,\n         marker=None,\n         label = 'SGD')\n# Plot Grad. Descent Weights\nplt.plot(np.array(range(len(gd_weights)))*len(index),\n         gd_weights,\n         marker='o',\n         label = 'GD')\n# Plot True Answer\nplt.hlines(wn, 0, len(weights), \n           label = \"Optimal Weight\", \n           color='k', linestyle=\"--\")\nplt.xlabel('# Samples Used')\nplt.ylabel('Weight')\n#plt.ylim([30,55])\nplt.xlim([0,len(weights)])\nplt.legend()\nplt.show()\n\nInitial weight:  80.0\nFinal Weight from SGD: 41.30\nCompared to 42.57 (Normal Equations)\n\n\nC:\\Users\\mafuge\\AppData\\Local\\Temp\\ipykernel_5008\\3561310644.py:2: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  return float(-2*(y-w*x)*x)\n\n\n\n\n\n\n\n\n\nWhile Gradient Descent looks much better than SGD here, let’s now scale the axis by the number of model evaluations needed:\n\nplt.figure(figsize=(10,5))\nplt.plot(range(len(weights)), weights,\n         label = 'SGD')\nplt.plot(np.array(range(len(gd_weights)))*len(index),\n         gd_weights,\n         marker='o',\n         label = 'GD')\nplt.hlines(wn, 0, len(weights), \n           label = \"Optimal Weight\", \n           color='k', linestyle=\"--\")\n#plt.ylim([35,50])\nplt.xlim([0,len(weights)])\nplt.xlabel('# Samples Used')\nplt.ylabel('Weight')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nnp.array(range(len(gd_weights)))*len(index)\n\narray([   0,  100,  200,  300,  400,  500,  600,  700,  800,  900, 1000,\n       1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900])",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Gradient Descent</span>"
    ]
  },
  {
    "objectID": "notebooks/supervised_linear_models.html#existing-implementations",
    "href": "notebooks/supervised_linear_models.html#existing-implementations",
    "title": "3  Introduction to Gradient Descent",
    "section": "4.2 Existing Implementations",
    "text": "4.2 Existing Implementations\nSGD is a fairly simple and popular technique for solving many problems where you can easily express the derivatives of those functions. For certain types of loss function (like the square error/L2 norm we discussed above), many folks have written optimized libraries for just that purpose, such as Scikit-Learn’s SGD functions including SGDRegressor.\nFor reference, the SGD Regressor in ScikitLearn uses an update rule similar to: \\[\n\\eta^{(t)} = \\frac{eta_0}{t^{power_t}}\n\\]\n\nfrom sklearn.linear_model import SGDRegressor\nsgd = SGDRegressor(loss = 'squared_error',\n                   eta0 = 0.01,  # Initial Learning rate/step size\n                   power_t = 0.25, # how quickly sould eta decay?\n                   max_iter = 100,  # Max # of passes to do over the data?\n                   tol = 1e-3,     # Tolerance for change in loss\n                   fit_intercept=False # Not worrying about b term in w*x+b\n                  )\n# Here eta = eta0/(t^power_t) where t is the iteration\nX = np.asarray(X)\ny = np.array(y).reshape(len(y),) # Reshape y so that scikit doesn't complain\nsgd.fit(X,y)\nprint('Final Weight from SKLearn SGD: {:.2f}'.format(sgd.coef_[0]))\nprint(\"Compared to {:.2f} (Normal Equations)\".format(wn))\n\nFinal Weight from SKLearn SGD: 42.55\nCompared to 42.57 (Normal Equations)\n\n\n\nplt.figure(figsize=(10,5))\nplt.scatter(np.asarray(X).ravel(),\n            np.asarray(y).ravel(),\n            color='k'\n           )\n#plt.scatter(X,y)\nXp = [[-3],[3]]\nplt.plot(Xp,sgd.predict(Xp),label='SGD')\nplt.plot([-3,3],[-3*wn, 3*wn],\n         label='Normal Eqns',\n         linestyle='--' )\nplt.legend()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Gradient Descent</span>"
    ]
  },
  {
    "objectID": "notebooks/supervised_linear_models.html#advanced-techniques",
    "href": "notebooks/supervised_linear_models.html#advanced-techniques",
    "title": "3  Introduction to Gradient Descent",
    "section": "4.3 Advanced Techniques",
    "text": "4.3 Advanced Techniques\nThere are a variety of more advanced SGD techniques, most of which involve one or more of the following tricks: 1. Using “acceleration” procedures that leverage “momentum” of some type. You can read more about this phenomenon at: “Why Momentum Really Works” 2. “Batching” the SGD updates: that is, taking steps that are considering \\(N&gt;n&gt;1\\) in size (e.g., averaging the gradients of, say, 5 data points before taking a step). This can help stablize gradients and improve convergence. 3. “Normalizing” the gradient updates: that is, re-scaling the gradient updates at each step to achieve better convergence. This is commonly used in Neural Networks for things like Batch Normalization (Wikipedia) (or, for a more advanced introduction, you can read the NeurIPS paper Understanding Batch Normalization).",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Gradient Descent</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html",
    "href": "part1/linear_decompositions.html",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "",
    "text": "4.1 Principal Component Analysis\nIn this notebook, we will briefly review some of the key concepts of linear unsupervised learning, including Principal Component Analysis (PCA) and the effects of regularizations on linear decompositions, such as SparsePCA and Non-negative Matrix Factorization (NMF). We will use a set of airfoil geometries to demonstrate these effects visually.\nFirst, let’s load some airfoil geometry coordinates, take a look at the shape of the data matrix, and pick a random one to visualize:\nGreat, as we can see there are 1528 airfoils, expressed as 192 surface coordinates each with an x and y value. We can turn this into a matrix compatible with a linear decomposition by flattening the last two dimensions, so that each airfoil is a row vector of length 384 (2x192).\nNow let’s demonstrate how to use various dimension reduction algorithms on this example.\nMathematically, given centered data \\(X \\in R^{n×d}\\), PCA finds k orthonormal components that best reconstruct the data in least-squares sense. One convenient formulation is\n\\[\n\\min_{W,Z}\\;\\|X - Z W\\|_{F}^{2}\\quad\\text{s.t. }Z=XW^{T},\\;W W^{T}=I.\n\\]\nEquivalently PCA maximizes the projected variance:\n\\[\n\\max_{W:\\;W W^{T}=I} \\;\\mathrm{tr}(W \\Sigma W^{T}),\\quad \\Sigma=\\frac{1}{n}X^{T}X.\n\\]\nNotes:\nfrom sklearn import decomposition\n\n# We can set the maximum number of components that we want to truncate to\n# Or can just leave it as None to get all components\nn_components = 20\nestimator = decomposition.PCA(n_components=n_components)\nZ_pca = estimator.fit_transform(data)\ncomponents_ = estimator.components_\nWe see that we now possess a matrix (i.e., linear operator) that goes from the target 20 components/dimensions back to the original 382 dimensions.\nprint(f\"The shape of the components_ matrix (W) is {components_.shape}\")\n\nThe shape of the components_ matrix (W) is (20, 384)\nIf we wanted to visualize how each of these components looks like in terms of the original airfoil coordinates, we can reshape each row of the components matrix back to the original airfoil shape:\ncomponents_.reshape((n_components, -1, 2)).shape\n\n(20, 192, 2)\nLet’s go ahead and visualize the first all of the learned components:\nCode\n# Component heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(components_, aspect='auto', cmap='RdBu_r', interpolation='nearest', \n           norm=TwoSlopeNorm(vmin=components_.min(), vcenter=0, vmax=components_.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Feature index ($d \\\\in {Z_pca.shape[0]}$)\")\nplt.ylabel(f\"Latent index ($k \\\\in {Z_pca.shape[1]}$)\")\nplt.title('PCA Component Matrix (latent codes x original features)')\nplt.show()\nWe notice a kind of alternating aliasing pattern in the components, but recall, this is because of how we reshaped the original data, which had rows of x and y coordinates interleaved. To make this clearer, we can visually re-order the indices of the components so that all the x-coordinates come first (first 192 features), followed by all the y-coordinates (second 192 features):\nCode\n# make an array with all of the odd indices of components_\ndef reorder_indices(components):\n    return np.hstack([components[:,0:-1:2],components[:,1:-1:2]])\n\n# Component heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(reorder_indices(components_), aspect='auto', cmap='RdBu_r', interpolation='nearest', \n           norm=TwoSlopeNorm(vmin=components_.min(), vcenter=0, vmax=components_.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Feature index ($d \\\\in {Z_pca.shape[0]}$)\")\nplt.ylabel(f\"Latent index ($k \\\\in {Z_pca.shape[1]}$)\")\nplt.title('PCA Component Matrix (latent codes x original features) - Reordered')\nplt.show()\nOK, this now looks a little clearer. What do you notice?\nWe can next visualize how each data point is mapped to all of the latent coordinates:\nCode\n# Latent Coordinate heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(Z_pca, aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=Z_pca.min(), vcenter=0, vmax=Z_pca.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Latent index ($k \\\\in {Z_pca.shape[1]}$)\")\nplt.ylabel(f\"Data Point index ($n \\\\in {Z_pca.shape[0]}$)\")\nplt.title('Latent codes for each Data Point')\nplt.show()\nWhy do you think the first few components are more important than the later ones? We can gain some intuition here by looking at the amount of variance explained by each component:\nCode\nplt.figure()\nplt.plot(estimator.explained_variance_)\nplt.ylabel(\"Explained Variance\")\nplt.xlabel(\"Latent Dimension\")\nplt.title(\"PCA Explained Variance\")\nplt.xticks(np.arange(n_components))\nplt.show()\nWe can also visualize the cumulative explained variance to see how many components are needed to explain a certain amount of variance in the data. For example, we can plot a line at the number of dimensions we need to keep to explain 99% of the variance:\nCode\ncumulative_explained_var_ratio = np.cumsum(estimator.explained_variance_ratio_)\nnumber_of_vars_to_99 = np.argmax(cumulative_explained_var_ratio&gt;.99)\nplt.figure()\nplt.plot(cumulative_explained_var_ratio,label=\"Cumulative Explained Variance\")\nplt.ylabel(\"Explained Variance\")\nplt.xlabel(\"Latent Dimension\")\nplt.xticks(np.arange(n_components))\nplt.vlines(number_of_vars_to_99,\n           np.min(cumulative_explained_var_ratio),1.0,\n           colors=\"k\",linestyles='dashed',\n          label = \"99% Explained Variance\")\nplt.legend()\nplt.title(\"PCA Cumulative Explained Variance\")\nplt.show()\nOr plot the explained variance ratio as a function of the number of components, which tells us how much each additional component contributes to the total explained variance:\nCode\nplt.figure()\nplt.plot(estimator.explained_variance_ratio_)\nplt.ylabel(\"Explained Variance Ratio\")\nplt.xlabel(\"Latent Dimension\")\nplt.xticks(np.arange(n_components))\nplt.title(\"PCA Explained Variance Ratio\")\nplt.show()\nWe can see that this approximately follows the singular values of the data matrix:\nCode\nprint(f\"Singular Values: {estimator.singular_values_}\")\nplt.figure()\nplt.plot(estimator.singular_values_)\nplt.ylabel(\"Singular Values\")\nplt.xlabel(\"Latent Dimension\")\nplt.xticks(np.arange(n_components))\nplt.title(\"PCA Singular Values\")\nplt.show()\n\n\nSingular Values: [7.96347431 7.30793278 2.11271967 1.880563   1.43978062 1.06857152\n 0.86184436 0.79754077 0.46006406 0.40013427 0.3503857  0.33218949\n 0.28107742 0.20708862 0.19063632 0.17582911 0.14392864 0.1288838\n 0.12505431 0.10367465]\nMoving beyond just the singular values, we can also now look at the projection of each airfoil into any of the latent dimensions. It is natural to explore the first two, since those explain the largest variance:\nCode\nz = estimator.transform(data)\nplt.figure()\nplt.scatter(z[:,0],z[:,1],alpha=0.3)\nplt.xlabel(\"1st Principal Component\")\nplt.ylabel(\"2nd Principal Component\")   \nplt.title(\"PCA Projection onto First Two Principal Components\")\nplt.show()\nWe can also pull out the “latent code/coordinates/vector” for an individual airfoil:\nz[airfoil_id,:]\n\narray([ 0.26565954, -0.30692116,  0.18797174, -0.0349173 , -0.0307379 ,\n        0.02908489, -0.02403219,  0.02460046, -0.01230913, -0.0146706 ,\n        0.0124125 ,  0.00509173, -0.01270168,  0.00192036, -0.00573131,\n        0.00744313,  0.00506638,  0.00798497,  0.01136562,  0.00033123])",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#principal-component-analysis",
    "href": "part1/linear_decompositions.html#principal-component-analysis",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "",
    "text": "There is no explicit L1/L2 penalty in vanilla PCA; the constraint \\(W W^{T}=I\\) enforces orthonormality of components.\nThe analytical solution is given by the top-k eigenvectors of the covariance (or the top-k left/right singular vectors from SVD).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.1 How does the # Components affect the Airfoil Reconstruction?\nWe can see visually the effect of including an increasing number of components by looking at how this affects the reconstruction of a single airfoil:\n\n\nCode\nfor n_components in range(2,10):\n    estimator = decomposition.PCA(n_components=n_components, whiten=False)\n    \n    # Train the model\n    estimator.fit(data)\n    # Project down to the low dimensional space\n    z     = np.dot(data - estimator.mean_, estimator.components_.T)\n    # Re-Project back to the high dimensional space\n    x_hat = np.dot(z[0], estimator.components_) + estimator.mean_\n\n    # Now plot them\n    airfoil_original = make_airfoil(data[0])\n    airfoil_reconstructed = make_airfoil(x_hat)\n    airfoil_mean = make_airfoil(estimator.mean_)\n    plt.figure()\n    plt.plot(airfoil_original[:,0],\n             airfoil_original[:,1],\n             alpha=0.75,\n             label = 'Original')\n    plt.plot(airfoil_reconstructed[:,0],\n                airfoil_reconstructed[:,1],\n                alpha=0.5,\n                label='Reconstructed')\n\n    plt.title(f\"Using {n_components} Number of Components\")\n    plt.legend()\n    plt.axis('equal')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n# Interactive top-k overlay for contribution\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=1, min=1, max=9, step=1, description='top_k')\n    def show_topk(k=4):\n        # Find the first k principal components and multiply them with the corresponding latent variables\n        contrib = np.dot(z[airfoil_id,:k], components_[:k,:])+ estimator.mean_\n        plt.figure(figsize=(6,3))\n        plt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\n        plt.plot(make_airfoil(contrib)[:,0], make_airfoil(contrib)[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '-', color='g', label='Mean')\n        plt.axis('equal')\n        plt.title(f'Top-{k} component contributions (overlay)')\n        plt.legend()\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_topk, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n\n\n\n\n\n\nCode\n# Interactive visualization of each component\nif widgets is not None:\n    component_selector = widgets.IntSlider(value=1, min=1, max=n_components, step=1, description='component')\n    def show_component(k=4):\n        # Find the first k principal components and multiply them with the corresponding latent variables\n        #contrib = np.dot(z[airfoil_id,:k], components_[:k,:])+ estimator.mean_\n        plt.figure(figsize=(6,5))\n        selected_component = components_[k-1,:]+ estimator.mean_\n        plt.plot(make_airfoil(selected_component)[:,0], make_airfoil(selected_component)[:,1], label='Component', alpha=1.0)\n        plt.axis('equal')\n        plt.title(f'Component {k}')\n        plt.xlim(-0.6,0.6)\n        plt.show()\n    display(widgets.VBox([component_selector, widgets.interactive_output(show_component, {'k': component_selector})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#sparse-pca",
    "href": "part1/linear_decompositions.html#sparse-pca",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "4.2 Sparse PCA",
    "text": "4.2 Sparse PCA\nSparse PCA encourages components that have many zeros, which can make the learned components easier to interpret as localized shape features. Below we fit a SparsePCA model and visualize a small set of sparse components and their effect on reconstructing an example airfoil.\nMathematically, Sparse PCA augments a PCA-style reconstruction loss with an L1 penalty on the (unconstrained) components to encourage sparsity. A common optimization is:\n\\[\\min_{Z,W}\\;\\|X - ZW\\|_{F}^{2} + \\alpha\\|W\\|_{1}\\quad\\text{s.t. }\\|Z_{i}\\|_{2}^{2}\\le 1\\;\\forall i,\\,\\]\nwhere \\(W \\in R^{k×d}\\) holds the component vectors (rows), \\(Z \\in R^{n×k}\\) are the codes, and \\(\\alpha&gt;0\\) controls sparsity. Different implementations (e.g., the sklearn SparsePCA) solve related objectives (sometimes using a LASSO subproblem or elastic-net style updates).\nNotes:\n\nThe L1 term on W encourages many component weights to be exactly zero, producing localized/part-like components.\nIn practice one often also adds a small L2 (ridge) term to stabilize optimization (an elastic-net variant).\n\n\nfrom sklearn.decomposition import SparsePCA\n\n#### Try Changing both n_components and alpha  ########\nn_components = 12\n# Warning: setting alpha too low will take the algorithm a long time to converge\nalpha = 0.01\n##########################\nspca = SparsePCA(n_components=n_components, alpha=alpha, random_state=42)\n# fit on the flattened data matrix 'data' used above\nspca.fit(data)\nW_sp = spca.components_\nZ_sp = spca.transform(data)\n\nNow let’s take a look at what the Sparse PCA model has learned and how it reconstructs the example airfoil differently than vanilla PCA:\n\n\nCode\n# Component heatmap (components x features)\nplt.figure(figsize=(10,4))\nplt.imshow(reorder_indices(W_sp), aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=W_sp.min(), vcenter=0, vmax=W_sp.max()))\nplt.colorbar(label='weight')\nplt.ylabel(f\"Latent index ($k \\\\in {W_sp.shape[0]}$)\")\nplt.xlabel(f\"Feature index ($d \\\\in {W_sp.shape[1]}$)\")\nplt.title('SparsePCA component weights (components x features)')\nplt.show()\n\n# Latent Coordinate heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(Z_sp, aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=Z_sp.min(), vcenter=0, vmax=Z_sp.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Latent index ($k \\\\in {Z_sp.shape[1]}$)\")\nplt.ylabel(f\"Data Point index ($n \\\\in {Z_sp.shape[0]}$)\")\nplt.title('Latent codes for each Data Point')\nplt.show()\n\n# Now let's try reconstructing an airfoil from the trained model\n# Pull out the latent representation of this airfoil ID\nz = Z_sp[airfoil_id]\nx_hat = spca.inverse_transform(z.reshape(-1, 1).T)\n# Alternatively, we could do the following manual reconstruction:\n#x_hat = np.dot(z, W_sp) + data.mean(axis=0)\n\n# Interactive visualization of each component\nif widgets is not None:\n    component_selector = widgets.IntSlider(value=1, min=1, max=n_components, step=1, description='component')\n    def show_component(k=4):\n        # Find the first k principal components and multiply them with the corresponding latent variables\n        #contrib = np.dot(z[airfoil_id,:k], components_[:k,:])+ estimator.mean_\n        plt.figure(figsize=(6,5))\n        selected_component = W_sp[k-1,:] + data.mean(axis=0)\n        plt.plot(make_airfoil(selected_component)[:,0], make_airfoil(selected_component)[:,1], label='Component', alpha=1.0)\n        plt.axis('equal')\n        plt.title(f'Component {k}')\n        #plt.legend()\n        plt.xlim(-0.6,0.6)\n        #plt.ylim(-0.4,0.4)\n        plt.show()\n    display(widgets.VBox([component_selector, widgets.interactive_output(show_component, {'k': component_selector})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n# Interactive top-k overlay for contribution\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=4, min=1, max=n_components, step=1, description='top_k')\n    def show_topk(k=4):\n        top_idx = np.argsort(np.abs(z))[::-1][:k]\n        contrib = np.sum(z[top_idx][:,None] * W_sp[top_idx], axis=0)\n        plt.figure(figsize=(6,3))\n        plt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\n        plt.plot(make_airfoil(contrib + data.mean(axis=0))[:,0], make_airfoil(contrib + data.mean(axis=0))[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '--', color='g', label='Mean')\n        plt.axis('equal')\n        plt.title(f'Top-{k} component contributions (overlay)')\n        plt.legend()\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_topk, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Comparison between PCA and SparsePCA\n\n\n\nCompare both the component weight matrix and the reconstruction of a single airfoil when we used PCA and SparsePCA.\nConsider the following questions:\n\nWhat are the main differences in the component weights between PCA and SparsePCA? How does the addition of the L1 penalty within SparsePCA manifest itself in the weight matrix? How does this manifest itself in the learned components, either individually or as we add them up during reconstruction?\nAlso compare the latent codes \\(Z\\) between PCA and SparsePCA. How do they differ? What does this tell you about how the data is represented in the latent space? How about which components are most important?\nFor SparsePCA, how does changing the value of the sparsity parameter \\(\\alpha\\) affect the learned components and reconstruction? What happens when we increase or decrease \\(\\alpha\\)? Look at both the weight matrix and the components.\nIf you set \\(\\alpha\\) very low (e.g., 0.001) and increase the number of components, how does the result compare to vanilla PCA with the same number of components? What would you expect to happen, comparing the loss functions? Why are they different?\nUnder what practical circumstances or for what types of problems might SparsePCA be preferred over PCA and vice versa?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#dictionary-learning",
    "href": "part1/linear_decompositions.html#dictionary-learning",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "4.3 Dictionary Learning",
    "text": "4.3 Dictionary Learning\nDictionary learning finds a set of atoms (basis elements) and sparse codes that reconstruct the data. This is useful when localized, part-based representations are desirable. Essentially, unlike Sparse PCA (which adds an L1 penalty to the components), dictionary learning adds an L1 penalty to the codes. Below we fit a Dictionary Learning model and visualize a small set of learned atoms and their effect on reconstructing an example airfoil.\nMathematically, Dictionary learning (sparse coding) models X as the product of a dictionary \\(W \\in R^{k×d}\\) (atoms) and sparse codes \\(Z \\in R^{n×k}\\):\n\\[\n\\min_{Z,W}\\;\\|X - Z W\\|_{F}^{2} + \\alpha\\|Z\\|_{1} \\,\\quad\\text{s.t. }\\|W_{j}\\|_{2}\\le 1\\; \\forall j.\n\\]\nHere Z contains the sparse coefficients for each example and \\(\\alpha&gt;0\\) controls the sparsity of the codes. The constraint on \\(W\\) prevents trivial scaling to reduce the penalty term (i.e., just pulling weight into W and shrinking Z). Intuitively, each data point is reconstructed as a sparse linear combination of dictionary atoms, essentially selecting a few “parts” (where the parts are elements of \\(W\\)) to compose the whole as a weighted sum.\nNotes:\n\nThe L1 penalty acts on the coefficients (Z) rather than \\(W\\) to encourage sparse usage of dictionary elements.\nMany algorithms alternate between solving for Z (a LASSO-type problem) and updating W (a constrained least-squares step).\n\n\n# Warning, running this particular cell takes a long time as the sklearn implementation \n# is not particularly fast. (~4-8 mins)\nfrom sklearn.decomposition import DictionaryLearning\n# Alternatively, you can use the MiniBatchDictionaryLearning which is faster, but less accurate or stable:\n#from sklearn.decomposition import MiniBatchDictionaryLearning as DictionaryLearning\n\n\n### Try changing n_components and alpha ########\nn_components = 10\nalpha = 0.1\n##########################\n\nDL = DictionaryLearning(n_components=n_components, alpha=alpha, random_state=42)\nZ_dl = DL.fit_transform(data)\nW_dl = DL.components_\n\n\n\nCode\n# Visualize first few atoms\nplt.figure(figsize=(12,6))\nfor i in range(n_components):\n    plt.subplot(4,6,i+1)\n    xy = make_airfoil(W_dl[i])\n    plt.plot(xy[:,0], xy[:,1], lw=1)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title(f'Atom {i}')\n    plt.axis('equal')\n    plt.xticks([])\n    plt.yticks([])\nplt.suptitle(f'Dictionary atoms (n={n_components}, alpha={alpha})')\nplt.show()\n\n# Component heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(reorder_indices(W_dl), aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=W_dl.min(), vcenter=0, vmax=W_dl.max()))\nplt.colorbar(label='weight')\nplt.ylabel(f\"Latent index ($k \\\\in {n_components}$)\")\nplt.xlabel(f\"Feature index ($d \\\\in {W_dl.shape[1]}$)\")\nplt.title('Dictionary of Features (aka Component weights)')\nplt.show()\n\n# Latent Coordinate heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(Z_dl, aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=Z_dl.min(), vcenter=0, vmax=Z_dl.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Latent index ($k \\\\in {n_components}$)\")\nplt.ylabel(f\"Data Point index ($n \\\\in {Z_dl.shape[0]}$)\")\nplt.title('Latent codes for each Data Point')\nplt.show()\n\n# Reconstruction an airfoil using the learned codes\nx_hat = np.dot(Z_dl[airfoil_id], W_dl)\nplt.figure(figsize=(6,3))\nplt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\nplt.plot(make_airfoil(x_hat)[:,0], make_airfoil(x_hat)[:,1], label='Reconstruction', color='C3')\nplt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '--', color='g', label='Mean')\nplt.axis('equal')\nplt.title('Comparison of Original and Reconstructed Airfoil')\nplt.legend()\nplt.show()\n\n# Interactive top-k atoms display\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=6, min=1, max=n_components, step=1, description='top_k')\n    def show_top_atoms(k=6):\n        top_idx = np.argsort(np.abs(Z_dl[airfoil_id]))[::-1][:k]\n        plt.figure(figsize=(8,4))\n        for i, idx in enumerate(top_idx):\n            plt.subplot(1,k,i+1)\n            plt.plot(make_airfoil(W_dl[idx])[:,0], make_airfoil(W_dl[idx])[:,1])\n            plt.axis('equal')\n            plt.title(f'atom {idx}')\n            plt.xticks([])\n            plt.yticks([])\n        plt.suptitle('Top atoms used for this example')\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_top_atoms, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n# Interactive top-k overlay for contribution\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=4, min=1, max=8, step=1, description='top_k')\n    def show_topk(k=4):\n        top_idx = np.argsort(np.abs(Z_dl[airfoil_id]))[::-1][:k]\n        #contrib = np.sum(z[top_idx][:,None] * W_sp[top_idx], axis=0)\n        contrib = np.sum(Z_dl[airfoil_id,top_idx][:,None] * W_dl[top_idx], axis=0)\n        plt.figure(figsize=(6,3))\n        plt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\n        #plt.plot(make_airfoil(contrib + data.mean(axis=0))[:,0], make_airfoil(contrib + data.mean(axis=0))[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(contrib)[:,0], make_airfoil(contrib)[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '--', color='g', label='Mean')\n        plt.axis('equal')\n        plt.title(f'Top-{k} component contributions (overlay)')\n        plt.legend()\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_topk, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n# Show sparsity level of codes\nsparsity = np.mean(np.abs(Z_dl) &lt; 1e-6)\nprint(f'Average fraction of near-zero coefficients: {sparsity:.3f}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage fraction of near-zero coefficients: 0.540\n\n\n\n\n\n\n\n\nTipExperiment: Comparison between Dictionary Learning (Sparse Coding) and SparsePCA\n\n\n\nCompare both the component weight matrix and the reconstruction of a single airfoil when we used Sparse PCA and Dictionary Learning (Sparse Coding).\nConsider the following questions:\n\nBoth algorithms use the same sparsity penalty (L1), but on different matrices. How does this difference manifest itself in the learned components and reconstructions? What happens to the component weights versus the latent codes?\nFor Dictionary Learning, how does changing the value of the sparsity parameter \\(\\alpha\\) affect the learned components and reconstruction? What happens when we increase or decrease \\(\\alpha\\)? Look at both the weight matrix and the components.\nUnder what practical circumstances or for what types of problems might Dictionary Learning (Sparse Coding) be preferred over PCA and Sparse PCA, and vice versa?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#non-negative-matrix-factorization-nmf",
    "href": "part1/linear_decompositions.html#non-negative-matrix-factorization-nmf",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "4.4 Non-Negative Matrix Factorization (NMF)",
    "text": "4.4 Non-Negative Matrix Factorization (NMF)\nNMF constrains both the basis elements and coefficients to be non-negative. This often yields parts-based, additive representations which can be intuitive for shape components under certain circumstances.\nMathematically, Non-negative matrix factorization approximates \\(X\\) (with \\(X\\gt 0\\) after shift) as the product of non-negative factors \\(W\\in R^{n×k}_{+}\\) and \\(H \\in R^{k×d}_{+}\\) by minimizing reconstruction error under non-negativity constraints:\n\\[\\min_{W\\ge 0,H\\ge 0}\\;\\|X - Z W\\|_{F}^{2} + \\beta_Z\\|Z\\|_{1} + \\beta_W\\|W\\|_{1} + \\alpha_Z\\|Z\\|^{2}_{F} + \\alpha_W\\|W\\|^{2}_{F},\\]\nwhere optional L1 penalties (\\(\\beta_Z\\), \\(\\beta_W \\ge 0\\)) encourage sparse parts or sparse activations, and L2 penalties (\\(\\alpha_Z\\), \\(\\alpha_W \\ge 0\\)) encourage stability or shrinkage. The essential constraint is \\(W,Z \\ge 0\\) which induces additive, parts-based representations.\nNotes:\n\nIn this notebook we shift data by the minimum to ensure \\(X \\ge 0\\) before fitting and then undo the shift when plotting reconstructions.\nTypical solvers use multiplicative updates or alternating non-negative least squares; regularization (L1 or L2) can be added to encourage sparsity or stability.\n\n\nfrom sklearn.decomposition import NMF\n\n#### Try changing n_components ########\nn_components = 14\n# This is the L2 penalty on the components\nalpha_W  = 0.001\n# This is the L1 ratio on the components (between 0 and 1)\n# Set to 0 for pure L2, 1 for pure L1, and in between for a mix\nl1_ratio = 0.001\n##########################\n\n# NMF needs non-negative data. We'll shift by the min and remember the offset.\nshift = data.min()\nXpos = data - shift + 1e-6\n\nnmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=5000,\n          alpha_W = alpha_W, l1_ratio=l1_ratio)\nZ_nmf = nmf.fit_transform(Xpos)\nW_nmf = nmf.components_\n\n\n\nCode\n# Visualize NMF components (H) as parts\nplt.figure(figsize=(12,4))\nfor i in range(n_components):\n    plt.subplot(2, n_components//2, i+1)\n    xy = make_airfoil(W_nmf[i])\n    plt.plot(xy[:,0], xy[:,1], lw=1)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.axis('equal')\n    plt.title(f'Part {i}')\n    plt.xticks([])\n    plt.yticks([])\nplt.suptitle('NMF learned parts (components)')\nplt.show()\n\n# Component heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(reorder_indices(W_nmf), aspect='auto', cmap='Reds', interpolation='nearest')\nplt.colorbar(label='weight')\nplt.xlabel('feature index')\nplt.ylabel('part index')\nplt.title('NMF part weights (parts x features)')\nplt.show()\n\n# Latent Coordinate heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(Z_nmf, aspect='auto', cmap='Reds', interpolation='nearest')\nplt.colorbar(label='weight')\nplt.xlabel(f\"Latent index ($k \\\\in {Z_nmf.shape[1]}$)\")\nplt.ylabel(f\"Data Point index ($n \\\\in {Z_nmf.shape[0]}$)\")\nplt.title('Latent codes for each Data Point')\nplt.show()\n\n# Reconstruction the airfoil using W @ H\nx_hat_pos = np.dot(Z_nmf[airfoil_id], W_nmf)\n# undo the shift to bring back to original centered data\nx_hat = x_hat_pos + shift - 1e-6\nmean_airfoil = make_airfoil(data.mean(axis=0))\n\n# Use plot_reconstruction helper for consistent display; but NMF uses shifted data so show overlay manually\nplt.figure(figsize=(6,3))\n#plt.subplot(1,2,1)\nplt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original')\nplt.plot(mean_airfoil[:,0], mean_airfoil[:,1], color='g', label='Mean')\nplt.plot(make_airfoil(x_hat)[:,0], make_airfoil(x_hat)[:,1], label='Reconstructed')\nplt.title('Original')\nplt.axis('equal')\nplt.legend()\nplt.show()\n\n# Interactive top-k parts\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=6, min=1, max=n_components, step=1, description='top_k')\n    def show_top_parts(k=6):\n        top_idx = np.argsort(Z_nmf[airfoil_id])[::-1][:k]\n        plt.figure(figsize=(8,3))\n        for i, idx in enumerate(top_idx):\n            plt.subplot(1,k,i+1)\n            plt.plot(make_airfoil(W_nmf[idx])[:,0], make_airfoil(W_nmf[idx])[:,1])\n            plt.axis('equal')\n            plt.title(f'part {idx}')\n            plt.xlim(-0.1,0.6)\n            plt.xticks([])\n            plt.yticks([])\n        plt.suptitle('Top parts used for this example')\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_top_parts, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually')\n\n# Interactive top-k overlay for contribution\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=4, min=1, max=14, step=1, description='top_k')\n    def show_topk(k=4):\n        top_idx = np.argsort(np.abs(Z_nmf[airfoil_id]))[::-1][:k]\n        contrib = np.sum(Z_nmf[airfoil_id,top_idx][:,None] * W_nmf[top_idx], axis=0)\n        plt.figure(figsize=(6,3))\n        plt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\n        #plt.plot(make_airfoil(contrib + data.mean(axis=0))[:,0], make_airfoil(contrib + data.mean(axis=0))[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(contrib+shift)[:,0], make_airfoil(contrib+shift)[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '--', color='g', label='Mean')\n        plt.axis('equal')\n        plt.title(f'Top-{k} component contributions (overlay)')\n        plt.legend()\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_topk, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\nprint(f'Explained variance (approx): {1 - np.linalg.norm(Xpos - Z_nmf.dot(W_nmf)) / np.linalg.norm(Xpos):.3f}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplained variance (approx): 0.994",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#model-comparison",
    "href": "part1/linear_decompositions.html#model-comparison",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "4.5 Model Comparison",
    "text": "4.5 Model Comparison\nFinally, let’s compare the different models side-by-side. We will visualize the two two learned projections of the data into the latent space, as well as the component reconstruction matrix that translates those components back into the original feature space, and lastly the mapping of all data points to all latent coordinates. This is similar to plots you have seen before, but it is nice to see them all together for comparison.\n\n\nCode\n# Comparison: first-two latent projections and component heatmaps for each method\nfrom sklearn.decomposition import PCA\n\nmodels_info = {}\n# PCA (reuse estimator from above if present, otherwise fit)\npca = PCA(n_components=8, random_state=42)\nZ_pca = pca.fit_transform(data)\nmodels_info['PCA'] = {'Z': Z_pca, 'components': pca.components_}\n\n# SparsePCA (use Z_sp, W_sp computed earlier)\nmodels_info['SparsePCA'] = {'Z': Z_sp, 'components': W_sp}\n\n# DictionaryLearning: use W as Z and H as components\nmodels_info['DictionaryLearning'] = {'Z': Z_dl, 'components': W_dl}\n\n# NMF: use W as Z (already non-negative)\nmodels_info['NMF'] = {'Z': Z_nmf, 'components': W_nmf}\n\n# Plot the first-two latent projections\nplt.figure(figsize=(10,8))\nfor i, (name, info) in enumerate(models_info.items()):\n    plt.subplot(2,2,i+1)\n    Z = info['Z']\n    plt.scatter(Z[:,0], Z[:,1], alpha=0.2, s=8)\n    plt.xlabel('Latent dim 1')\n    plt.ylabel('Latent dim 2')\n    plt.title(f'{name} projection (first two dims)')\nplt.tight_layout()\nplt.show()\n\n# Plot heatmaps of component/weight matrices (showing first 120 features for clarity)\nplt.figure(figsize=(12,8))\nfor i, (name, info) in enumerate(models_info.items()):\n    comps = info['components']\n    # ensure comps is (n_components, n_features)\n    if comps.ndim == 1:\n        comps = comps[None, :]\n    plt.subplot(4,1,i+1)\n    if np.any(comps &lt; 0):\n        plt.imshow(reorder_indices(comps), aspect='auto', cmap='RdBu_r', interpolation='nearest',\n                   norm=TwoSlopeNorm(vmin=comps.min(), vcenter=0, vmax=comps.max()))\n    else:\n        plt.imshow(reorder_indices(comps), aspect='auto', cmap='Reds', interpolation='nearest')\n    plt.colorbar()\n    plt.title(f'{name} components (components x features)')\n    plt.ylabel('component')\nplt.tight_layout()\nplt.show()\n\n# Plot heatmaps of component/weight matrices (showing first 120 features for clarity)\nplt.figure(figsize=(12,8))\nfor i, (name, info) in enumerate(models_info.items()):\n    Z_map = info['Z']\n    # ensure comps is (n_components, n_features)\n    if Z_map.ndim == 1:\n        Z_map = Z_map[None, :]\n    plt.subplot(4,1,i+1)\n    if np.any(Z_map &lt; 0):\n        plt.imshow(Z_map, aspect='auto', cmap='RdBu_r', interpolation='nearest',\n                   norm=TwoSlopeNorm(vmin=Z_map.min(), vcenter=0, vmax=Z_map.max()))\n    else:\n        plt.imshow(Z_map, aspect='auto', cmap='Reds', interpolation='nearest')\n    plt.colorbar()\n    plt.title(f'{name} components (components x features)')\n    plt.ylabel('component')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html",
    "href": "part1/taking_derivatives.html",
    "title": "5  Taking Derivatives",
    "section": "",
    "text": "5.1 Automatic Differentiation\nInsert Table showing benefits and drawbacks",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html#automatic-differentiation",
    "href": "part1/taking_derivatives.html#automatic-differentiation",
    "title": "5  Taking Derivatives",
    "section": "",
    "text": "Forward Mode AD\nBackward Mode AD\n\n\n5.1.1 Simple Example\n\n\n5.1.2 Differentiable Simulation using a Verlet Integrator",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html#demonstration-of-ad-on-verlet-integration",
    "href": "part1/taking_derivatives.html#demonstration-of-ad-on-verlet-integration",
    "title": "5  Taking Derivatives",
    "section": "5.2 Demonstration of AD on Verlet Integration",
    "text": "5.2 Demonstration of AD on Verlet Integration\nThis notebook demonstrates how to use Automatic Differentiation to determine the gradients of the initial conditions of a dynamical system (in this case a damped oscillator). To do this, we will define a numerical routine (Verlet Integration) and then use Automatic Differentiation to back propagate the gradient information from the output (Total system energy) to the initial conditions.\n\n\n\n\n\n\n\n\n\n\n\ntensor([0.6137], grad_fn=&lt;MulBackward0&gt;)\n\n\nNow let’s print the gradient of the system Energy with respect to some of the initial conditions:\n\n\ntensor([-5.9562])\ntensor([0.6026])\ntensor([0.6248])\n\n\n\n\ntensor([-0.2245], grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\ntensor([-4.7535])\ntensor([-0.0437])\ntensor([1.0466])",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html#optimizing-the-damping-coefficient-via-sgd-and-ad",
    "href": "part1/taking_derivatives.html#optimizing-the-damping-coefficient-via-sgd-and-ad",
    "title": "5  Taking Derivatives",
    "section": "5.3 Optimizing the Damping Coefficient via SGD and AD",
    "text": "5.3 Optimizing the Damping Coefficient via SGD and AD\nFirst let’s just get a visual intuition for how \\(\\gamma\\) affects the final energy:\n\n\nText(0, 0.5, 'E Final')\n\n\n\n\n\n\n\n\n\nWe can see that there is a pretty flat plateau from around \\(\\gamma=1\\) until around \\(\\gamma=3\\).\nNow let’s use our backward mode AD to actually optimize \\(\\gamma\\) directly by calling backward on the output of the final energy of the Verlet integration of the ODE:\n\n5.3.1 ADAM Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 SGD Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 LBFGS Example\n(Warning: Per-run solves of LBFGS take a while, so don’t set num_steps too high here)\n\n\n5.3.4 Compare the steps taken",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html#higher-order-derivatives",
    "href": "part1/taking_derivatives.html#higher-order-derivatives",
    "title": "5  Taking Derivatives",
    "section": "5.4 Higher-Order Derivatives",
    "text": "5.4 Higher-Order Derivatives\n\nComputing Hessians, Hessian Vector Products, Jacobian Vector Products",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html#implicit-differentiation",
    "href": "part1/taking_derivatives.html#implicit-differentiation",
    "title": "5  Taking Derivatives",
    "section": "5.5 Implicit Differentiation",
    "text": "5.5 Implicit Differentiation\n\n5.5.1 Simple Example\n\n\n5.5.2 Hyper-Parameter Tuning with Gradient Descent",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives</span>"
    ]
  },
  {
    "objectID": "problems/problems.html",
    "href": "problems/problems.html",
    "title": "Problems",
    "section": "",
    "text": "Here are the problem sets",
    "crumbs": [
      "Problems"
    ]
  },
  {
    "objectID": "problems/ps1.html",
    "href": "problems/ps1.html",
    "title": "6  Problem Set 1",
    "section": "",
    "text": "6.1 PS1 Part 1: Linear Models and Validation",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Problem Set 1</span>"
    ]
  },
  {
    "objectID": "problems/ps1.html#ps1-part-1-linear-models-and-validation",
    "href": "problems/ps1.html#ps1-part-1-linear-models-and-validation",
    "title": "6  Problem Set 1",
    "section": "",
    "text": "6.1.1 Preamble\nWe’ll be loading some CO2 concentration data that is a commonly used dataset for model building of time series prediction. You will build a few baseline linear models and assess them using some of the tools we discussed in class. Which model is best? Let’s find out.\nFirst let’s just load the data and take a look at it:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_openml\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\nsns.set_context('notebook')\n\n# Fetch the data\nmauna_lao = fetch_openml('mauna-loa-atmospheric-co2', as_frame = False)\nprint(mauna_lao.DESCR)\ndata = mauna_lao.data\n# Assemble the day/time from the data columns so we can plot it\nd1958 = datetime(year=1958,month=1,day=1)\ntime = [datetime(int(d[0]),int(d[1]),int(d[2])) for d in data]\nX = np.array([1958+(t-d1958)/timedelta(days=365.2425) for t in time]).T\nX = X.reshape(-1,1)  # Make it a column to make scikit happy\ny = np.array(mauna_lao.target)\n\n**Weekly carbon-dioxide concentration averages derived from continuous air samples for the Mauna Loa Observatory, Hawaii, U.S.A.**&lt;br&gt;&lt;br&gt;\nThese weekly averages are ultimately based on measurements of 4 air samples per hour taken atop intake lines on several towers during steady periods of CO2 concentration of not less than 6 hours per day; if no such periods are available on a given day, then no data are used for that day. The _Weight_ column gives the number of days used in each weekly average. _Flag_ codes are explained in the NDP writeup, available electronically from the [home page](http://cdiac.ess-dive.lbl.gov/ftp/trends/co2/sio-keel-flask/maunaloa_c.dat) of this data set. CO2 concentrations are in terms of the 1999 calibration scale (Keeling et al., 2002) available electronically from the references in the NDP writeup which can be accessed from the home page of this data set.\n&lt;br&gt;&lt;br&gt;\n### Feature Descriptions\n_co2_: average co2 concentration in ppvm &lt;br&gt;\n_year_: year of concentration measurement &lt;br&gt;\n_month_: month of concentration measurement &lt;br&gt;\n_day_: day of month of concentration measurement &lt;br&gt;\n_weight_: number of days used in each weekly average &lt;br&gt;\n_flag_: flag code &lt;br&gt;\n_station_: station code &lt;br&gt;\n&lt;br&gt;\n**Author**: Carbon Dioxide Research Group, Scripps Institution of Oceanography, University of California-San Diego, La Jolla, California, USA 92023-0444 &lt;br&gt;\n**Source**: [original](http://cdiac.ess-dive.lbl.gov/ftp/trends/co2/sio-keel-flask/maunaloa_c.dat) - September 2004\n\nDownloaded from openml.org.\n\n\n\n# Plot the data\nplt.figure(figsize=(10,5))    # Initialize empty figure\nplt.scatter(X, y, c='k',s=1) # Scatterplot of data\nplt.xlabel(\"Year\")\nplt.ylabel(r\"CO$_2$ in ppm\")\nplt.title(r\"Atmospheric CO$_2$ concentration at Mauna Loa\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ny[:100]\n\narray([316.1, 317.3, 317.6, 317.5, 316.4, 316.9, 317.5, 317.9, 315.8,\n       315.8, 315.4, 315.5, 315.6, 315.1, 315. , 314.1, 313.5, 313. ,\n       313.2, 313.5, 314. , 314.5, 314.4, 314.7, 315.2, 315.2, 315.5,\n       315.6, 315.8, 315.4, 316.9, 316.6, 316.6, 316.8, 316.7, 316.7,\n       317.7, 317.1, 317.6, 318.3, 318.2, 318.7, 318. , 318.4, 318.5,\n       318.1, 317.8, 317.7, 316.8, 316.8, 316.4, 316.1, 315.6, 314.9,\n       315. , 314.1, 314.4, 313.9, 313.5, 313.5, 313. , 313.1, 313.4,\n       313.4, 314.1, 314.4, 314.8, 315.2, 315.1, 315. , 315.6, 315.8,\n       315.7, 315.7, 316.4, 316.7, 316.5, 316.6, 316.6, 316.9, 317.4,\n       317. , 316.9, 317.7, 318. , 317.7, 318.6, 319.3, 319. , 319. ,\n       319.7, 319.9, 319.8, 320. , 320. , 319.4, 320. , 319.4, 319. ,\n       318.1])\n\n\n\n\n6.1.2 Linear Models\nConstruct the following linear models: 1. Model 1: “Vanilla” Linear Regression, that is, where \\(CO_2 = a+b \\cdot time\\) 2. Model 2: Quadratic Regression, where \\(CO_2 = a+b \\cdot t + c\\cdot t^2\\) 3. Model 3: A more complex “linear” model with the following additive terms \\(CO_2=a+b\\cdot t+c\\cdot sin(\\omega\\cdot t)\\): * a linear (in time) term * a sinusoidal additive term with period such that the peak-to-peak of the sinsusoid is roughly ~1 year and phase shift of zero (set \\(\\omega\\) as appropriate to match the peaks) 4. Model 4: A “linear” model with the following additive terms (\\(CO_2=a+b\\cdot t+c\\cdot t^2+d\\cdot sin(\\omega\\cdot t)\\): * a quadratic (in time) polynomial * a sinusoidal additive term with period such that the peak-to-peak of the sinsusoid is roughly ~1 year and phase shift of zero (set \\(\\omega\\) as appropriate to match the peaks)\nEvauate these models using the appropriate kind of Cross Validation for each of the following amounts of Training data: 1. N=50 Training Data Points 2. N=100 3. N=200 4. N=500 5. N=1000 6. N=2000\nQuestion: Before you even construct the models or do any coding below, what is your initial guess or intuition behind how each of those four models will perform? Note: there is no right or wrong answer to this part of the assignment and this question will only be graded on completeness, not accuracy. It’s intent is to get you to think about and write down your preliminary intuition regarding what you think will happen before you actually implement anything, based on your approximate understanding of how functions of the above complexity should perform as N increases.\nStudent Response: [Insert your response here]\nQuestion: What is the appropriate kind of Cross Validation to perform in this case if we want a correct Out of Sample estimate of our Test MSE?\nStudent Response: [Insert your response here]\nNow, for each of the above models and training data sizes: * Plot the predicted CO2 as a function of time, including the actual data, for each of the N=X training data examples. This should correspond to six plots (one for each amount of training data) if you plot all models on the same plot, or 6x4 = 24 plots if you plot each model and training data plot separately. * Create a Learning Curve plot for the model which plots its Training and Test MSE as a function of training data. That is, plot how Training and Testing MSE change as you increase the training data for each model. This could be a single plot for all four models (8 lines on the plot) or four different plots corresponding to the learning curve of each model separately.\n\nimport numpy as np\n\nX_train_100 = X[:100]\ny_train_100 = y[:100]\nX_test = X[100:200]\nprint(\"Shape of X_train_100: %s\" % str(X_train_100.shape))\nprint(\"Beginning of X_train_100: %s\" % str(X_train_100[0:5]))\nprint(\"Shape of y_train_100: %s\" % str(y_train_100.shape))\nprint(\"Beginning of y_train_100: %s\" % str(y_train_100[0:5]))\n\nprint('Shape of X_test: %s' % str(X_test.shape))\nprint(\"Beginning of X_test: %s\" % str(X_test[0:5]))\n\n### Modify the below code. You can leave the code above as is. ###\n\n\nShape of X_train_100: (100, 1)\nBeginning of X_train_100: [[1958.23819791]\n [1958.25736326]\n [1958.27652861]\n [1958.29569396]\n [1958.31485931]]\nShape of y_train_100: (100,)\nBeginning of y_train_100: [316.1 317.3 317.6 317.5 316.4]\nShape of X_test: (100, 1)\nBeginning of X_test: [[1960.51887445]\n [1960.5380398 ]\n [1960.55720514]\n [1960.57637049]\n [1960.59553584]]\n\n\n\n# Insert Modeling Building or Plotting code here\n# Note, you may implement these however you see fit\n# Ex: using an existing library, solving the Normal Eqns\n#     implementing your own SGD solver for them. Your Choice.\nfrom sklearn.linear_model import SGDRegressor, LinearRegression\nsgd = SGDRegressor()\nlr = LinearRegression()\n\n\nsgd.fit(X,y)\nlr.fit(X,y)\n\nLinearRegression()\n\n\n\nsgd.predict(X)\n\narray([-1.78959753e+15, -1.78961505e+15, -1.78963256e+15, ...,\n       -1.82954889e+15, -1.82956640e+15, -1.82958392e+15])\n\n\n\nlr.predict(X)\n\narray([310.2080183 , 310.23375578, 310.25949326, ..., 368.9152125 ,\n       368.94094999, 368.96668747])\n\n\nQuestion: Which Model appears to perform best in the N=50 or N=100 Condition? Why is this?\nStudent Response: [Insert your response here]\nQuestion: Which Model appears to perform best as the N=200 to 500? Why is this?\nStudent Response: [Insert your response here]\nQuestion: Which Model appears to perform best as N = 2000? Why is this?\nStudent Response: [Insert your response here]",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Problem Set 1</span>"
    ]
  },
  {
    "objectID": "problems/ps1.html#ps1-part-2-unsupervised-linear-models",
    "href": "problems/ps1.html#ps1-part-2-unsupervised-linear-models",
    "title": "6  Problem Set 1",
    "section": "6.2 PS1 Part 2: Unsupervised Linear Models",
    "text": "6.2 PS1 Part 2: Unsupervised Linear Models\n\n6.2.1 Toy Dataset\nFor this problem, you will use the data file hb.csv. The input is 2,280 data points, each of which is 7 dimensional (i.e., input csv is 2280 rows by 7 columns). Use Principal Component Analysis (either an existing library, or through your own implementation by taking the SVD of the Covariance Matrix) for the follow tasks.\n\n%matplotlib inline\nimport pandas\nurl = \"https://raw.githubusercontent.com/IDEALLab/ML4ME_Textbook/main/problems/hb.csv\"\ndata = pandas.read_csv(url,header=None)\n#data.head()\n\n\n\n6.2.2 Task 1\nAssuming that the 7-dimensional space is excessive, you would like to reduce the dimension of the space. However, what dimensionality of space should we reduce it to? To determine this we need to compute its intrinsic dimensionality. Plot the relative value of the information content of each of the principal components and compare them.\nNote: this information content is called the “explained variance” of each component, but you can also get this from the magnitude of the singular values. This plot is sometimes called a “Scree Plot”.\n\n# Code Here\n\nQuestion: Approximately how many components dominate the space?, and what does this tell us about the intrinsic dimensionality of the space?\nResponse:\n\n6.2.2.1 Task 2\nNow use PCA to project the 7-dimensional points on the K-dimensional space (where K is your answer from above) and plot the points. (For K=1,2, or 3, use a 1, 2, or 3D plot, respectively. For 4+ dimensions, use a grid of pairwise 2D Plots, like the Scatter Matrix we used in class).\n\n# Code Here\n\nQuestion: What do you notice?\nResponse:\n\n\n\n6.2.3 Topology Optimization Dataset\nFor this problem, you will be using unsupervised linear models to help understand and interpret the results of a mechanical optimization problem. Specifically, to understand the solution space generated by a topology optimization code; that is, the results of finding the optimal geometries for minimizing the compliance of various bridge structures with different loading conditions. The input consists of 1,000 images of optimized material distribution for a beam as described in Figure 1. A symmetrical boundary condition, left side, is used to reduce the analysis to only half. Also, a rolling support is included at the lower right corner. Notice that the rolling support is the only support in the vertical direction.\n \n\n\n\n\nFigure 1: Left: Nx-by-Ny design domain for topology optimization problem. Right: Example loading configuration and resulting optimal topology. Two external forces, Fi, were applied to the beam at random nodes represented by (xi, yi) coordinates.1\n\n \nUse Principal Component Analysis (either an existing library, or through your own implementation by taking the SVD of the Covariance Matrix) for the follow tasks.\n1. This problems data is based on the problem setup seen in the following paper: Ulu, E., Zhang, R., & Kara, L. B. (2016). A data-driven investigation and estimation of optimal topologies under variable loading configurations. Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization, 4(2), 61-72.\n\n# To help you get started, the below code will load the images from the associated image folder:\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom PIL import Image\nimport requests, zipfile, io\n\n#im_dir = './topo_opt_runs/'\nurl = \"https://raw.githubusercontent.com/IDEALLab/ML4ME_Textbook/main/problems/topo_opt_runs.zip\"\nresp = requests.get(url)\nresp.raise_for_status()\nzf = zipfile.ZipFile(io.BytesIO(resp.content))\n\nimages = []\nfor name in sorted(zf.namelist()):\n    with zf.open(name) as f:\n        img = Image.open(f).convert('L')\n        images.append(np.asarray(img))\n\nheight,width = images[0].shape\nprint('The images are {:d} pixels high and {:d} pixels wide'.format(height,width))\n\n# Print matrix corresponding to the image:\nprint(images[-1])\n# And show example image, so you can see how matrix correponds:\nimg\n\nThe images are 217 pixels high and 434 pixels wide\n[[  0   0   0 ... 255 255 255]\n [  0   0   0 ... 255 255 255]\n [  0   0   0 ... 255 255 255]\n ...\n [  0   0   0 ...   0   0   0]\n [  0   0   0 ...   0   0   0]\n [  0   0   0 ...   0   0   0]]\n\n\n\n\n\n\n\n\n\n\n\n6.2.4 Task 1: Scree/Singular Value Plot\nAs with the toy example, assume that the 94,178-dimensional space is excessive. You would like to reduce the dimension of the image space. First compute its intrinsic dimensionality. For this application, “good enough” means capturing 95% of the variance in the dataset. How many dimensions are needed to capture at least 95% of the variance in the provided dataset? Store your answer in numDimsNeeded. (Hint: A Scree plot may be helpful, though visual inspection of a graph may not be precise enough.)\nQuestion: Approximately how many components dominate the space? What does this tell us about the intrinsic dimensionality of the space?\nResponse:\n\n\n6.2.5 Task 2: Principal Components\nNow plot the first 5 principal components. Hint: looking at each of these top 5 principal components; do they make sense physically, in terms of what it means for where material in the bridge is placed? Compare, for example, the differences between the 1st and 2nd principal component?",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Problem Set 1</span>"
    ]
  },
  {
    "objectID": "problems/ps1.html#ps1-part-3-bi-linear-models-and-sgd",
    "href": "problems/ps1.html#ps1-part-3-bi-linear-models-and-sgd",
    "title": "6  Problem Set 1",
    "section": "6.3 PS1 Part 3: Bi-Linear Models and SGD",
    "text": "6.3 PS1 Part 3: Bi-Linear Models and SGD\n\n6.3.1 Bilinear Models for Recommendation\nFor this problem, you will derive a very simple recommendation system that uses a combination of unsupervised and supervised approachs and demonstrates use of Stochastic Gradient Descent.\nSpecifically, in class we discussed recommender models of the form: \\[\nf(user,movie) = \\langle v_u,v_m \\rangle + b_u + b_m + \\mu\n\\]\nwhere \\(v\\) is a vector that represents a user’s or movie’s location in an N-Dimensional space, \\(b\\) is a vector that represents a specific “bias” term fo r each movie and user, and \\(\\mu\\) is a scalar that represents a kind of global anchor or base score (i.e., a sort of average movie rating). This means that each user has two vectors (e.g., \\(v_{\\mathrm{jack~smith}}\\) and \\(b_{\\mathrm{jack~smith}}\\)), and each movie has two vectors (e.g., \\(v_{\\mathrm{Avengers}}\\) and \\(b_{\\mathrm{Avengers}}\\)), with each of those vectors being N-Dimensional (in class we used two dimensions). For this, we constructed a loss function as follows: \\[\nCost = Loss + Penalty\n\\] where \\[\nLoss = \\Sigma_{(u,m)\\in \\mathrm{Ratings}} \\frac{1}{2}\\left( \\langle v_u,v_m \\rangle + b_u + b_m + \\mu - y_{u,m}\\right)^2\n\\] and \\[\nPenalty = \\frac{\\lambda}{2}\\left(\\Sigma_u \\left[\\| v_u\\|^2_2 + b_u^2\\right] + \\Sigma_m \\left[\\|v_m\\|^2_2 + b_m^2\\right]\\right)\n\\]\n\n\n6.3.2 Task 1: Analytical Gradients\nTo use stochastic gradient descent, we first need to write down the gradients. Using the above cost function (including both the loss and penalty), compute the following partial derivatives:\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial v_u } =\n\\]\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial v_m } =\n\\]\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial b_u} =\n\\]\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial b_m} =\n\\]\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial \\mu} =\n\\]\nYou can either do this directly in the notebook using LaTeX notation, or via a scanned image. Please remember to show your work in how you computed the derivatives, not just the final result. Note: Recall that the partial derivative of e.g. Nicholas’s rating on Titanic with respect to the user Mark would be zero. When computing your SGD updates, consider how this might impact individual terms for users and movies in the loss function.\n\n\n6.3.3 Task 2: Stochastic Gradient Descent\nNow you are actually going to implement SGD on this type of model and optimize it until convergence on a toy dataset. To simplify the implementation, we’ll actually make the model a little simpler than the one you derived updates for in task 1. Specifically, we’ll just use:\n\\[\nCost = \\Sigma_{(u,m)\\in \\mathrm{Ratings}} \\frac{1}{2}\\left( \\langle v_u,v_m \\rangle + \\mu - y_{u,m}\\right)^2 + \\frac{\\lambda}{2}\\left(\\| v_u\\|^2_2 + \\|v_m\\|^2_2\\right)\n\\]\nThis way all you have to estimate is two vectors — \\(v_u\\) for each user and \\(v_m\\) for each movie — and \\(\\mu\\) — a scalar value similar to an average rating. For simplicity, we’ll assume here that the size of the latent space (K) is 2 (i.e., the length of each \\(v_u\\) & \\(v_m\\)).\nUsing your above gradients, write down the update equations for each vector using stochastic gradient descent. Once you have done this, implement those update equations in code like we did in the in-class notebook. For simplicity, you can just use a constant step size \\(\\alpha\\) if you wish, though you may change this if you want. Note: depending on exactly how you implement your model and what batch size you use, i.e., one point at a time, or some subset of data points, values of \\(\\alpha\\) anywhere between around 0.7 and 0.01 should be sufficient to converge the model in under 1000 epochs, i.e., passes through the dataset. If you implement more advanced tricks covered in some optional readings this can converge much faster, but that is not necessary for this assignment, and it does not matter to me how quickly your model coverges, so long as it does so.\nUse the below small sample dataset of movie ratings for five users and six movies to perform stochastic gradient descent to update those vectors until your model converges. To initialize your SGD, you can use the initial weights/terms we provide below, or you can initialize the model any other way you wish – the exact initialization should not make a big difference here.\n\n# Your Code below!\n\n\nimport numpy as np\nimport pandas as pd\nmissing_ratings = pd.read_csv('missing.csv')\nratings = pd.read_csv('ratings.csv')\nratings\n\n\n\n\n\n\n\n\nmovie\nuser\nratings\n\n\n\n\n0\nThe Avengers\nAlex\n3.0\n\n\n1\nThe Avengers\nPriya\n3.5\n\n\n2\nThe Avengers\nYichen\n3.5\n\n\n3\nWhen Harry Met Sally\nAlex\n3.0\n\n\n4\nWhen Harry Met Sally\nSally\n4.5\n\n\n5\nWhen Harry Met Sally\nPriya\n3.0\n\n\n6\nWhen Harry Met Sally\nYichen\n3.0\n\n\n7\nSilence of the Lambs\nAlex\n3.0\n\n\n8\nSilence of the Lambs\nSally\n4.0\n\n\n9\nSilence of the Lambs\nJuan\n3.5\n\n\n10\nSilence of the Lambs\nPriya\n3.0\n\n\n11\nSilence of the Lambs\nYichen\n2.5\n\n\n12\nShawshank Redemption\nJuan\n2.5\n\n\n13\nShawshank Redemption\nPriya\n4.0\n\n\n14\nShawshank Redemption\nYichen\n4.0\n\n\n15\nThe Hangover\nAlex\n3.0\n\n\n16\nThe Hangover\nSally\n3.5\n\n\n17\nThe Hangover\nPriya\n3.0\n\n\n18\nThe Hangover\nYichen\n2.5\n\n\n19\nThe Godfather\nAlex\n3.0\n\n\n20\nThe Godfather\nPriya\n3.5\n\n\n\n\n\n\n\n\n# Alternatively, if you prefer, you can convert it into numpy first:\nratings_numpy = ratings.to_numpy()\nratings_numpy\n\narray([['The Avengers', 'Alex', 3.0],\n       ['The Avengers', 'Priya', 3.5],\n       ['The Avengers', 'Yichen', 3.5],\n       ['When Harry Met Sally', 'Alex', 3.0],\n       ['When Harry Met Sally', 'Sally', 4.5],\n       ['When Harry Met Sally', 'Priya', 3.0],\n       ['When Harry Met Sally', 'Yichen', 3.0],\n       ['Silence of the Lambs', 'Alex', 3.0],\n       ['Silence of the Lambs', 'Sally', 4.0],\n       ['Silence of the Lambs', 'Juan', 3.5],\n       ['Silence of the Lambs', 'Priya', 3.0],\n       ['Silence of the Lambs', 'Yichen', 2.5],\n       ['Shawshank Redemption', 'Juan', 2.5],\n       ['Shawshank Redemption', 'Priya', 4.0],\n       ['Shawshank Redemption', 'Yichen', 4.0],\n       ['The Hangover', 'Alex', 3.0],\n       ['The Hangover', 'Sally', 3.5],\n       ['The Hangover', 'Priya', 3.0],\n       ['The Hangover', 'Yichen', 2.5],\n       ['The Godfather', 'Alex', 3.0],\n       ['The Godfather', 'Priya', 3.5]], dtype=object)\n\n\nLet’s initialize the vectors to some random numbers, and \\(\\mu\\) to 2.5\n\nK=2\nuser_names = ratings['user'].unique()\nmovie_names = ratings['movie'].unique()\nmu= 2.5\n# Setting the seed of the random generator to a value so that everyone sees the same initialization\n# should should be able to comment out the below with no ill-effects on whatever model you implement\n# this may just help us in office hours if folks have difficulty implementing things\nnp.random.seed(0)\nV = pd.DataFrame(np.random.random((len(user_names)+len(movie_names),K)),index=np.hstack([user_names,movie_names]))\nprint(V)\n\n                             0         1\nAlex                  0.548814  0.715189\nPriya                 0.602763  0.544883\nYichen                0.423655  0.645894\nSally                 0.437587  0.891773\nJuan                  0.963663  0.383442\nThe Avengers          0.791725  0.528895\nWhen Harry Met Sally  0.568045  0.925597\nSilence of the Lambs  0.071036  0.087129\nShawshank Redemption  0.020218  0.832620\nThe Hangover          0.778157  0.870012\nThe Godfather         0.978618  0.799159\n\n\n\n# Here is one example of how to go through rows of a ratings matrix\nfor index, rating in ratings.iterrows():\n    user  = rating['user']\n    movie = rating['movie']\n    score = rating['ratings']\n    print(f\"{user} gave {movie} a score of {score}\")\n\nAlex gave The Avengers a score of 3.0\nPriya gave The Avengers a score of 3.5\nYichen gave The Avengers a score of 3.5\nAlex gave When Harry Met Sally a score of 3.0\nSally gave When Harry Met Sally a score of 4.5\nPriya gave When Harry Met Sally a score of 3.0\nYichen gave When Harry Met Sally a score of 3.0\nAlex gave Silence of the Lambs a score of 3.0\nSally gave Silence of the Lambs a score of 4.0\nJuan gave Silence of the Lambs a score of 3.5\nPriya gave Silence of the Lambs a score of 3.0\nYichen gave Silence of the Lambs a score of 2.5\nJuan gave Shawshank Redemption a score of 2.5\nPriya gave Shawshank Redemption a score of 4.0\nYichen gave Shawshank Redemption a score of 4.0\nAlex gave The Hangover a score of 3.0\nSally gave The Hangover a score of 3.5\nPriya gave The Hangover a score of 3.0\nYichen gave The Hangover a score of 2.5\nAlex gave The Godfather a score of 3.0\nPriya gave The Godfather a score of 3.5\n\n\n\n# Here is an example of one way to access rows of V\nfor index, rating in ratings.iterrows():\n    user  = rating['user']\n    movie = rating['movie']\n    print(f\"{user}'s location in V is {V.loc[user].to_numpy()}.\")\n    print(f\"{movie}'s location in V is {V.loc[movie].to_numpy()}.\")\n    print()\n\n# You could also do it in Numpy directly, which will likely lead to much faster SGD updates,\n# but that shouldn't be necessary for problems of this size. Up to you!\n\nAlex's location in V is [0.5488135  0.71518937].\nThe Avengers's location in V is [0.79172504 0.52889492].\n\nPriya's location in V is [0.60276338 0.54488318].\nThe Avengers's location in V is [0.79172504 0.52889492].\n\nYichen's location in V is [0.4236548  0.64589411].\nThe Avengers's location in V is [0.79172504 0.52889492].\n\nAlex's location in V is [0.5488135  0.71518937].\nWhen Harry Met Sally's location in V is [0.56804456 0.92559664].\n\nSally's location in V is [0.43758721 0.891773  ].\nWhen Harry Met Sally's location in V is [0.56804456 0.92559664].\n\nPriya's location in V is [0.60276338 0.54488318].\nWhen Harry Met Sally's location in V is [0.56804456 0.92559664].\n\nYichen's location in V is [0.4236548  0.64589411].\nWhen Harry Met Sally's location in V is [0.56804456 0.92559664].\n\nAlex's location in V is [0.5488135  0.71518937].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nSally's location in V is [0.43758721 0.891773  ].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nJuan's location in V is [0.96366276 0.38344152].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nPriya's location in V is [0.60276338 0.54488318].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nYichen's location in V is [0.4236548  0.64589411].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nJuan's location in V is [0.96366276 0.38344152].\nShawshank Redemption's location in V is [0.0202184  0.83261985].\n\nPriya's location in V is [0.60276338 0.54488318].\nShawshank Redemption's location in V is [0.0202184  0.83261985].\n\nYichen's location in V is [0.4236548  0.64589411].\nShawshank Redemption's location in V is [0.0202184  0.83261985].\n\nAlex's location in V is [0.5488135  0.71518937].\nThe Hangover's location in V is [0.77815675 0.87001215].\n\nSally's location in V is [0.43758721 0.891773  ].\nThe Hangover's location in V is [0.77815675 0.87001215].\n\nPriya's location in V is [0.60276338 0.54488318].\nThe Hangover's location in V is [0.77815675 0.87001215].\n\nYichen's location in V is [0.4236548  0.64589411].\nThe Hangover's location in V is [0.77815675 0.87001215].\n\nAlex's location in V is [0.5488135  0.71518937].\nThe Godfather's location in V is [0.97861834 0.79915856].\n\nPriya's location in V is [0.60276338 0.54488318].\nThe Godfather's location in V is [0.97861834 0.79915856].\n\n\n\n\n\n6.3.4 Train your Bilinear Model using SGD\n\n# Your Model building and training code here!\n\n\n\n6.3.5 Assessing your accuracy\nLet’s predict the ratings for the missing entries using our (randomly initialized) model.\n\nfor index, rating in missing_ratings.iterrows():\n    user  = rating['user']\n    movie = rating['movie']\n    prediction = np.dot(V.loc[user],V.loc[movie])+mu\n    print(f\"Prediction: {user} will rate {movie}: {prediction:.2f}\")\n\nPrediction: Sally will rate The Avengers: 3.32\nPrediction: Juan will rate The Avengers: 3.47\nPrediction: Juan will rate When Harry Met Sally: 3.40\nPrediction: Alex will rate Shawshank Redemption: 3.11\nPrediction: Sally will rate Shawshank Redemption: 3.25\nPrediction: Juan will rate The Hangover: 3.58\nPrediction: Sally will rate The Godfather: 3.64\nPrediction: Juan will rate The Godfather: 3.75\nPrediction: Yichen will rate The Godfather: 3.43",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Problem Set 1</span>"
    ]
  },
  {
    "objectID": "notebooks/notebooks.html",
    "href": "notebooks/notebooks.html",
    "title": "In-Class Notebooks",
    "section": "",
    "text": "This is a list of some of the in-class notebooks I have created for my course, elements of which are already part of the book. I am listing the individual notebooks here to have a direct link to download individual notebooks, since we often use these in class and it is helpful to have them in a centralized place with download links:\n\nReviewing Data Visualization: California Housing Dataset\nIntroduction to Linear Regression and Cross-Validation\nIntroduction to Gradient Descent\nLoss Functions for Linear Models",
    "crumbs": [
      "In-Class Notebooks"
    ]
  },
  {
    "objectID": "notebooks/california_housing_visualization.html",
    "href": "notebooks/california_housing_visualization.html",
    "title": "7  Housing Price Data Visualization In-Class Exercise",
    "section": "",
    "text": "7.1 Exercise 1: Visualize the relationship between features and price\nIn this review notebook, we will review the basics of data visualization and the importance of this for being able to build performant ML models. It is also a good starting point for people to get used to Python and the use of Jupyter Notebooks. It is designed as an in-class (or on your own) exercise to get your feet wet in working with data. Specifically, in this notebook we will:\nThe below code is a useful starting point that should get you started with exploring the California Housing data.\nSome people might find it useful to use a Pandas dataframe to manipulate the data, but that is not really required for basic plotting and visualization:\nFor the below tasks, you might want to try out some basic Python plotting libraries. I recommend:\nIf you aren’t familiar with any of the above libraries, I would suggest starting with Seaborn, since it hides many of the complex features you might not need right away (check out their tutorial). Bokeh also has a nice Quick Start guide if you like having the ability to pan/zoom the data.\nVisualize the 2D relationship between housing prices and the provided features of the data. You can choose how you want to do this.\n[Enter your code into the empty cell below to create the necessary visualizations. You can create multiple cells for code or Markdown code if that helps you.]\nfeatures = ['MedInc','HouseAge','AveRooms','AveBedrms','Population','AveOccup']\nfor i,feature in enumerate(features):\n    plt.figure()\n    sns.jointplot(x=df[feature],y=df['price'],alpha=0.1)\n    plt.show()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\nQuestion: Do any of the features appear linearly correlated with price?",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Housing Price Data Visualization In-Class Exercise</span>"
    ]
  },
  {
    "objectID": "notebooks/california_housing_visualization.html#exercise-2-visualize-the-relationships-between-features",
    "href": "notebooks/california_housing_visualization.html#exercise-2-visualize-the-relationships-between-features",
    "title": "7  Housing Price Data Visualization In-Class Exercise",
    "section": "7.2 Exercise 2: Visualize the relationships between features",
    "text": "7.2 Exercise 2: Visualize the relationships between features\nVisualize the 1D and 2D relationships between the features in the dataset. For example, how are house ages distributed? What is the relationship between house age and the number of bedrooms? Feel free to explore different 1D and 2D options.\n\n\nfor feature in features:\n    plt.figure()\n    sns.displot(df[feature])\n    plt.show()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# Plot the pairwise distributions between each of the features:\nfor i,feature in enumerate(features):\n    for j,feature2 in enumerate(features):\n        if j&gt;i:\n            plt.figure()\n            sns.jointplot(x=df[feature],y=df[feature2],alpha=1.0)\n            plt.show()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nQuestion: Are there any anomalies that look strange in the data, and which visualization helped you identify them (hint: there should be several)?",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Housing Price Data Visualization In-Class Exercise</span>"
    ]
  },
  {
    "objectID": "notebooks/california_housing_visualization.html#exercise-3-visualize-relationships-with-the-anomalies-removed",
    "href": "notebooks/california_housing_visualization.html#exercise-3-visualize-relationships-with-the-anomalies-removed",
    "title": "7  Housing Price Data Visualization In-Class Exercise",
    "section": "7.3 Exercise 3: Visualize relationships with the Anomalies Removed",
    "text": "7.3 Exercise 3: Visualize relationships with the Anomalies Removed\nUsing your knowledge of the anomalies you found above, remove those anomalies using appropriate code below (either by removing the entire data record, or just the specific values that were anomalous, if you prefer to be more surgical) and replot the Task 1 and Task 2 plots you produced above.\n\n# Code goes here for dealing with outliers\n# You can copy/paste some of your plotting code from above, if that is helpful.\n\nQuestion: Does this change your answer to the original Task 1 or Task 2 questions?",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Housing Price Data Visualization In-Class Exercise</span>"
    ]
  },
  {
    "objectID": "notebooks/pytorch_autograd_verlet_integrator.html",
    "href": "notebooks/pytorch_autograd_verlet_integrator.html",
    "title": "9  Demonstration of AD on Verlet Integration",
    "section": "",
    "text": "9.1 Optimizing the Damping Coefficient via SGD and AD\nThis notebook demonstrates how to use Automatic Differentiation to determine the gradients of the initial conditions of a dynamical system (in this case a damped oscillator). To do this, we will define a numerical routine (Verlet Integration) and then use Automatic Differentiation to back propagate the gradient information from the output (Total system energy) to the initial conditions.\nNow let’s print the gradient of the system Energy with respect to some of the initial conditions:\nFirst let’s just get a visual intuition for how \\(\\gamma\\) affects the final energy:\nnum_gammas = 30\ngamma_plot = np.logspace(-0.5,1.0,num_gammas)\nEfs = np.zeros(num_gammas)\nfor i,g in enumerate(gamma_plot):\n    ((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,g)\n    Efs[i] = Ef\nsemilogx(gamma_plot,Efs)\nxlabel(r'$\\gamma$')\nylabel('E Final')\n\nText(0, 0.5, 'E Final')\nWe can see that there is a pretty flat plateau from around \\(\\gamma=1\\) until around \\(\\gamma=3\\).\nNow let’s use our backward mode AD to actually optimize \\(\\gamma\\) directly by calling backward on the output of the final energy of the Verlet integration of the ODE:\n# This part is just a helper library for plotting\ndef plot_optimization(initial_gamma, num_steps, optimizer, opt_kwargs={}):\n    # Take an initial guess at the optimum:\n    gamma = torch.tensor([initial_gamma], requires_grad=True)\n\n    # Initialize the optimizer\n    optimizer = optimizer([gamma], **opt_kwargs)\n\n    steps = [ ] # Here is where we'll keep track of the steps\n    # Take num_steps of the optimizer\n    for i in range(num_steps):\n        # This function runs an actual optimization step. We wrap it in closure so that optimizers\n        # that take multiple function calls per step can do so -- e.g., LBFGS.\n        def closure():\n            # Get rid of the existing gradients on the tape\n            optimizer.zero_grad()\n            # Run the numerical integration -- this is the forward pass through the solver\n            ((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,gamma) # x0 = 0.0, v0 = 1.0, gamma = 0.0\n            # Compute the backward mode AD pass\n            Ef.backward()\n            return Ef\n        # Now ask the optimizer to take a step\n        optimizer.step(closure)\n        \n        # The below part is just for printing/plotting. We call torch.no_grad() here to signify that\n        # we do not need to track this as part of the gradient operations. That is, these parts will not\n        # be added to the computational graph or used for backward mode AD.\n        with torch.no_grad():\n            #print(gamma)\n            # Run again just to plot the solution for this gamma\n            ((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,gamma)\n            #print(Ef)\n            if num_steps&gt;10 and i%3==0:\n                plot_solution(x1,E1,gamma)\n            # Add it to steps so that we can see/plot it later.\n            steps.append(np.array(gamma.detach().numpy()))\n            \n    steps = np.array(steps)\n    return steps",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Demonstration of AD on Verlet Integration</span>"
    ]
  },
  {
    "objectID": "notebooks/pytorch_autograd_verlet_integrator.html#optimizing-the-damping-coefficient-via-sgd-and-ad",
    "href": "notebooks/pytorch_autograd_verlet_integrator.html#optimizing-the-damping-coefficient-via-sgd-and-ad",
    "title": "9  Demonstration of AD on Verlet Integration",
    "section": "",
    "text": "9.1.1 ADAM Example\n\nsteps_Adam = plot_optimization(0.05, 20,\n                              torch.optim.AdamW,\n                              {'lr':0.5})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.2 SGD Example\n\nsteps_SGD = plot_optimization(0.05, 20,\n                              torch.optim.SGD,\n                              {'lr':0.05,'momentum':0.9})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.3 LBFGS Example\n(Warning: Per-run solves of LBFGS take a while, so don’t set num_steps too high here)\n\nsteps_LBFGS = plot_optimization(0.05, 5,\n                              torch.optim.LBFGS,\n                             {'lr':0.3})\n\n\n\n9.1.4 Compare the steps taken\n\nsemilogx(gamma_plot,Efs)\nsteps_Adam = steps_Adam.flatten()\nplot(steps_Adam,[0.0]*len(steps_Adam),'|', color = 'r', label = 'Adam Steps')\nplot(steps_SGD,[0.005]*len(steps_SGD),'|', color = 'g', label = 'SGD Steps')\nplot(steps_LBFGS,[0.01]*len(steps_LBFGS),'|', color = 'k', label = 'LBFGS Steps')\nxlabel(r'$\\gamma$')\nylabel('E Final')\nlegend()",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Demonstration of AD on Verlet Integration</span>"
    ]
  },
  {
    "objectID": "notebooks/loss_functions_for_linear_models.html",
    "href": "notebooks/loss_functions_for_linear_models.html",
    "title": "10  Examples of Loss Functions for Linear Models",
    "section": "",
    "text": "10.1 Loss Functions for Regression\nThus far we have been discussing Linear Models in their most familiar context — minimizing the Mean Squared Error (MSE) with respect to the training data, optionally with an L2 regularization on the weight vector:\n\\[\n\\mathcal{L}(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 + \\alpha ||\\mathbf{w}||_2^2\n\\]\nFor now, let us ignore the regularization term, and just focus on the Loss term. Why should we minimize the squared error?1 Why not the absolute error or other possible loss functions? Let’s explore a few of those options and then see, in practice, how they affect the learned linear model.\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import zero_one_loss\n\ndef modified_huber_loss(y):\n    if(abs(y)&lt;1):\n        return y**2\n    else:\n        return 2*abs(y)-1\nmhuber = np.vectorize(modified_huber_loss)\n\neps = 0.7\ndef sq_esp_insensitive(y):\n    if(abs(y)&lt;eps):\n        return 0\n    else:\n        return (abs(y)-eps)**2\nsq_eps_ins = np.vectorize(sq_esp_insensitive)\nCode\nxmin, xmax = -4, 4\nxx = np.linspace(xmin, xmax, 100)\n\nplt.figure(figsize=(10,5))\nplt.plot(xx, xx**2, 'g-',\n         label=\"Squared Loss\")\nplt.plot(xx, abs(xx), 'g--',\n         label=\"Absolute Loss\")\n\nplt.plot(xx, abs(xx)-eps, 'b--',\n         label=\"Epsilon-Insensitive Loss\")\n\nplt.plot(xx, sq_eps_ins(xx), 'b-',\n         label=\"Sq-Epsilon-Insensitive Loss\")\nplt.plot(xx, mhuber(xx), 'r-',\n         label=\"Modified-Huber Loss\")\n\nplt.ylim((0, 8))\nplt.legend(loc=\"upper center\")\nplt.xlabel(\"Error$\")\nplt.ylabel(\"$L(y, f(x))$\")\nplt.show()",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Examples of Loss Functions for Linear Models</span>"
    ]
  },
  {
    "objectID": "notebooks/loss_functions_for_linear_models.html#logistic-regression",
    "href": "notebooks/loss_functions_for_linear_models.html#logistic-regression",
    "title": "10  Examples of Loss Functions for Linear Models",
    "section": "12.1 Logistic Regression",
    "text": "12.1 Logistic Regression\n\n\nCode\nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_features=1, n_redundant=0, \n                           n_informative=1,\n                           random_state=1,\n                           n_clusters_per_class=1,\n                           flip_y=0.0, class_sep=1)\nplt.figure()\nplt.xlabel('x')\nplt.ylabel('True or False')\nplt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel = SGDRegressor(loss='squared_error', fit_intercept=True, max_iter = 2000,\n                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\nmodel.fit(X, y)\nXp = np.linspace(X.min(),X.max(),100)\nXp = Xp[:, np.newaxis]\nplt.figure()\nplt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\nplt.plot(Xp,model.predict(Xp))\nplt.show()",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Examples of Loss Functions for Linear Models</span>"
    ]
  },
  {
    "objectID": "notebooks/loss_functions_for_linear_models.html#classification-loss-functions",
    "href": "notebooks/loss_functions_for_linear_models.html#classification-loss-functions",
    "title": "10  Examples of Loss Functions for Linear Models",
    "section": "12.2 Classification Loss Functions",
    "text": "12.2 Classification Loss Functions\nFor many linear classification problems, we can use a decision function: \\[\ny_i\\cdot(w\\cdot x_i)\n\\] where \\(y_i = \\pm 1\\) such that if \\(y_i\\) and \\(w\\cdot x_i\\) point have similar signs, then the decision function is positive, otherwise it is negative.\n\n\nCode\n# From: http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html\ndef modified_huber_loss(y_true, y_pred):\n    z = y_pred * y_true\n    loss = -4 * z\n    loss[z &gt;= -1] = (1 - z[z &gt;= -1]) ** 2\n    loss[z &gt;= 1.] = 0\n    return loss\n\n\nxmin, xmax = -4, 4\nxx = np.linspace(xmin, xmax, 100)\nlw = 2\nfig = plt.figure(figsize=(15,8))\nplt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], color='gold', lw=lw,\n         label=\"Zero-one loss\")\nplt.plot(xx, -np.minimum(xx, 0), color='yellowgreen', lw=lw,\n         label=\"Perceptron loss\")\nplt.plot(xx, modified_huber_loss(xx, 1), color='darkorchid', lw=lw,\n         linestyle='--', label=\"Modified Huber loss\")\nplt.plot(xx, np.log2(1 + np.exp(-xx)), color='cornflowerblue', lw=lw,\n         label=\"Log loss\")\nplt.plot(xx, np.where(xx &lt; 1, 1 - xx, 0), color='teal', lw=lw,\n         label=\"Hinge loss\")\nplt.plot(xx, np.where(xx &lt; 1, 1 - xx, 0) ** 2, color='orange', lw=lw,\n         label=\"Squared hinge loss\")\nplt.ylim((0, 8))\nplt.legend(loc=\"upper right\")\nplt.xlabel(r\"Decision function $f(x)$\")\nplt.ylabel(\"$L(y, f(x))$\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.linear_model import SGDClassifier\n\n#loss = 'squared_error'\nloss = 'log_loss'\n#loss = 'hinge'\n\nmodel = SGDClassifier(loss=loss, fit_intercept=True, max_iter = 2000,\n                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\nmodel.fit(X, y)\nXp = np.linspace(X.min(),X.max(),100)\nXp = Xp[:, np.newaxis]\nplt.figure()\nplt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\ntry:\n    plt.plot(Xp,model.predict_proba(Xp)[:,1],label='probability')\nexcept:\n    pass\nplt.plot(Xp,model.predict(Xp))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\n\n\nX, y = make_classification(n_features=2, n_redundant=0, \n                           n_informative=2,\n                           random_state=1,\n                           n_clusters_per_class=1,\n                           flip_y=0.0, class_sep=0.7)\n#rng = np.random.RandomState(2)\n#X += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\n# Change: try 0,1, or 2\nds = datasets[2]\n\nX, y = ds\nX = StandardScaler().fit_transform(X)\n\nplt.figure(figsize=(9, 9))\nplt.scatter(X[y==1,0],X[y==1,1],marker='+')\nplt.scatter(X[y==0,0],X[y==0,1],marker='o',facecolors='none',edgecolors='k')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.svm import LinearSVC\n\n# Try modifying these:\n#====================\nloss = 'squared_error'\n#loss = 'perceptron'\n#loss = 'log_loss'\n#loss = 'hinge'\n#loss = 'modified_huber'\n#loss = 'squared_hinge'\n\n# Also try the effect of Alpha:\n# e.g., between ranges 1e-20 and 1e0\n#=============================\nalpha=1e-3\n\n# You can also try other models by commenting out the below:\nmodel = SGDClassifier(loss=loss, fit_intercept=True,\n                      max_iter=200,tol=1e-5, n_iter_no_change =100,\n                      penalty='l2',alpha=alpha) \n#model = SGDClassifier(loss = 'hinge')\n#model = LinearSVC(loss='hinge',C=1e3)\nmodel.fit(X, y)\n\nplt.figure(figsize=(9, 9))\n\nh=0.01\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nif hasattr(model, \"decision_function\"):\n    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\nelse:\n    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\nZ = Z.reshape(xx.shape)\n\nvmax = max(abs(Z.min()),abs(Z.max()))\ncm = plt.cm.RdBu\nplt.contourf(xx, yy, Z, cmap=cm, alpha=.5, vmax = vmax, vmin = -vmax)\nlevels = [-1.0, 0.0, 1.0]\nlinestyles = ['dashed', 'solid', 'dashed']\ncolors = 'k'\nplt.contour(xx, yy, Z, levels, colors=colors, linestyles=linestyles)\nplt.scatter(X[y==1,0],X[y==1,1],marker='+')\nplt.scatter(X[y==0,0],X[y==0,1],marker='o',facecolors='none',edgecolors='k')\nplt.show()\n\n\nc:\\Users\\mafuge\\Box Sync\\ETHZ\\teaching\\ML4ME\\ML4ME_Textbook\\.conda\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:738: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n  warnings.warn(",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Examples of Loss Functions for Linear Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html",
    "href": "appendices/helpful_tooling.html",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "",
    "text": "A.1 TL;DR Checklist\nMachine learning projects are notoriously brittle: minor implementation details can cause major differences in outcomes. Good practices and tools make your implementation reproducible (and thus debuggable), and portable across different hardware environments, such as a teammate’s laptop or a high-performance computing (HPC) system.\nUnlike traditional programming, debugging ML models by simply “running and fixing in a loop” is rarely effective. Instead, a structured set of practices and tools is needed to understand model behavior and reproduce results reliably.\nHere is what we encourage doing, sorted by impact over effort.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#tldr-checklist",
    "href": "appendices/helpful_tooling.html#tldr-checklist",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "",
    "text": "Environment & Dependencies: Use a proper development environment, virtual environments and pin package versions. Document GPU/CUDA.\n\nLinting: Use tools like ruff to help you write clean code from the start.\nRandom Seeds: Seed all libraries (torch, numpy, random) and enable deterministic operations.\n\nLogging & Checkpoints: Log hyperparameters, save models, track experiments (e.g., wandb).\n\nVisualization: Plot data and learning curves.\nStart Small Then Scale: Debug on small datasets first.\n\nVersion Control: Track code with Git; use branches for experiments.\n\nModular Code: Split code into functions/classes and separate files.\n\nTesting & Type Hints: Write pytest tests and use mypy for type checking.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#reproducible-runs",
    "href": "appendices/helpful_tooling.html#reproducible-runs",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "A.2 Reproducible Runs",
    "text": "A.2 Reproducible Runs\nThe first requirement for a robust ML project is to make the codebase deterministic. Because ML is inherently probabilistic, running the same code twice without precautions may yield different results. Determinism is therefore a prerequisite for debugging.\n\nA.2.1 Python Project Setup\nA good practice is to use a virtual environment (e.g., venv) to isolate dependencies. This prevents conflicts between projects (e.g., my linear regression project uses sklearn 1.0.2, and my generative design project uses sklearn 2.5.3) and avoids interfering with the base operating system.\nDependencies should be pinned to exact versions in a file such as pyproject.toml or requirements.txt. This enables others (including future you) to reproduce the same environment.\nNote that not all dependencies are automatically captured in Python configuration files—for instance, the CUDA version used for GPU processing must be documented separately (typically in a README.md).\n\n\nA.2.2 Seeded Runs\nMost ML techniques involve randomness (e.g., parameter initialization, sampling, data shuffling). To ensure reproducibility, it is necessary to set a random seed so that random number generators produce a deterministic sequence.\nIn practice, several libraries must be seeded and it will look similar to:\nimport torch as th\nimport numpy as np\nimport random\n\nmy_seed = 42\n\nth.manual_seed(my_seed)  # PyTorch\nth.backends.cudnn.deterministic = True\ntorch.cuda.benchmark = False\nrng = np.random.default_rng(my_seed)  # NumPy\nrandom.seed(my_seed)  # Python's built-in random\n\n\nA.2.3 Hyperparameters\nHyperparameter values can influence results as strongly as changing the algorithm itself. It is essential to record which hyperparameters were used for each experiment.\nExperiment-tracking platforms automate this process. For example, Weights and Biases (wandb) can log hyperparameters, Python version, and hardware details, as well as visualize results such as learning curves. See, for example:\n\nRun overview with hyperparameter “Config.”\nLearning curves and sampled designs\n\n\n\n\n\n\n\nTipCheckpoint\n\n\n\nOnce versions, seeds and hyperparameters are fixed, running the model multiple times should yield identical results across runs (look at the wandb curves). Inconsistent results usually indicate a missing seed or an unpinned dependency. Without determinism, debugging will be much more time-consuming. We strongly advise to pay attention to this.\n\n\n\n\n\n\n\n\nNoteOther Sources of Non-Determinism\n\n\n\n\n\nThis is unlikely to happen within the course but it is still worth mentioning.\nEven if you set seeds and pin library versions, some sources of non-determinism may persist due to the environment:\n\nMultithreading or parallelism: Operations may be executed in different orders on CPU threads.\n\nGPU operations: Certain GPU kernels are non-deterministic by design, even with fixed seeds.\n\nLibrary versions or BLAS/CUDA backends: Different versions of underlying math libraries may produce slightly different results.\n\nTo mitigate these issues:\n\nEnable deterministic operations where possible (e.g., torch.backends.cudnn.deterministic = True for PyTorch).\n\nBe aware that some operations may never be fully deterministic on GPU—document this for reproducibility.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#code-management",
    "href": "appendices/helpful_tooling.html#code-management",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "A.3 Code Management",
    "text": "A.3 Code Management\nML projects should be approached as software engineering projects. Code quality and management are especially critical: poorly organized or fragile code increases the likelihood of errors and makes debugging more difficult. In addition, Python’s permissiveness can hide subtle mistakes. For example, automatic broadcasting of scalars to vectors may not raise an exception when performing operations on vectors or matrices of mismatched sizes, yet it can still produce incorrect results. Such silent errors are often harder to detect than explicit crashes.\n\nA.3.1 Code Organization\nNotebooks are valuable for exploration and prototyping, but they are less suited for building robust and reproducible experiments. Relying on a single notebook or script often leads to unmanageable code as the project grows. By contrast, a modular codebase is easier to test, extend, and maintain. Organizing code into smaller, modular components simplifies both debugging and collaboration.\n\nDivide the project into functions, each with a single, well-defined purpose. A useful rule of thumb is: if you cannot clearly explain what a function does in one sentence, it should probably be split.\n\nUse classes when it is natural to group related data and behavior together.\n\nSplit large projects across multiple files to make navigation easier. Avoid single files with 1000+ lines, as they are hard to read, debug, and extend.\n\n\n\nA.3.2 Version Control\nVersion control ensures that specific states of a project can be identified and restored. We strongly recommend using Git (with GitHub or similar platforms) for ML projects. When working in teams, branches help manage changes and prevent conflicts.\n\n\nA.3.3 Formatting and Linting\nCode formatting conventions (e.g., number of spaces per indentation, placement of comments, naming conventions) do not affect program behavior but improve readability. There are formatters that can automatically fix the visual style of the code—things like indentation, line breaks, spacing around operators, and alignment.\nLinting, goes beyond formatting: linters analyze your code for potential errors or risky patterns, such as unused variables, variables that may be undefined, or suspicious comparisons.\nruff integrates formatting, linting, and error detection in a single tool. It improves code quality, reduces stylistic disagreements, and allows developers to focus on the intent rather than the syntax.\n\n\nA.3.4 Type Hints\nIn addition to formatting and linting, static type checking helps catch errors before running your code. Python is dynamically typed, which means you can easily pass the wrong type of object to a function without immediate errors. Tools like mypy analyze your code using type hints and report mismatches.\nFor example, in:\ndef add_numbers(a: int, b: int) -&gt; int:\n    return a + b\n\nadd_numbers(2, \"3\") # note the String here\nRunning mypy will output:\nmain.py:4: error: Argument 2 to \"add_numbers\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nFound 1 error in 1 file (checked 1 source file)\nUsing type hints (a: int, -&gt; int) together with mypy lets you detect bugs early, improves code readability, and helps IDEs provide better autocompletion and refactoring support.\n\n\nA.3.5 Testing\nTesting individual components—functions, classes, or modules—is an important way to ensure reliability. Well-written tests allow developers to:\n\nIsolate potential error sources: When a bug occurs, thoroughly tested components can be excluded from investigation, saving time.\n\nDetect unintended side effects: Tests help ensure that changes in one part of the codebase do not break other parts.\n\nTesting and good code organization go hand in hand: modular code is naturally easier to test, and writing tests often encourages cleaner, more maintainable designs.\nThe most common tool for this is pytest. For example:\nIf you define a function in your project:\n# my_project/utils.py\ndef add_numbers(a, b):\n    return a + b\nYou can define tests with:\n# tests/test_math.py -- this is your test file\nfrom my_project.utils import add_numbers\n\ndef test_add_numbers():\n    assert add_numbers(2, 3) == 5\n    assert add_numbers(-1, 1) == 0\nRunning pytest will automatically discover these tests and report any failures.\n\n\nA.3.6 Integrated Development Environments (IDEs)\nWhile you can write Python code in any text editor, using an IDE significantly improves productivity. Visual Studio Code (VS Code) is the most popular choice for Python and ML development. It supports:\n\nExtensions: Add functionality and friendly interface for linting ruff, type checking mypy, testing pytest, and git integration.\n\nHandling of virtual environments: VSCode can create and handle virtual environments for you.\nDebugger: Set breakpoints and inspect the current state of variables, run instruction by instruction. This is much easier than putting prints everywhere.\nNotebooks inside VS Code: You can run Jupyter notebooks directly within your IDE.\nLLMs integration: Students have access to GitHub education (and Copilot), VSCode has a direct LLM integration for code completion and agent.\n\n\n\nA.3.7 Large Language Models (LLMs) for Coding\nTools like ChatGPT or GitHub Copilot can generate code quickly. While this can accelerate boilerplate writing, it does not replace understanding.\nMachine learning code is particularly sensitive to details: a small mistake in data preprocessing, tensor dimensions, or random seeding can completely change results. Using LLMs without knowing what the code does may:\n\nHide important assumptions.\n\nLead to silent bugs that are hard to detect.\n\nPrevent you from learning how ML algorithms really work.\n\nGuideline: LLMs are great for generating snippets (e.g., “write a function to convert my CSV data to JSON”), but always read, run, and understand the code before using it in experiments. For ML, correctness and reproducibility are more important than speed.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#debugging",
    "href": "appendices/helpful_tooling.html#debugging",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "A.4 Debugging",
    "text": "A.4 Debugging\nIf the codebase is well-structured and reproducible but issues persist, the problem is likely related to the maths, hyperparameters, or data.\n\nA.4.1 Visualizing\nVisualization is one of the most effective debugging tools in ML, particularly in engineering contexts where results can often be represented graphically.\n\nA.4.1.1 Algorithm-level Visualizations\n\nLoss curves: Simple plots can reveal overfitting, underfitting, or learning failures.\nPredictions: Comparing model outputs with reference data at various training stages provides direct insight into progress.\n\nFor these, we often report metrics and outputs in wandb, see for instance these lines.\n\n\nA.4.1.2 Data-level Visualizations\n\nInspect dataset distributions: Check whether features are on compatible scales, whether rescaling or normalization is needed, and whether outliers are present. Tools like matplotlib or seaborn can help.\n\nAssess assumptions: Determine whether the data distribution aligns with the model’s underlying assumptions, e.g., can the data distribution be captured by a Gaussian distribution.\n\n\n\n\nA.4.2 Split Your Pipeline\nIt is good practice to split your training pipeline into distinct stages:\n\nData analysis: Visualize your data. Look at the distributions, detect outliers, and gain insights into what preprocessing might be needed and which models may perform well.\nData pre-processing: Massage your data before feeding it to the model. Visualize to ensure transformations are correct and consistent.\nTraining: Train your model on the preprocessed data. Save trained models to disk after each run (torch.save, pickle, or similar). This allows you to avoid retraining from scratch every time you tweak evaluation code.\n\nEvaluation: Load the saved model and run your evaluation routines on validation or test datasets.\n\nBy separating these stages, you can debug each part independently, and validate progress.\n\n\nA.4.3 Start Small, Then Scale\nWhen debugging, it is inefficient to run large-scale experiments immediately. Instead:\n\nBegin with small, fast experiments (e.g., a reduced dataset or a lightweight simulator).\n\nValidate that the model can learn on trivial cases.\n\nAttempt to reproduce established results or baseline performance.\n\nScaling to larger, more complex runs should only occur once smaller experiments confirm that the model behaves as expected.\n\n\nA.4.4 Performance Profiling with timeit\nSometimes the bug is actually that the code is too slow. When this happens, the first step is often to measure where the time goes. You can do that with Python’s built-in timeit.\ntimeit runs a snippet of code multiple times and reports the average execution time, helping you compare different implementations or detect bottlenecks.\nHere is an example for normalizing data:\nimport numpy as np\nimport timeit\n\nsetup = \"\"\"\nimport numpy as np\ndata = np.random.rand(10000, 100)  # 10k samples, 100 features\n\"\"\"\n\n# Option 1: Pure Python loops\nstmt1 = \"\"\"\nnormalized = []\nfor row in data:\n    mean = np.mean(row)\n    std = np.std(row)\n    normalized.append((row - mean) / std)\nnormalized = np.array(normalized)\n\"\"\"\n\n# Option 2: NumPy vectorization\nstmt2 = \"\"\"\nmeans = np.mean(data, axis=1, keepdims=True)\nstds = np.std(data, axis=1, keepdims=True)\nnormalized = (data - means) / stds\n\"\"\"\n\nprint(\"Python loops:\", timeit.timeit(stmt1, setup=setup, number=10))\nprint(\"NumPy vectorization:\", timeit.timeit(stmt2, setup=setup, number=10))\nResults:\nPython loops: 0.8857301659882069\nNumPy vectorization: 0.04489224997814745\nUsing NumPy vectorization is ~20x faster than Python loops.\nIn notebooks, you don’t even need imports:\n%timeit sum(range(1000))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#a-practical-example",
    "href": "appendices/helpful_tooling.html#a-practical-example",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "A.5 A Practical Example",
    "text": "A.5 A Practical Example\nThis section shows a practical example using the techniques explained above on an actual code.\n\nA.5.1 Step 0: The Ugly Script\nWe start with some messy code that Ruff would flag:\n# train.py\nimport numpy as np, torch, torch.nn as nn, torch.optim as optim, matplotlib.pyplot as plt, random\n\nX=np.linspace(0,100,100).reshape(-1,1)\ny=5*np.sin(0.1*X)+np.random.randn(100,1)\n\nmodel = nn.Linear(1,1)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nloss_fn = nn.MSELoss()\n\nfor epoch in range(500):\n    pred = model(torch.tensor(X))\n    loss = loss_fn(pred, torch.tensor(y))\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(f\"Epoch {epoch} - Loss: {loss.item()}\")\nAt first glance, it looks okay but it won’t run. Try executing python train.py to see the errors.\n\n\nA.5.2 Step 1: From Ugly to Bad\nRun ruff to format and check. Fix the errors (or call ruff check --fix train.py).\nNow the code is already cleaner and easier to debug. Still, running the file throws errors.\n\n\nA.5.3 Step 2: Debugging\nRunning python train.py gives a cryptic type error at the loss computation: RuntimeError: mat1 and mat2 must have the same dtype, but got Double and Float.\nThe problem is that the model outputs float32 predictions while y is float64. This causes a mismatch in the loss computation. The fix is to change the lines to:\n- pred = model(torch.tensor(X))\n- loss = loss_fn(pred, torch.tensor(y))\n+ pred = model(torch.tensor(X, dtype=torch.float32))\n+ loss = loss_fn(pred, torch.tensor(y, dtype=torch.float32))\n\n\nA.5.4 Step 3: Make your Script as Deterministic as possible\nIt is important to remove sources of non determinism when debugging ML models, see seeding.\n# right after the imports\nrng = np.random.default_rng(42)  # seed NumPy random\ntorch.manual_seed(42)  # seed PyTorch\ntorch.cuda.manual_seed(42)  # see PyTorch CUDA (for NVIDIA GPUs)\ntorch.backends.cudnn.deterministic = True  # tell PyTorch to use deterministic kernels\ntorch.backends.cudnn.benchmark = False # removes internal optimizations that can cause non-determinism \nAnd when defining your outputs:\n# replace np.random by the seeded RNG\n- y = 5 * np.sin(0.1 * X) + np.random.randn(100, 1)\n+ y = 5 * np.sin(0.1 * X) + rng.standard_normal(size=(100, 1))\n\n\nA.5.5 Step 4: Visualizing\nIt is extremely important to visualize your data. For this, we can add this to the script:\n\nA.5.5.1 Visualizing data\nimport matplotlib.pyplot as plt\nand before the training loop:\nplt.scatter(X, y)\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"Raw sinusoidal data\")\nplt.show()\n\n\nA.5.5.2 Visualizing loss\n# before training loop\nlosses = []\n\n# in your training loop\nlosses.append(loss.item())\n\n# after training loop\nplt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss over time\")\nplt.show()\nObservations:\n\nThe target is sinusoidal, a simple linear model cannot capture this (we’ve made bad assumptions for the model).\nThe loss is producing nans and going to inf.\nThe features are not normalized, making learning difficult.\n\n\n\n\nA.5.6 Step 5: Normalizing Features\nX_mean = X.mean(axis=0, keepdims=True)\nX_std = X.std(axis=0, keepdims=True)\nX_norm = (X - X_mean) / X_std\n\nX_tensor = torch.tensor(X_norm, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.float32)\nUse these normalized tensors in the training loop instead of the raw values.\n\n\nA.5.7 Step 6: Visualizing Predictions\nNow the loss seems to go down, the code runs. Let’s look at the predictions. This code will help you visualize the predictions vs. the true values:\n# after the training loop\nwith torch.no_grad():\n    predictions = model(X_tensor)\n\nplt.figure(figsize=(8, 5))\nplt.scatter(X_tensor.numpy(), y_tensor.numpy(), label=\"True data\", alpha=0.5)\nplt.scatter(\n    X_tensor.numpy(), predictions.numpy(), label=\"Predictions\", color=\"red\", alpha=0.5\n)\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"True vs Predicted\")\nplt.legend()\nplt.show()\nObservation: It is pretty obvious that our model has not enough capacity to capture the data.\n\n\nA.5.8 Step 7: Adjusting Hyperparameters\nLet’s try to increase the model size.\nmodel = nn.Sequential(\n    nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 16), nn.ReLU(), nn.Linear(16, 1)\n)\nAnd re-run. Now we see the loss is not optimal.\n\nCan you adjust the learning rate and model size?\nAnd how are you going to keep track of what combination of hyperparameter values you have tried?\nAlso, you have several plots (prediction, loss) for each run which are helpful.\n\nFor this, we recommend using experiment trackers, such as weights and biases.\nFirst, you start by defining your hyperparameters on top the file:\nhyperparameters = {\n    \"learning_rate\": 0.01,\n    \"model_layers\": [16, 16],\n    \"activation\": \"ReLU\",\n}\nand use them in your training script. For instance, your model definition becomes\nmodel_layers: list[int] = hyperparameters[\"model_layers\"]\nlayers = []\n\n# Build all layers including input and hidden layers -- this allows to just change the model_layers in your hyperparameters dictionary.\ncurrent_size = 1\nfor layer_size in model_layers:\n    layers.append(nn.Linear(current_size, layer_size))\n    if hyperparameters[\"activation\"] == \"ReLU\":\n        layers.append(nn.ReLU())\n    elif hyperparameters[\"activation\"] == \"Sigmoid\":\n        layers.append(nn.Sigmoid())\n    current_size = layer_size\n\n# Add output layer\nlayers.append(nn.Linear(current_size, 1))\n\nmodel = nn.Sequential(*layers)\noptimizer = optim.SGD(model.parameters(), lr=hyperparameters[\"learning_rate\"]) # see hyperparameter here\nThen, you log these hyperparameters for each experiment:\n wandb.init(project=\"example\", config=hyperparameters)\nIn your training loop:\nwandb.log({\"loss\": loss.item()})\nYou can even log an image of prediction vs. true data at each training step. See below.\n\n\nA.5.9 Final Code\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport wandb\n\nrng = np.random.default_rng(42)  # seed NumPy random\ntorch.manual_seed(42)  # seed PyTorch\ntorch.cuda.manual_seed(42)  # see PyTorch CUDA (for NVIDIA GPUs)\ntorch.backends.cudnn.deterministic = True  # tell PyTorch to use deterministic kernels\ntorch.backends.cudnn.benchmark = (\n    False  # removes internal optimizations that can cause non-determinism\n)\n\nhyperparameters = {\n    \"learning_rate\": 0.01,\n    \"model_layers\": [16, 16],\n    \"activation\": \"ReLU\",\n}\n\n\ndef predictions_plot(visualize: bool = False, log: bool = False) -&gt; None:\n    \"\"\"Plot the predictions vs. the true values.\n    \n    Args:\n        visualize: Whether to show the plot.\n        log: Whether to log the plot to wandb.\n    \"\"\"\n    with torch.no_grad():\n        predictions = model(X_tensor)\n\n    plt.figure(figsize=(8, 5))\n    plt.scatter(X_tensor.numpy(), y_tensor.numpy(), label=\"True data\", alpha=0.5)\n    plt.scatter(\n        X_tensor.numpy(),\n        predictions.numpy(),\n        label=\"Predictions\",\n        color=\"red\",\n        alpha=0.5,\n    )\n    plt.xlabel(\"X\")\n    plt.ylabel(\"y\")\n    plt.title(\"True vs Predicted\")\n    plt.legend()\n    if visualize:\n        plt.show()\n    else:\n        plt.savefig(\"predictions.png\")\n        if log:\n            wandb.log({\"predictions\": wandb.Image(\"predictions.png\")})\n        plt.close()  # Close the figure to free memory\n\n\nif __name__ == \"__main__\":\n    wandb.init(project=\"example\", config=hyperparameters)\n\n    X = np.linspace(0, 100, 100).reshape(-1, 1)\n    y = 5 * np.sin(0.1 * X) + rng.standard_normal(size=(100, 1))\n\n    X_mean = X.mean(axis=0, keepdims=True)\n    X_std = X.std(axis=0, keepdims=True)\n    X_norm = (X - X_mean) / X_std\n\n    X_tensor = torch.tensor(X_norm, dtype=torch.float32)\n    y_tensor = torch.tensor(y, dtype=torch.float32)\n\n    model_layers: list[int] = hyperparameters[\"model_layers\"]\n    layers = []\n\n    # Build all layers including input and hidden layers\n    current_size = 1\n    for layer_size in model_layers:\n        layers.append(nn.Linear(current_size, layer_size))\n        if hyperparameters[\"activation\"] == \"ReLU\":\n            layers.append(nn.ReLU())\n        elif hyperparameters[\"activation\"] == \"Sigmoid\":\n            layers.append(nn.Sigmoid())\n        current_size = layer_size\n\n    # Add output layer\n    layers.append(nn.Linear(current_size, 1))\n\n    model = nn.Sequential(*layers)\n    optimizer = optim.SGD(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n    loss_fn = nn.MSELoss()\n\n    losses = []\n    for epoch in range(500):\n        pred = model(X_tensor)\n        loss = loss_fn(pred, y_tensor)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        print(f\"Epoch {epoch} - Loss: {loss.item()}\")\n        wandb.log({\"loss\": loss.item()})\n\n        # Log predictions image to wandb during training (but don't show plot)\n        predictions_plot(visualize=False, log=True)\n\n        losses.append(loss.item())\n\n    plt.plot(losses)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss over time\")\n    plt.show()\n\n    # Show predictions plot at the end of training\n    predictions_plot(visualize=True, log=False)\n    wandb.finish()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html",
    "href": "appendices/course_progression.html",
    "title": "Appendix B — Course Lecture Progression",
    "section": "",
    "text": "B.1 Part 0: Review and Foundations\nThis section describes an example progression through the course material based on my “Machine Learning for Mechanical Engineering” course at ETHZ. It assumes two lecture+exercise sessions twice a week, for a duration of 1 hour and 45 minutes each with a 5-10 minutes break in the middle of each session. This provides a sample of how you might work through the content in this book, and also acts as a reference to the students. My course is structured such that lectures and exercises occur in the same session, often alternating between lecture and in-class demonstrations or exercises, and this is frequently reflected in each of the book chapters. In class, for time reasons, I may skip some of the longer derivations in the book, since these can be effectively studied on their own.\nThese lectures introduce the course and also cover some background information that will be foundational and critical later in the course. For illustrative and notational purposes, we will use Linear Regression as a simple model to understand these foundations first, and this will allow us to build up to more complex models as we go through the course.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html#part-0-review-and-foundations",
    "href": "appendices/course_progression.html#part-0-review-and-foundations",
    "title": "Appendix B — Course Lecture Progression",
    "section": "",
    "text": "B.1.1 Lecture 1: Course Introduction and Review of ML Basics\n\nOverview the course structure and syllabus\nReview several basics that should have been covered in the prior Stochastics and Machine Learning course\n\nVisualizing Data Review using the California Housing Dataset Notebook\nReview of Cross Validation in Evaluating ML Models\nReview of Linear models\n\n(Re-)Introduction to Coding and Tooling basics to help with the rest of the course, such as Editors, Colab, Version Control, Basic Debugging. Read Helpful Tooling.\n\n\n\nB.1.2 Lecture 2: Review of (Stochastic) Gradient Descent\n\nRead Stochastic Gradient Descent\nRead Why Momentum Really Works\nReview of Regularization for Linear Regresson models, including weight decay (L2) and L1/sparsity control, and effects on loss functions.\nReview of Linear Unsupervised Learning (e.g., PCA, Sparse PCA, etc.)\n\n\n\nB.1.3 Lecture 3: Automatic Differentiation and Taking Derivatives\n\nRead Taking Derivatives, except for Advanced topics like Implicit Differentiation and JVP/HVP (this is for a later lecture)\nIn-Class example of Forward and Reverse AD on a simple function\nIn-Class example of AD through a Verlet Integrator\n\n\n\nB.1.4 Lecture 4: Advanced Differentiation\n\nRead the remaining parts of Taking Derivatives\nIn-Class example of Implicit Differentiation\nIn-Class example with JVPs for computing some example properties of Linear Models, for example, the Fisher Information Matrix.\n(Time Permitting) Examples of Projected Gradient or Proximal Gradient methods.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html#part-1-advanced-neural-network-models",
    "href": "appendices/course_progression.html#part-1-advanced-neural-network-models",
    "title": "Appendix B — Course Lecture Progression",
    "section": "B.2 Part 1: Advanced Neural Network Models",
    "text": "B.2 Part 1: Advanced Neural Network Models\n\nB.2.1 Lecture 5: Review of Neural Network Models\n\nRead Review of Neural Networks\nIn-Class example of Visualizing NN Layers using ConvNetJS\nIn-Class examples of Auto-Encoders on Simple Data, Auto-Encoders on Airfoil Data, and Least Volume Auto-Encoders (including Spectral Normalization)\n\n\n\nB.2.2 Lecture 6: Push-Forward Generative Models\n\nRead Push-Forward Generative Models\nIn-Class examples of GANs, VAEs, and Normalizing Flows on Synthetic Data and the Airfoil Problem\n\n\n\nB.2.3 Lecture 7: Interlude – Techniques for Building and Debugging ML Models\n\nRead Helpful Tooling for Working with and Debugging Machine Learning Models\nIn-Class Before and After example implementing the changes\nIn-Class example of improving/correcting an LLM-provided code solution\n\n\n\nB.2.4 Lecture 8: Optimal Transport\n\nRead Measuring Differences Between Distributions\nIn-Class example of an Optimal Transport Generative Model\n\n\n\nB.2.5 Lecture 9: Stochastic Generative Models\n\nRead Stochastic Generative Models\nIn-Class examples of Diffusion Models on Synthetic Data and the Airfoil Problem\nIn-Class examples of Sequence Data generation (e.g., the Tangrams Example)\n\n\n\nB.2.6 Lecture 10: Latent Generative Neural Models\n\nIn-Class example of Latent Diffusion Models via Least Volume AEs.\nIn-Class example of continuous+discrete models\nPractical Demonstrations of complex models on EngiBench\n\n\n\nB.2.7 Lecture 11:\n\nRead Introduction to Transformers\nIn-Class experiment with the Attention Mechanism\nIn-Class demonstrations of Latent Transformer models (e.g., VQGAN)\n\n\n\nB.2.8 Lecture 12: Review of Reinforcement Learning\n\nRead Reinforcement Learning\nIn-Class example of Linear Functional Q-Learning\n\n\n\nB.2.9 Lecture 13: Actor-Critic and Policy Gradient Methods\n\nPolicy Gradients\n\n\n\nB.2.10 Lecture 14: Advanced Policy Methods\n\nDiffusion Policies\nTransformer Policies\nLatent Generative Policies (e.g., Dreamer and variants)\n\n\n\nB.2.11 Lecture 15: Message Passing, Graph Neural Networks, and other Structured Data\n\nRead Graph Neural Networks\nLearning with Meshes and Point Clouds\nLearning with Signed Distance Fields",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html#part-2-probabilistic-models-and-kernels",
    "href": "appendices/course_progression.html#part-2-probabilistic-models-and-kernels",
    "title": "Appendix B — Course Lecture Progression",
    "section": "B.3 Part 2: Probabilistic Models and Kernels",
    "text": "B.3 Part 2: Probabilistic Models and Kernels\n\nB.3.1 Lecture 16: Review of Probabilistic Models\n\nRead ?sec-reviewprobability of Probabilistic Models, since this reviews the information from the prior course. We will revisit the later sections in a later lecture.\nIn-Class exercise deriving the MLE for various common distributions\n\n\n\nB.3.2 Lecture 17: Introduction to Probablistic Programming\n\nBayesian Linear Regression in a Probabilistic Programming Language\nIntroduction to Approximate Inference Methods, such as MCMC and Variational Inference\n\n\n\nB.3.3 Lecture 18: Large-Scale Bayesian Models and Debugging Probabilistic Inference\n\nStochastic Variational Inference and Stein Variational Gradient Descent\nPitfalls of Probabilistic Models and Debugging Inference\n\n\n\nB.3.4 Lecture 19: Kernel Basics\n\nRepresenter Theorem\nKernelized Ridge Regression\nRegularization of Kernels and effects of Fourier Spectra\n\n\n\nB.3.5 Lecture 20: Gaussian Processes and Bayesian Optimization\n\nBasics of Gaussian Processes\nBayesian Optimization\nEffect of Dimension on GPs and related variants (AdditiveGPs, etc.)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html#part-3-engineering-relevant-machine-learning-topics",
    "href": "appendices/course_progression.html#part-3-engineering-relevant-machine-learning-topics",
    "title": "Appendix B — Course Lecture Progression",
    "section": "B.4 Part 3: Engineering-relevant Machine Learning Topics",
    "text": "B.4 Part 3: Engineering-relevant Machine Learning Topics\n\nB.4.1 Lecture 21: Active Learning and Semi-Supervised Learning\n\nNotebook on Active Learning and Semi-Supervised Learning\n\n\n\nB.4.2 Lecture 22: Transfer Learning and Foundation Models\n\nFine-Tuning a Pre-trained model\nJointly embedded models\n\n\n\nB.4.3 Lecture 23: Integrating Everything Together\n\nIn-Class exercise integrating all prior topics using EngiBench/EngiOpt\n\n\n\nB.4.4 Lecture 24: Ethical and Legal Implications of ML within Engineering\n\nAdversarial Attacks and Defenses\nPrivacy-Preserving and Federated ML\nInterpretability Methods\nCase Study: National Algorithms Safety Board",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  }
]